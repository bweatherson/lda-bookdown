# Methodology {#methodology-chapter}

The point of this chapter is to explain the choices I made in building the model that the book is based around. But to understand the choices that I made, it helps to know a little bit about what a Latent Dirilecht Algorithm (LDA) does.

The inputs to the model are some texts and a number. The model doesn't care about the ordering of words in the texts, so really the input isn't texts but a list of lists of ordered pairs. Each ordered pair is a word and a number. In the version I'm using, the outer list is a list of philosophy articles. And each element of that list is a list of words in that article, along with the number of times the word appears.

Along with that, you give the model a number. This is the number of _topics_ that you want the model to divide the texts into. I'll call this number $t$ in this introduction. And intuitively there is a function $T$ that maps articles into the $t$ topics. 

What the model outputs is, for our purposes, a pair of probability functions: one for articles and one for words.

The probability function for articles gives, for each article $a$ and topic number $n \in \{1, \dots, t\}$, a probability for $T(a) = n$; that is, it gives a probability that the article is in that topic. Notably, it doesn't identify the topics with any more than numbers. I'm going to give names to the topics—this one is [Kant](#topic32); this one is [composition and constitution](#topic89), etc.—but the model doesn't do that. For it, the topics really are just integers between 1 and $t$.

The probability function for words gives, for each word $w$ from any of the articles, and topic number $n \in \{1, \dots, t\}$, the probability that a randomly chosen word from the articles in that topic is $w$. So in the Kant topic, the probability that a randomly chosen word is _Kant_ is about 0.14. 

That number feels absurdly high, but it makes sense for a couple of reasons. One is that to make the models compile in even semireasonable time, I filtered out a lot of words. What's it's really saying is that the word _Kant_ produces about 1/7 of the tokens that remain. The other is that what it's really giving you here is the probability that a random word in an article is _Kant_ conditional on the probability of that article being in the [Kant](#topic32) is 1. And in fact the model is never that confident. Even for articles that might be considered to be clearly articles about Kant, the model is rarely more than 40 percent confident that that's what they are about. And this is for a good reason. Most articles about Kant in philosophy journals are, naturally enough, about Kantian philosophy. And any part of Kantian philosophy is, well, philosophy. So the model has a topic on [beauty](#topic08), and when it sees an article on Kantian aesthetics, it gives some probability the correct classification of that article is in the topic on Beauty. So the word probabilities are quite abstract things—they are something like word frequencies in a certain kind of stereotyped article. What the model really wants to do is find $t$ stereotypes such that each real article is a linear mixture of the stereotypes. 

The way the model approaches this goal is by building two probability functions, checking how well they cohere, and recursively refining them in places that they don't cohere. One probability function is the probability, for each article, that it is in one or other of the ninety topics. So it might say this article is 0.4 likely to be in topic 32, 0.3 likely to be in topic 68, and so on down to some vanishing probability that it is in topic 89. The other probability function is the probability, for each topic, of a given word appearing. So it might say that given the article is in topic 32, there is a 0.15 likelihood that a randomly selected word in the article is _Kant_, an 0.05 likelihood that a randomly selected wordis _ideal_, and so on, down to a vanishingly small likelihood that the word is, say, _Weatherson_. Combining those functions, we get the probability, for each actual article, that a randomly selected word in it is _Kant_ or _ideal_ or _Weatherson_ or any other word. And we can check that calculated probability against the actual frequency of each word in the article. I'll call the calculated probabilities the _modeled frequencies_, and say that the goal of the model is to have the modeled frequencies of words in articles match the actual frequencies of the words in the articles. A perfect match here is impossible to achieve; there aren't enough degrees of freedom. But the model can minimize the error, and it does so recursively.

The process involved is slow. I was able to build all the models I'll discuss on personal computers, but it takes some processing time. The particular model I'm primarily using took about twenty hours to build, but I ran through many more hours than that building other models to compare it to.

And the process is very path dependent. The algorithm, like many algorithms, has the basic structure of pick a somewhat random starting point, then look for a local equilibrium. That's incredibly dependent on how you start and somewhat dependent on how you travel.

The point of this chapter is to describe how I chose the inputs to the model I ended up using, and then how I set various parameters within the model. The parameters are primarily, in terms of the metaphor of the previous paragraph, the starting point of the search, and how long the search should go before we decide one is something close enough to an equilibrium.

The inputs are more complex. Very roughly, the inputs I used are the frequently occurring substantive words from research articles in twelve important philosophy journals. I'll start by talking about how and why I selected the particular twelve journals that I did.

## Selecting the Twelve Journals

This is a study about the trajectory of topics across leading philosophy journals. But presumably most people reading this aren't interested in philosophy journals as such; they are interested in the trajectory of philosophy. So it is important to select, as far as possible, journals that accurately reflect what's going on in philosophy.

An obvious idea would be to just use generalist journals, because they will reflect what's generally happening in philosophy. But this turns out to be a bad idea, since there really aren't any generalist journals in philosophy. Perhaps that's because the journals in moral and political philosophy, and in philosophy of science, are so good, that so-called generalist journals tend to under-represent work in those fields. Or, perhaps more precisely, they don't always reflect the cutting-edge work in those fields.

In [previous work](http://tar.weatherson.org/2017/04/26/citation-patterns-across-journals/), I noted how little attention the leading generalist journals had paid to two of the most important late twentieth-century articles, Elizabeth Anderson's ["What is the Point of Equality?"](https://philpapers.org/rec/ANDWIT), and Peter Machamer, Lindley Darden and Carl Craver's ["Thinking about Mechanisms"](https://philpapers.org/rec/MACTAM). These papers each have over 2500 Google Scholar citations, but they have barely been mentioned in the leading generalist journals. An accurate picture of recent philosophy has to include the literatures these papers spawned, and those literatures on the whole aren't found in generalist journals. So to get an accurate picture of philosophy, you need to include at least some specialist journals.

As a reminder, here are the journals that I've included:

```{r journaltablereprise, echo=FALSE, cache=TRUE}
  kable(journals_summary, 
        col.names = c("Journal", "First Year", "Number of Articles"), 
        align=c("l", "c", "c")
  )
```

As you can see, there are two moral and political journals, _Ethics_ and _Philosophy and Public Affairs_ (_P&PA_), and two philosophy of science journals, _Philosophy of Science_ and the _British Journal for the Philosophy of Science_. I could possibly have gotten by with just one of each. But I thought _Ethics_ and _P&PA_ brought in different fields of philosophy, and so they were both worth including. That meant it would be good to balance them with two philosophy of science journals. This had the side benefit of my not having to decide which of those two philosophy of science journals was more representative.

But if I had those four "specialist" journals, I needed enough "generalist" journals that what I had felt representative of philosophy as a whole. Partially to get the balance right and partially to make the graphs look nice, it felt like I needed eight more journals. I started with the current "big four" journals.

- _Mind_
- _The Philosophical Review_
- _Journal of Philosophy_
- _Noûs_

I added _Analysis_ because I wanted to be very sensitive to trends, and _Analysis_ is often ahead of the trends in the field. That leaves three more spots. Here were the criteria I used to fill those:

- The data for the journal had to be available through JSTOR's Data for Researchers. This was not negotiable since that was my data source. But it was unfortunate, since it ruled out the _Australasian Journal of Philosophy_, which would otherwise have been perfect.
- The journal had to be active for a long time. It wouldn't help balance much to add a journal that didn't exist during the timeframe I'm looking at. This ruled out _Philosophical Studies_. That's a bit of a shame since the story of twenty-first century philosophy can't be told without _Philosophical Studies_. But it just doesn't have enough history for the purposes of this study.
- The journal could not be too idiosyncratic. I wanted it to tell something about the field, not just about those journals. This ruled out _The Monist_, which was very idiosyncratic in its early years. In recent years, it is idiosyncratic in a good way; highlighting work the others sometimes overlook. But before World War II it is barely a philosophy journal in any recognizable sense.
- The journal could not be a philosophy of science journal, since the aim is to balance the two philosophy of science journals I have.
- The journal had to primarily publish in English, since the analysis tools I'm using simply don't work for cross-linguistic data sets. The last two criteria ruled out _Synthese_ and _Erkenntnis_.

After all that, I was left with:

- _Philosophy and Phenomenological Research_: This has slightly more non-English work than is ideal for current purposes, but I thought adding a little continental philosophy from its early years was worthwhile. And it became such an important journal that it felt wrong to leave it out.
- _Proceedings of the Aristotelian Society_: Note that I'm including the supplementary volumes here.^[Including them also causes a few headaches. The articles in the supplementary volumes often have respondents. When they do, the metadata often lists both the original article and the reply article as coauthored. This is bizarre, though even more bizarre is that often the running head on the print article does the same thing. This can totally mess up the calculating of philosophical Erdös numbers which one would probably know if they have calculated them). This study isn’t tracking authors, so it isn’t too painful. But I am using auto-generated citations as a way of picking out articles, and they will sometimes look coauthored when they are really not. I’m not going to try fixing this; it’s just a weirdness that I’ll live with.] This is idiosyncratic in its early years; some secretaries of the Aristotelian Society have a bigger impact on this study than they do on the field. But without it, so much of British philosophy is missed, including some themes in contemporary philosophy that aren’t always covered in the other major journals.
- _Philosophical Quarterly_. For much of the twentieth century, this is much less prestigious than the other eleven journals I'm looking at, and this will become relevant when I look at the citation data. But it fits the other criteria very well. It adds a Scottish journal to the English and US journals I am otherwise looking at. It has slightly better history coverage than the other journals, and since I worked at St Andrews for so many years, I'm personally fond of it.

One might wonder why I didn't add any other specialist journals, along with journals in ethics and in philosophy of science. The reasons were a bit varied.

I think there is a better sense of midcentury philosophy if one logic journal is added. But text mining can't be done on symbols. And in more recent years, the sense in which the logic journals are primarily philosophy journals as opposed to mathematics journals has gotten weaker. So, I left them out.

The twelve journals I have don;t include as much history of philosophy as there is in the profession. But that's simply unavoidable if doing a study based on journals. History of philosophy is primarily a book discipline rather than a journal discipline. This can be seen in the citation data. Pick almost any prominent figure in history of philosophy and odds are that I'll have several journal articles with more citations than their most cited article. The prominent figure picked will almost surely have several books that are more widely cited than any of my articles. The point isn't that historians of philosophy are never cited but that they rarely have highly cited articles. Just as importantly, when a history article is widely cited, it usually appears in one of the twelve journals I've already included. For this reason, the model that I end up working with does have a lot of history categories. Just remember that the absolute numbers of articles in each of these categories is not representative of how important the categories are in philosophy.

And the other specialist journals are either too new (e.g., _Mind and Language_, or _Linguistics and Philosophy_) or representative of too small a section of contemporary philosophy to be worth including. Aesthetics, for example, is an important philosophical field. (And it shows up in the model in an interesting way.) But including the _Journal of Aesthetics and Art Criticism_ in the study would have made it look like aesthetics was 1/13 of the field, and that's misleading. So I stuck with these twelve.

## Selecting the Articles

Journals publish a lot, and I had to decide what to include and what to leave out. The aim was to include all and only research articles, but this was harder than it looks.

The metadata that JSTOR provides includes a tag for article kind. I only included articles with the tag "research-article", which does a reasonable job of getting rid of book reviews. But it turns out that it includes a lot of things that are not really research articles. It functions in the JSTOR metadata as something of a generic article kind, one that applies if nothing else seems right. So we have to manually edit out a bunch of articles.

I deleted all articles without a listed author. These were often editorials, corrections and the like.

After that, I started working through various words in titles that indicated something was not actually a research article. So I deleted all articles with these titles:

- Descriptive Notices
- Editorial
- Letter to Editor
- Letter
- Introduction

The first four are clear enough. The last was mostly a problem for special issues, but there were enough special issues of one kind or another to make it worthwhile. Then I deleted any articles that had the following phrases anywhere in the title:

- Correction
- Foreword
- Introductory Note
- Errat
- Erata
- Abstract of C
- Abstracts of C
- To the Editor
- Corrigenda
- Obituary
- Congress

The last is the only one that really needs comment. All the articles I found with this in the title were reports on one or another philosophy congress, not genuine research articles. Maybe there was a political philosophy article that referenced the United States Congress in its title and should not have been excluded but I didn't see it.

Since text mining only works within a single language, I excluded all the articles whose listed language in the metadata was anything other than English. And I manually excluded, when I saw them, articles whose title was not in English and which seemed like non-English articles.

That left me with `r nrow(articles)` articles to work with.

## Selecting the Words {#stop-words}

The JSTOR data excludes a few stop words (like _the_ and _and_), and words with one or two characters. On the other hand, it takes nonletters to be word breaks. So _doesn't_ would be split into _doesn_ and _t_ and the second rejected as too short. And hyphenated words are split as well. It turned out that this made _est_ into a reasonably common word. But I didn't want to include all the words for various reasons.

It seems common in text mining to exclude a more expansive list of stop words than JSTOR leaves out. I was playing around with making my own list of stop words, but I decided it would be more objective to use the commonly used list from the **tm** package. They use the following list of stop words:

```{r stop_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- common_words[1]
for (i in 2:length(common_words)){
  sw <- paste0(sw, ", ", common_words[i])
}
cat("-", sw)
```

I excluded all of these words from the analysis. The intuition here is that including them would mean that the analysis is more sensitive to stylistic ticks than to content, and in practice that seemed to be right. The models did look more reflective of substance than style with the stop words excluded. In principle I'm not sure it was right to exclude all those quantifiers from the end of the list, but it doesn't seem to have hurt the analysis. I'll come back to this point at the end of the chapter, but it is possible I should have been more aggressive in filtering out stop words.

The stop words list from **tm** includes a lot of contractions. I wrote a small script to extract the parts of those contractions before the apostraphe, and excluded them too. The parts after then apostrophe were always one or two letters, so they were already excluded.

I've also looked through the list of the five thousand most common words in the data set to see what shouldn't be there, and the rest of this section comes from what was cut on the basis of that.

In some cases, JSTOR's source for the text was from the LaTeX code for the article, so there was a lot of LaTeX junk in the text file. I'm sure I didn't clean out all of this, but to clean out a lot of it, I deleted the following words.

```{r latex_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- latex_words[1]
for (i in 2:length(latex_words)){
  sw <- paste0(sw, ", ", latex_words[i])
}
cat("-", sw)
```

I'm a bit worried that excluding _document_ meant I lost some signal about historical articles in the LaTeX noise. But this was unavoidable. 

Also note that _anid_ is not a LaTeX term, but it was worthwhile to exclude it here. Something about how the text recognition software JSTOR uses interacted with nineteenth- and early twentieth-century articles meant that several words, especially 'and', got coded as 'anid'. But this was the OCR verison of a typo, and best deleted. (There were a few more of these that were not in the five thousand most common words that on reflection I wish I'd cut too. But I don't think they make a huge difference to the analysis given how rare they are.)

Somewhat reluctantly, I deleted a bunch of spellings out of Greek letters for the same reason; they were mostly from LaTeX code. This meant deleting the following words:

```{r greek_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- greek_words[1]
for (i in 2:length(greek_words)){
  sw <- paste0(sw, ", ", greek_words[i])
}
cat("-", sw)
```

I'm sure this lost some signal. But there was so much LaTeX noise that it was unavoidable.

Next I deleted a few honorifics; in particular:

```{r honorifics, echo=FALSE, cache=TRUE, results='asis'}
sw <- gendered_words[5]
for (i in 6:length(gendered_words)){
  sw <- paste0(sw, ", ", gendered_words[i])
}
cat("-", sw)
```

These just seemed to mark the article as being old, not anything about the content of the article. I didn't need to exclude _mr_ or _dr_ since they were already excluded as too short.

Although I was trying to exclude foreign-language articles, I also excluded a bunch of foreign words. One reason was that it was a check on whether I missed any foreign-language articles. Another was that if I didn't do this, then articles that had extensive quotation from foreign languages would be seen by the model as being in their own distinctive topic merely in virtue of having non-English quotations. And that seemed wrong. So to fix it, I excluded these words:

```{r foreign_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- foreign_words[1]
for (i in 2:length(foreign_words)){
  sw <- paste0(sw, ", ", foreign_words[i])
}
cat("-", sw)
```

Finally, I excluded a bunch of words that seemed to turn up primarily in bibliographies or in text citations. Including them seemed to just make the model be more sensitive to the referencing style of the journal rather than the content. But here the deletions really did cost some content, because some of the words were philosophically relevant. But I deleted them because they seemed to be turning up more often in bibliographies than in text:

```{r ref_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- ref_words[1]
for (i in 2:length(ref_words)){
  sw <- paste0(sw, ", ", ref_words[i])
}
cat("-", sw)
```

The surprising one there is _compilation_. But it most often appears because some journals have a footer saying "Journal compilation ©".

Then to speed up processing, I deleted any word that appeared in any article three times or less This lost some content, but it sped up the processing a lot. Some of the steps I'll describe below took several days computing time. Without this restriction they would have taken several weeks. And I thought words that appear one to three times in an article shouldn't be that significant for determining its content. Though as I'll note below, this might have been too aggressive in retrospect.

## Building a Model

So at this stage we have a list of `r nrow(articles)` articles to include, and a list of several hundred words to exclude. JSTOR provides text files for each article that can easily be converted into a two-column spreadsheet. The first column is a word; the second column is the number of times the word appears. I added a third column for the code number of the article and then merged all the spreadsheets for each article into one giant spreadsheet. (Not for the last time, I used code that was very closely based on code that [John Bernau](https://www.johnabernau.com/about/) built for a similar purpose [@Bernau2018].) Now I had a file that was 137MB large, and had the word counts of all the words in all the articles.

I filtered out the words in all the lists above, and all the words that appeared in an article one to three times. And I filtered out all the articles that weren't on the list of `r nrow(articles)` research articles. This was the master word list I'd work with.

I turned that word list, which at this stage looked like a regular spreadsheet, into something called a document-term-matrix using the ```cast_dtm``` command from Julia Slige and David Robinson's package [tidytext](https://www.rdocumentation.org/packages/tidytext/versions/0.1.3). The DTM format is important only because that's what the [topicmodels](https://cran.r-project.org/web/packages/topicmodels/index.html) package (written by Bettina Grün and Kurt Hornik) takes as input before producing an LDA model as output.

I'm not going to go over the full details of how a Latent Dirichlet Allocation (LDA) model is built, because the description that [Grün and Hornik provide](https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf) is better than what I could do. I'll just note that I'm using the default VEM algorithm.

The basic idea is to use word frequency to estimate which words go in which topics. This makes some amount of sense. Every time the word _Rawls_ appears in an article, that increases the probability that the article is about political philosophy. And every time the word _Bayesian_ appears, that increases the probability that the article is about formal epistemology. These aren't surefire signs, but they are probabilistic signs, and by adding up all these signsthe probability that the article is in one topic rather than another can be worked out.

But what's striking about the LDA method is that the topics are not specified in advance. The model is not told, "Hey, there's this thing called political philosophy, and here are some keywords for it." Rather, the algorithm itself comes up with the topics. This works a little bit by trial and error. The model starts off guessing at a distribution of articles into topics, then works out what words would be keywords for each of those topics, then sees if, given those keywords, it agrees with its own (probabilistic) assignment of articles into topics. It almost certainly doesn't, since the assignment was random, so it reassigns the articles and repeats the process. And this process repeats until it is reasonably satisfied with the (probabilistic) sorting. At that point, it tells us the assignment of articles, and keywords, to topics. (Really though, go see the link above for more details if you want to understand the math.)

The output provides topics, and keywords, but not any further description of the topics. They are just numbered. It might be that topic 52 has a bunch of articles about liberalism and democracy, broadly construed, and has words like _Rawls_, _liberal_, _democracy_, and _democratic_ as keywords, and then we can recognize it as political philosophy. But to the model it's just topic 52.

At this stage there are three big choices the modeler has:

1. How many topics should the articles be divided into?
2. How satsfied shoudl the model be with itself before it reports the data?
3. What random assignment should be used to initialize the algorithm?

Although the algorithm can sort the articles into any number of topics one asks it to, it cannot say what makes for a natural number of topics to use. (There is a caveat to this that I'll get to.) That has to be coded by hand into the request for a model. And it's really the biggest decision to make. The next section discusses how I eventually made it.

## Choosing the Number of Topics {#choose-topic-number}

The model building algorithm automates most of the work; it even chooses what the topics are. But the one thing it doesn't do is choose how many topics there are. You have to specify that in advance. And it's a big choice.

In principle, you can give it as few as two topics to work with. If you ask the model to divide all the articles into two groups, it will usually divide them into something like ethics articles and something like M&E articles. I say 'usually' because it's a fairly random process. And about a quarter of the time, it will find some other way of dividing the articles in two, such as earlier or later, or perhaps things that look maximally like philosophy of science, and maximally unlike philosophy of science. But none of these are helpful models; they tell us more about the nature of the modeling function than they tell us about the history of philosophy.

The topic models package itself comes with a measure that's intended to be used for this purpose. The 'perplexity' function asks the model, in effect, how confused it is by the data once it has built the model.^[You can ask it this about the data that was used to build the model, or hold back some of the data from the model building stage and use it on the held back data. The second probably makes more sense theoretically, but it didn't make a huge difference here.] The thought is that once you've got too many topics, the perplexity score won't change as you add more topics. That's a sign that you've reached a  natural limit. But it didn't help here. As far as I could tell, I could have had something like 400 topics and the perplexity score would still have been falling every time I added more topics. Philosophers are just too idiosyncratic, and you really need to get very very fine-grained topics before the computer is comfortable thinking it has the classifications of articles into topics right.

But a model with 400 topics wouldn't help anyone. (I did build one such model, and the rest of this paragraph is about why I'm not using it.) On it's own, it's too fine-grained to be useful. I don't think anyone would actually read it closely. To make the model human-readable, I'd have to bundle the 400 topics into something like the familiar categories: ethics, metaphysics, philosophy of science, etc. But when I tried to do that, I found just as many edge cases as clear cases. The only data that would come out of this approach that would be legible to humans would be a product of my choices not the underlying model. And the aim was to get my prejudices out of the system as much as possible.

So I needed something more coarse-grained than the model with lowest perplexity, but obviously more fine grained than simply two topics. I ended up doing a lot of trial and error, and looking at how the models came up with different numbers of topics. (This feels like the thing that most people using topic modeling tools end up doing.) 

When I looked at the models that were produced with different numbers of topics, I was generally looking at these four factors. The first two factors push you towards more and more topics. The next two were designed to put downwards pressure on the number of topics.

First, how often did the model come up with topics that simply looked disjunctive? The point of the model is to group the articles into _n_ topics, and hopefully each of these topics has a sensible theme. But sometimes the theme is a disjunction - i.e., the topic consists of papers from philosophical debate X and papers from mostly unrelated debate Y. There are always some of these. Some debates are distinctive enough that the papers within that topic always cluster together - the model can tell that it shouldn't be separating them - but small enough (in these twelve journals) that the model doesn't want to use up a valuable topic on just that debate. There were three of these that almost always came up: feminism, Freud, and vagueness. If you build a model out of these journals with, say, 40 topics, then it is almost certain that three of the topics you'll end up are simply disjunctive, with one of the disjuncts being one of these three topics. My favourite was an otherwise sensible model that decided one of the topics in philosophy consisted of papers on material constitution, and papers on feminist philosophy. Now there are links there - some important feminist theories spend a lot of effort on carefully distinguishing causation from constitution - but it's really a disjunctive topic. And the fewer topics you have, the more disjunctive topics you get. So it's good to get rid of disjunctions, and that's a reason to increase the number of topics.

Second, how often did the model make divisions that cross-cut familiar disciplinary boundaries? Some such divisions are unavoidable, and the model I use ends up with a lot of them. But in the first instance I'd prefer, for example, a model that separates out papers on the metaphysics of causation from papers on the semantics of counterfactuals, to one that puts them together. The debates are obviously closely related - but there was a big advantage to me if they were separated. If they were, then measuring how prominent Metaphysics is in the journals becomes one step easier, and so does measuring how prominent Philosophy of Language is. So I'd rather models that split them up.

Third, how often did the model divide up debates not in terms of what question they were asking, but in terms of what answers they were giving (or at least taking seriously). For instance, sometimes the model would decide to split up work on causation into, roughly, those papers that did and those that did not take counterfactuals to be central to understanding causation. This tracked pretty closely (but not perfectly) the division into papers before and after David Lewis's paper  [Causation](https://philpapers.org/rec/LEWC) [@Lewis1973b]. (Though, amusingly, models that made this division usually put Lewis's own paper into the pre-Lewisian category; which makes sense since most of that paper is about theories of causation that had come before.) This seemed bad - we want a division into _topics_, and different answers to the same question shouldn't count.

Fourth, how often did the model make divisions that only specialists would understand? A bunch of models I looked at divided up, for instance, the philosophy of biology articles along dimensions that I, a non-specialist, couldn't see reason behind. The point of this is not that there are no real divisions there, or that the model was in any sense wrong. It's rather that I want the model to be useful to people across philosophy, and if non-experts can't see what the difference is between two topics just by looking at the headline data about the topic, then it isn't serving its function.

Still, after a lot of trial and error, it seemed like the best balance between these four criteria was hit at around 60 topics. This isn't to say it was perfect. For one thing, even with a fixed number of topics, different model runs produce very different models, and as I'll discuss in [the next section](#model-seed-choice), we have to choose between them. For another, the optimal balance between these criteria would come at different points in different fields. So perhaps at 48 topics you'd see a pretty good balance between these criteria within ethics (broadly construed), but it might be double that before you saw the right balance in philosophy of mind. So there are a lot of trade-offs, as you might expect given that we're trying to detect trends in the absence of anything like clear boundary lines.

But you might notice at this stage something odd. I said that we got the best balance at around 60 topics. Yet the model I've based the book on has 90 topics. How I got to that model involves yet more choices. I think each of the choices I made was defensible, but the reason this chapter is so long is that there really were quite a lot of choices, and I think it's worthwhile to lay them all out.

## Choosing Between The Models {#model-seed-choice}

Even once the number of topics is set, there are still a lot of ways that the model can change. Building a model starts with a somewhat random assignment of words and articles to topics, followed by a series of steps (themselves each involving a degree of randomisation) towards a local equilibrium. But there is a lot of path dependency in this process, as there always is in finding a local equilibrium.

Rather than walk through the mathematics of why this is so, I find it more helpful to think about what the model is trying to achieve, and why it is such a hard thing to achieve. Let's just focus on one subject matter in philosophy, friendship, and think about how we could classify it if we're trying to divide all of philosophy up into 60-90 topics.

It's too small a subject matter to be its own topic. We'll do best if we have the topics be roughly equal size, and discussions that are primarily about friendship are, I'd guess, about 0.001 to 0.002 of the articles in these twelve journals. It's an order of magnitude short of being its own topic. So it has to be grouped in with neighbouring subjects. But which ones? For some subjects, the problem is that there aren't enough natural neighbours. This is why the models never quite know what to do with vagueness, or feminism, or Freud. But here the problem is that there are too many.

One natural enough thing to do is to group papers on friendship with papers on love, and both of them with papers on other emotions, or  perhaps with papers on other reactive attitudes. That gives you a nice set of papers about aspects of the mental lives of humans that are central to actually being human, but not obviously well captured by simple belief-desire models. 

Another natural thing to do is to group papers on friendship with papers on families, and perhaps include both of them in broader discussions of ways in which special connections to particular others should be accounted for in a good ethical theory. Again, you get a reasonably nice set of papers here, with the general theme of special connections to others.

Or yet another natural thing to do is to group papers on friendship with papers on cooperation. And once you're thinking about cooperation, the natural paper to center the topic around is Michael Bratman's very highly cited paper [Shared Cooperative Activity](https://philpapers.org/rec/BRASCA). From there there are a few different ways you could go. You could expand the topic to Bratman's work on intention more broadly, and the literature it has spawned. Or you could expand it to include other work on group action, and even perhaps on group agency. (I teach that Bratman paper in a course on groups and choices, which is centered around game theory. Though I think getting from friendship to game theory in a single one of our 60-90 topics would be a step too far.)

Which of these is right? Well, I saw all of them when I ran the algorithm enough times. And they all seem like sensible choices to me. How should we choose which model to use when different models draw such different boundaries within the space of articles? A tempting thought is to see which one looks most like what one thinks philosophy really looks like, and choose it. But now we're back to imposing our prejudices on the model, rather than letting the model teach us something about the discipline.

A better thing to do is to run the algorithm a bunch of times, and find the output that most commonly appears. Intuitively, we're looking for an equilibrium, and there's something to be said for picking the equilibrium with the largest basin of attraction. This is more or less what I did, though there are two problems. 

The first is 'run the algorithm a bunch of times' is easier said than done. On the computers I was using (pretty good personal computers), it took about 8 hours to come up with a model with 60 topics. So running a bunch of them to find an average was a bit of work. (The University of Michigan has a good unit for doing intensive computing jobs like this. But I kept feeling I was close enough to being done that running things on my own devices was less work than setting up an account there. This ended up being a bad mistake.) But I could just leave them run overnight every night for a couple of weeks, and eventually I had 16 60-topic models to average out.

The models are distinguished by their **seed**. This is a number that you can specify to seed the random number generator. The intended use of it is to make it possible to replicate work like this that relies on randomisation. But it also means that we can run a bunch of models, then make slight changes to the one that seems most representative. And that's what I ended up doing. The seeds I used at this stage were famous dates from the revolutions of 1848. And to get ahead of ourselves, the model the book is based around has seed value 22031848, the date of both the end of the Five Days of Milan, and of the start of the Venetian Revolution.^[Why 1848 and not some other historical event? Well, I had originally been using dates from the French Revolution. But I made so many mistakes that I had to start again. In particular, I didn't learn how many words I needed to filter out, and how many articles I needed to filter out, until I saw how much they were distorting the models. And by that stage I had so many files with names starting with 14071789 and the like that I needed a clean break. So 1848, with all its wins and all its losses, it was.] 

The second is that it isn't obvious how to average them. At one level, what the model produces is a giant probability function. And there is a lot of literature on how to merge probability functions into a single function, or (more or less equivalently), how to find the most representative of a set of probability functions. But this literature assumes that the probability functions are defined over (more or less) the same possibility spaces. And that's precisely what isn't true here. When you build one of these models, what you're left with is a giant probability function all right. But no two model runs give you a function over the same space. Indeed, the most interesting thing about any model is what space it decides is most relevant. So the standard tools for merging probablity functions don't apply.

What I did instead was look for two things. 

The model doesn't just say, this article goes in this topic. It says that this article goes in this topic with probability _p_. Indeed, it gives non-zero probabilities to each article being in each topic. So one thing you can look at for a model is which articles does it think have the highest probability of being in any given topic. That is, roughly speaking, which articles does it think are the paradigms of the different topics it discovers. Then across a range of models, you can ask, how much does this model agree with the other models about which are the paradigm articles. So, for instance, you can find the 10 articles with the highest probability of being in each of the 60 topics. And then you can ask, of the 600 articles that this model thinks are the clearest instance of a particular topic, how many of them are similarly in the 600 articles that other models think are the paradigms of a particular topic. So that was one of the things I looked for - which models had canonical articles that were also canonical articles in a lot of other models.

The models don't just give you probabilistic judgments of an article being in a particular topic, they give you probabilistic judgments of a word being in an article in that topic. So the model might say that the probability of the word 'Kant' turning up in an article in topic 25 is 0.1, while the probability of it turning up in most other topics is more like 0.001. That tells you that topic 25 is about Kant, but it also tells you that the model thinks that 'Kant' is a keyword for a topic. Since some words will turn up frequently in a lot of topics no matter what, you have to focus here not just on the raw probabilities (like the 0.1 above), but on the ratio between the probability of a word being in one topic and it being in others. That tells you how characteristic the word is of the topic. And again you can use this trick to find the 600 characteristic words of a particular model, and ask how often those 600 words are characteristic words of any model at all. There is a lot of overlap here - the vast majority of models have a topic where 'Aristotle' is chaacteristic word in this sense, for example. But there are also idiosyncracies, and the models with the fewest idiosyncracies seem like better bets for being more representative. So that was another thing I looked for - which models had keywords that were also keywords in a lot of other models.

The problem was that these two approaches (and a couple of variations of them that I tried) didn't really pick out a unique model. It told me that three of them were better than the others, but not really which of those three was best. So I chose one in particular. Partially this was because I could convince myself it was a bit better on the two representativeness tests from the last two paragraphs, though honestly the other two would have done just as well. Partially it was because it did better on the four criteria from the previous section. But largely it was because the flaws it had all seemed to go one way; they were all flaws where the model failed to make distinctions I felt it should be making. The other models had a mix; some missing distinctions, but also some needless distinctions. And I felt at the time that having all the errors go one way was a good thing. All I had to do now was run the same model with slightly more topics, and I'd have a really good model. And that sort of worked, though it was more complicated than I'd hoped.

## Two Refinements {#refinements-section}

So now I had a model, with 60 topics, that looked good but not quite right. And, by design, there was a natural way to fix the problems; just add topics. It turns out that if you keep the seed number the same, and just give the model more topics to play with, it makes very few changes. Or, to be a bit more precise, it makes very few changes apart from permuting the numbers. So if you build two models with the same seed, and the second has one more topic than the first, for the vast majority of topics in the first model, there will typically be a 'matching' topic in the second model. And by 'matching' topic here I mean that the correlation between the probabilities the models give to articles being in those topics is very very high, above 0.99 or so. The matching models won't always have the same number, so it isn't always easy to find them. But by simply looking at the correlations between any pairs of topics (one from each model) they usually jumped out.

That meant it was possible every time a few topics were added to simply look at the new topics, and ask if they were improvements or not. In an earlier attempt at this project, one that was fatally undermined by not filtering out enough latex and bibliographic words, this had led to a clear optimum arising around 70 topics. And that's what I expected this time. But it didn't happen.

Instead what happened was that as I kept adding topics, it kept (a) finding relative sensible new topics to add, and (b) not splitting up the topics I really hoped it would split. This was something of a disappointment - the project would have been more manageable for me if the model had found an optimum number of topics in the low 70s or lower. But it simply didn't; by the standards I'd set before looking at the models, they just kept getting better as the number of topics got higher.

Eventually I settled on 90 topics. There was a bit more than I wanted, and I could have gone even higher. But it was starting to get a little more fine-grained than I wanted - we already have three distinct topics in philosophy of biology, for example. Still, the model runs where I asked for 96 topics and then for 100 topics weren't clearly worse than the one with 90 (by the standards I'd set myself). So stopping here was somewhat arbitrary.

Once I had the 90 topic model, it still wasn't perfect. There were a few places where it looked like the model had put some things in very odd spots. Some of this remains in the finished product - the model bundles together some work on probability and coherence with historical work on Hume, and puts one half of the Freud papers with [Medical Ethics](#topic70) and the other half of them with [Intention](#topic48). But at this stage there were more of these overlaps than I liked.

So I relied on one last feature of the **topicmodels** package. The algorithm doesn't stop when it reaches an equilibrium; it stops when it sees insufficient progress towards equilibrium. One thing you can do is refine what counts as 'insufficient', but I found this hard to control. A similar approach is to start not with a random distribution, but with a finished model, and then ask it to approach equilibrim from that starting point. It won't go very far; the model was finished to start with. But it will end up with a model that it likes slightly better. (It will, for example, have a lower perplexity score.) I'll call the resulting model a _refinement_.

The refinement process takes a model as input and returns a model as output, so it can be iterated.^[If you're interested in doing this yourself, the magic code looks like ```refinedlda <- LDA(all_dtm, k = 90, model = refinedlda, control = list(seed = 22031848, verbose = 1, initialize = "model"))```. That is ```refinedlda``` is an LDA that takes the DTM we started with, and has 90 topics, and is based on a model, where that model is ```refinedlda``` itself. If loops don't scare you, you can simply loop this process to get as many iterations of refinement as you like. They took about 45 minutes each to run when I did them.] And at this stage I had a clever thought. Since the refinement process improves the model, and it can be iterated, I should just iterate it as often as I can to get a better and better model. At the back of my mind I had two worries at this point. One was that this was a bit like tightening a string, and if you do it too much it will just snap. The other was that I had lost my mind, and was fretting about mathematical models of large text libraries using half-baked metaphors concerning the physics of everyday objects.

Reader, it snapped.

After 100 iterations, the model ended up making an interesting, and amusing, mistake. 

One signature problem with the kind of text mining I'm doing is that it can't tell the difference between a change of vocabulary that is the result of a change in subject matter, and a change of vocabulary that is the result of a change in verbal fashions. If you build these kind of models with almost any parameter settings, you'll get a distinctive topic (or two) for [ordinary language philosophy](#topic24). Why? Because the language of the ordinary language philosophers was so distinctive. That's not great, but it's unavoidable. Ideally, that would be the only such topic. And one of the reasons I filtered out so many words was to avoid having more such topics.

But it turns out that there is another period with a somewhat distincive vocabulary: the twenty-first century. It's not as distinctive as mid-century British philosophy. And usually it isn't distinctive enough to really confuse most of these models. But it is just distinctive enough that if you run refinements iteratively for, let's say, four days while you're away at a conference, the model will find this distinctive language. So after 100 iterations, we ended up with a model that wasn't a philosophical topic at all, but was characterised by [the buzzwords of recent philosophy](buzzwords-section).

Still, it turns out the refinements weren't all a bad idea. After 15 refinements, the model had separated out some of the disjunctive categories I'd hoped it would, and was only starting to gt thrown by the weird language of very recent philosophy. So that's the model I ended up using - the one with seed 22031848, 90 topics, and 15 iterations of the refinement process.

## The Output

The result of all this is a model with two giant probability functions. In this section I'll talk through what those functions look like with first a worked example, and then some graphs about how well the models perform at their intended task.

The worked example involves David Makinson's article [The Paradox of the Preface](https://philpapers.org/rec/MAKTPO-9) [@Makinson1965]. The input to the model looks like this.

```{r preface-words, cache=TRUE}
preface_words <- word_list %>%
  filter(document == "10.2307_3326519") %>%
  arrange(-wordcount) %>%
  select(word, wordcount)

kable(preface_words, 
      col.names = c("Word", "Wordcount"), 
      caption = "Words in The Paradox of the Preface",
      digits = c(0, 0)) %>% 
  kable_styling(full_width = F)
```

That is, the word 'rational' appears 14 times, 'beliefs' appear 11 times, and so on. This is a list of all of the words in the article, excluding the various stop words described above, and the words that appear one to three times.

The model gives a probability to the article being in each of 90 topics. For this article, as for most articles, it just gives a residual probability to the vast majority of topics. So for 83 topics, the probability it gives to the article being in that topic is about 0.0003. The seven topics it gives a serious probability to are:

```{r preface-topics, cache=TRUE}
preface_topics <- relabeled_gamma %>%
  select(document, topic, gamma) %>%
  filter(document == "10.2307_3326519") %>%
  arrange(-gamma) %>%
  select(topic, gamma) %>%
  filter(gamma > 0.02)

kable(preface_topics, 
      col.names = c("Topic", "Probability"), 
      caption = "Topic Probabilities for The Paradox of the Preface",
      digits = c(0, 4)) %>% 
  kable_styling(full_width = F)
```

I'm going to spend a lot of time in [the next chapter](#all-90-topics) on what these topics are. For now I'll just refer to them by number.

The model also gives a probability to each word turning up in a paradigm article for each of the topics. So for those nineteen words that the model saw as input, we can look at how frequently the model thinks a word should turn up in each of these 7 topics.

```{r preface-large-table, cache=TRUE}
preface_large_table <- relabeled_topics %>%
  select(-date) %>%
  filter(term %in% preface_words$word) %>%
  filter(topic %in% preface_topics$topic) %>%
  arrange(topic) %>%
  mutate(beta = as.character(signif(beta, 2)))

preface_wide_table <- preface_large_table %>%
  pivot_wider(id_cols = term, names_from = "topic", names_prefix = "t", values_from = beta)

kable(preface_wide_table, 
      col.names = c("Word", "Topic 4", "Topic 15", "Topic 37", "Topic 39", "Topic 59", "Topic 76", "Topic 81"), 
      caption = "Word Frequencies for topics in The Paradox of the Preface")
```

But the model doesn't think that "The Paradox of the Preface" is a paradigm case of any one of these topics; it thinks it is a mix of seven. So we can work out what it thinks the word frequences in that article should be by taking weighted means of these columns, with the weights given by the topic probabilities. And we get the following results.

```{r preface-cross-check, cache=TRUE}
overall_sum <- sum(preface_words$wordcount)

preface_check_table <- preface_large_table %>%
  inner_join(preface_topics, by = c("topic")) %>%
  group_by(term) %>%
  mutate(beta = as.numeric(beta)) %>%
  summarise(proj = weighted.mean(beta, gamma)) %>%
  inner_join(preface_words, by = c("term" = "word")) %>%
  mutate(f = wordcount/overall_sum) %>%
  select(term, wordcount, f, proj) %>%
  arrange(-wordcount)

kable(preface_check_table,
      col.names = c("Word", "Wordcount", "Measured Frequency", "Modelled Frequency"),
      digits = c(0, 0, 4, 4),
      caption = "Measured and Modelled Frequencies for The Paradox of the Preface")
```

The modelled frequency of 'rational' is given by multiplying, across seven topics, the probability of the article being in that topic, by the expected frequency of the word given it is in that topic. And the same goes for the other words. What I'm giving here as the measured frequency of a word is not its frequency in the original article; it is its frequency among the words that survive the various filters I described above. In general that will be two to three times as large as its original frequency.

The aim is that the two columns here would line up. And of course they don't. In fact, the model doesn't end up doing very well with this article; it is still a long way from equilibrium.

```{r preface-graph, fig.cap = "Modelled and Measured Frequency for Makinson (1965)", fig.alt = alt_text, cache=TRUE}
cross_check_graph <- function(x){
  temp_gamma <- relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document"))
  
  temp_words <- word_list %>%
    filter(document == x)
  
  total_words = sum(temp_words$wordcount)
  
  temp_words <- temp_words %>%
    mutate(f = wordcount/total_words)
  
  temp_topics <- relabeled_topics %>%
    select(-date) %>%
    filter(term %in% temp_words$word) %>%
    inner_join(temp_gamma, by = "topic") %>%
    group_by(term) %>%
    summarise(proj = weighted.mean(beta, gamma)) %>%
    arrange(-proj) %>%
    inner_join(temp_words, by = c("term" = "word"))
  
  salient_words <- temp_topics %>%
    mutate(outlier = f + proj) %>%
    arrange(-outlier) %>%
    slice(1:5)
  
  high_number <- max(salient_words$f, salient_words$proj)
  
print(  ggplot(temp_topics, aes(x = f, y=  proj)) + 
    spaghettistyle +
    geom_point(size = 0.5, alpha = 0.5) +
    coord_fixed(xlim = c(0, high_number * 1.02), ylim = c(0, high_number * 1.02)) +
    labs(caption = temp_gamma$citation[1],
        x = "Measured Word Frequency",
        y = "Modelled Word Frequency")  +
    ggrepel::geom_text_repel(data = salient_words, aes(label = term))
)
}

salient_words <- function(x){
    temp_gamma <- relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document"))
  
  temp_words <- word_list %>%
    filter(document == x)
  
  total_words = sum(temp_words$wordcount)
  
  temp_words <- temp_words %>%
    mutate(f = wordcount/total_words)
  
  temp_topics <- relabeled_topics %>%
    select(-date) %>%
    filter(term %in% temp_words$word) %>%
    inner_join(temp_gamma, by = "topic") %>%
    group_by(term) %>%
    summarise(proj = weighted.mean(beta, gamma)) %>%
    arrange(-proj) %>%
    inner_join(temp_words, by = c("term" = "word"))
  
  the_words <- temp_topics %>%
    mutate(outlier = f + proj) %>%
    arrange(-outlier) %>%
    slice(1:5)
  
  paste(the_words$term, collapse = ", ")
}

# Burp

x <- "10.2307_3326519"

cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x-axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y-axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are rarely met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is well below the 45 degree line. ",
  "That means they appear in the article more often than the model expects. ",
  "The word belief only appears a bit more often than expected, then others appear much more often."
)
```

On that graph, every dot is a word type. The x-axis represents the frequency of that word type in the article (after excluding the stop words and so on), and the y-axis represents how frequently the model thinks the word 'should' appear, given its classification of the article into 90 topics, and the frequency of words in those topics. Ideally, all the dots would be on the 45 degree line coming north-east out of the origin. Obviously, that doesn't happen. It can't really, because, to a very rough approximation, I've only given the model 90 degrees of freedom, and I've asked it to approximate over 32,000 data points.

Actually, this is one of the least impressive jobs the model does. I measured the correlations between measured and modelled word frequency, i.e., what this graph represents, for 600 highly cited articles. Among those 600, this was the 23rd lowest correlation between measured and modelled frequency. But in many cases, that correlation was very strong. For example, here are the graphs for three more articles where the model manages to understand what's happening.

```{r good-graph-1, fig.cap = "Modelled and Measured Frequency for Davidson (1990)", fig.alt = alt_text, cache=TRUE}
x <- "10.2307_2026863"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x-axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y-axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are usually met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is close to the 45 degree line. ",
  "That means they appear in the article about as often than the model expects. ",
  "The word truth appears a lot; it is 6% of the words in the article. The model expects a little less; around 5%. The others are very close to the 45 degree line."
)
```

```{r good-graph-2, fig.cap = "Modelled and Measured Frequency for Edgington (1995)", fig.alt = alt_text, cache=TRUE}
x <- "10.2307_2254793"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x-axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y-axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are usually met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is close to the 45 degree line. ",
  "That means they appear in the article about as often than the model expects."
)

```

```{r good-graph-3, fig.cap = "Modelled and Measured Frequency for Dworkin (1996)", fig.alt = alt_text, cache=TRUE}
x <- "10.2307_2961920"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x-axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y-axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are usually met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is close to the 45 degree line. ",
  "That means they appear in the article about as often than the model expects.",
  " The word moral appears a lot, about 4% of all words in the article. And the model predicts this correctly."
)
```

There are some articles that it doesn't manage as well, typically articles with unusual words. (It also does poorly with short articles, like "The Paradox of the Preface".)

```{r bad-graph-1, fig.cap = "Modelled and Measured Frequency for Thomson (1998)", fig.alt = alt_text, cache=TRUE}
x <- "10.2307_2671962"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x-axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y-axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are rarely met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is far frothe 45 degree line. ",
  "The model expects the words property and properties will appear a lot, 3-4% of the time, but they make up only about 1% of the words. It does not expect the words time, part and, especially, clay, to appear as often as they do."
)
```

```{r bad-graph-2, fig.cap = "Modelled and Measured Frequency for Elster (1990)", fig.alt = alt_text, cache=TRUE}
x <- "10.2307_2381783"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x-axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y-axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are rarely met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is far from to the 45 degree line. ",
  "The model expects the words society and social to appear more often than they do. But it is very surprised at how often the words honor, norms, and revenge, appear."
)
```

```{r bad-graph-3, fig.cap = "Modelled and Measured Frequency for Fara (2005)", fig.alt = alt_text, cache=TRUE}
x <- "10.2307_3506173"

cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x-axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y-axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are rarely met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Four of them are far from the 45 degree line. ",
  "The model expects the words possible, worlds and, especially, world, to appear much more often than they do.",
  " But it expects the word disposition to appear much less. ",
  "The model does correctly predict that the word true will appear about 2% of the time."
)
```

A few different things are going on here. In Elster's article, the model doesn't expect any philosophy article to use the word 'revenge' as much as he does. In Fara's article, the model lumps articles about modality (especially possible worlds) in with articles on dispositions. (This ends up being [Topic 80](#topic80).) And so it expected that Fara will talk about worlds, given he is also talking about dispositions, but he doesn't. Thomson's article has both of these features. The model is surprised that anyone is talking about clay so much. And it expects that a metaphysics article like Thomson's will talk about properties more than Thomson does.

So it isn't perfect, but as we saw above, it does pretty well with some cases. The papers I've shown so far are pretty much outliers though; here are some more typical examples.

```{r medium-graph-1, fig.cap = "Modelled and Measured Frequency for Lewis (1979)", fig.alt = alt_text, cache=TRUE}
x <- "10.2307_2215339"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x-axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y-axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are mostly met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Four of them are close to the 45 degree line. ",
  "But the model expects the word laws to appear much more often than it does."
)
```

```{r medium-graph-2, fig.cap = "Modelled and Measured Frequency for Lakoff and Johnson (1980)", fig.alt = alt_text, cache=TRUE}
x <- "10.2307_2025464"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x-axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y-axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are mostly met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Four of them are close to the 45 degree line. ",
  "That means they appear in the article about as often than the model expects.",
  " But the model does not expect the word metaphor to appear so often."
)
```

```{r medium-graph-3, fig.cap = "Modelled and Measured Frequency for Kelly (2003)", fig.alt = alt_text, cache=TRUE}
x <- "10.2307_20140564"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x-axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y-axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are rarely met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Belief and reason are far above the 45 degree line, epistemic and rationality far below it. ",
  "The model expects the word reasons to make up 2% of the words in the article, and in fact it makes up 3%."
)
```

It's not perfect, but the general picture is that the model does a pretty good job of modeling 32000 articles given the tools we offered it. And, more importantly from the perspective of this book, the way it models them ends up grouping like articles together. And that's what I'll use for describing trends in the journals over their first 138 years.

## Strengths and Weaknesses {#strengths-and-weaknesses-section}

The benefit of using this kind of modeling is that it allows you to take every article into account. This is the history of philosophy (in these journals) without any gaps whatsoever.

And this is no small feat. Remember that there are `r nrow(articles)` articles that we're looking at. Let's say that you could dedicate 8 hours a day, 5 days a week, just to reading these articles, and that you could on average read an article per hour. Some, to be sure, would take less than an hour even to read closely. But just one hour is an optimistic reading time for the longer articles. Still, let's make the optimistic assumption. That would mean `r round(nrow(articles)/40, 0)` weeks of just to read through them all. If you take 2 weeks a year off, you would take `r round(nrow(articles)/2000, 0)`  years just to do the reading. And at the end of that time, you'd at best have some sketchy notes on the articles, not anything you can use for an analysis.

If you want to analyse all the articles, if you want to really have no gaps, then the only way to do it is by machine.

But there are a number of downsides to this algorithmic approach, all of which come from the fact that the machine is just doing string recognition. The algorithm doesn't know any semantics, just syntax. And this causes some complications. I'll mention five here, along with a brief discussion of how badly they impacted the model I ended up using.

One problem that turned out not to be too big a deal was that the algorithm has a hard time distinguishing different uses of the same word. But while this is hard, it isn't impossible. The model seems, for example, to understand the difference between how 'function' is used in philosophy of biology, to how it is used in logic and mathematics. It didn't run together the different uses of 'realism', or 'internalism'/'externalism', like I would have expected. There is a hint of running together 'scepticism' in the sense most relevant to epistemology with other kinds of philosophical scepticism. (Someone who is a free will sceptic doesn't say we don't know whether free will exists, but that we know it doesn't.) But maybe this isn't too much of a problem, since the views aren't that separate.

The one time that this particular model seems to have gotten confused over the two related meanings of a word concerned 'free'. [Topic 35](#topic35) is a mishmash of work on free will, with work on political freedom. Now you might think this isn't too bad, since the subjects are somewhat connected. But it's not optimal, and we'll eventually work out a way to separate out free will and political freedom. But the big picture is that something that seemed likely to be a problem turned out, pleasingly, to not be that bad.

A second problem comes from the reverse direction. Sometimes the differences in topics just come from a change in terminology. You can see this most clearly, I think, in the logic topics in the model. Papers about sequents get put in a different topic to papers about syllogisms. Papers about implications get put in a different topic from papers about validities. Now there is a sense in which that's a good thing, and the model is picking up a philosophically significant change. But it's a relatively minor change compared to what the model thinks. Still, this isn't a particularly serious problem. The worst case scenario is that we have to come back in after and manually note that we should put together the papers on validities and papers on implications when we're doing analysis. That's a bit of work but it isn't too bad, we just have to remember that it happens.

A third, and related, problem, comes from when the model makes fine-grained distinctions within a subject. I mentioned earlier that I saw several models that ended up separating out work on causation that didn't discuss counterfactuals (like Mackie's work) from post-Lewisian work where counterfactuals are front and center. That's not great - these really are on the same topic - but it isn't too bad. Again, worst case scenario is you combine these topics by hand when doing analysis. But in practice I don't think we really saw this problem arise in this particular run of the model.

A potentially bigger problem is the converse, which I already discussed when talking about choosing the number of topics. Sometimes the topics are just disjunctive. For example, [Topic 37](#topic37) ends up being half about sets, and half about the grue paradox. Now there is a connection of sorts here - Nelson Goodman is kind of important to both literatures. But really this shouldn't be a single topic. As I already noted, this is a hard problem to fix. If you increase the number of topics, the model becomes harder to read, and you're just as likely to split a coherent topic (like causation) as to split a disjunctive topic.

I did three things here to address these disjunctive topics. One, that I've already mentioned, was to keep running refinements until the worst of the disjunctiveness was polished away. (Before the refinements, some papers on probabilistic epistemology got classified in with papers on Hume, and I don't know what the computer was thinking. A handful ended up there after the refinements, but not nearly as many.) A second is to use very clear labels for the topics, like "Sets and Grue", to indicate that it is a disjunctive topic. And a third is to run a further analysis on articles in that topic to divide up the sets articles from the grue articles. Eventually there ended up being 10 topics where I felt this kind of split was worthwhile.

The fifth and final problem is that the algorithm can't tell changes of topic apart from changes in style. If it becomes a requirement on all right-thinking philosophers to express onself more or less exclusively in monosyllables, as seems to have been the case in mid-century Britain, then the algorithm will think that there is a new topic that is being discussed right then. I'm exaggerating of course about mid-century Britain, but there is a trend that matters, and that I'll talk much more about later. 

Or imagine what would happen if every philosopher all at once decided that you shouldn't respond to objections with a new theory that has distinctive consequences, but instead you should respond to *worries* with a new *account* that has distinctive *commitments*. Well, the model will think that there is this cool new subject about 'worries', 'accounts', and 'commitments', and that you're all talking about it. And if this stylistic change happens all at once across philosophy, the model will think that the generalist journals, the philosophy of science journals, and the moral and political journals, are all obsessed all of a suddent with the worry/account/commitment subject. Of course, philosophy couldn't be so caught up chasing trends that something like this would all happen at once, could it? Could it? Let's return to this issue [at the very end](#buzzwords-section), and see how bad things got.

## Regrets {#regrets-section}

So that's the methodology I used. Now that I've written the whole thing up, there are a few things I wish I'd done differently. This wish clearly isn't strong enough to make me scrap the project and start again.^[I did in fact scrap several versions of this when writing up the model revealed mistakes in the model building. This is the model that resulted from acting on the lessons of those mistakes.] But I hope that others will learn from what I've done, and to that end I want to be upfront about things I could have done differently.

First, I should have filtered even more words. There are four kinds of words I wish I'd been more aggressive about filtering out.

- There are some systematic OCR errors in the early articles. I caught 'anid', which appears over 3000 times. (It's almost always meant to be 'and', I think.) But I missed 'aind', which appears about 1500 times. And there are other less common words that are also OCR errors and should be filtered out.
- I caught a lot of latex words, but somehow missed 'rightarrow', as well as a few much rarer words.
- If a word is hyphenated in the original journal, each half appears as a word in this data set. (At least if the data was generated by OCR.) I caught a few of the prefixes and suffixes that turn up for that reason, but missed 'ity', which ends up being a reasonably common word.
- And I caught a lot of words that almost always appear in bibliographies, headers or footers, but missed 'basil' (which turns up on a table later) and 'noûs' (though I caught 'nous').

In general I could have been way more aggressive filtering words like these out.

But second, I think it was a mistake to filter out words that appear 1-3 times in articles. This actually makes perfect sense for long articles, and for some really long articles you could get rid of words that appear 4 or 5 times as well. But it's too aggressive for short articles. I needed some kind of rule like filtering out words that appear less than 1 time in 2000 in the article. It is important, I think, to filter out the words that appear just once, or else you have to be perfect in catching OCR errors and weird latex code. But after that you need some kind of sliding scale.

The next three things are much more systematic, though also less clearly errors.

The third problem was that my model selection was too stepwise, and not holistic enough. I found the best 60 topic model I could find. Then I increased the topics on it (eventually to 90) until the topics looked as good as they could get holding fixed the seed number from the search through 60 topics. Then I ran refinements on it until the refinements looked like they were damaging the model. Then I split some of the topics up for categorisation. What I didn't do at any step was look back and ask, for example, how would the other 60 topic models look if I applied these adjustments to them?

Now there was a reason for that. Each of those adjustments cost quite a lot of my time, and even more computer time. Doing the best you can at each step and then locking in the result makes the process at least a bit manageable. But I should (a) have been a bit more willing to revisit earlier decisions, and (b) more forward looking when making each of those intermediate decisions. I was a bit forward looking at one point; one of my criteria for choosing between 60 topic models was a preference for unwanted conflations over unwanted splits. And that was because I knew I could fix conflations various ways. But I should have been both more forward looking, and more willing to take a step or two backwards. And maybe I could have stuck much closer to 60 topics if I had.

The fourth problem was that I didn't realise how bad a topic [Arguments](#topic55) would turn out to be. For the purposes of the kind of study I'm doing, it's really important that the topics really be _topics_ in the ordinary sense, and not tools or methods. Now this is hard in philosophy, because philosophy is so methodologically self-conscious that there are articles that really are about all the tools and methods you might care about. But I wish I'd avoided having a topic about a tool. (I'll come back in section \@ref(raw-weight-count) to a formal method one can use for detecting these kinds of topics early in the process.)

The fifth problem, if it is a problem, is that I wasn't more aggressive about expanding the list of stop words. This model has a topic on [Ordinary Language Philosophy](#topic24). Actually, _all_ the models I built had a topic like this (at least once they had at least 15 or so topics). But the keywords characteristic of this topic are words that really could have been included on a stop words list. They are words like 'ask' and 'try'. And one side-effect of this is that the model keeps thinking a huge proportion of the articles in the data set are maybe kind of Ordinary Language Philosophy articles.

Another way to put this is that the boundary between a stop word and a contentful word (in this context) is pretty vague. And given that Ordinary Language Philosophy was a thing that happened, and that affected how everyone (at least in the UK) was writing for a while, there is a good case for taking a very expansive understanding of what the stop words were.

The choice I made was to not lean on the scales at all, and just use the most common off-the-shelf list of stop words. And there was a good reason for that; I wanted the model to not simply replicate my prejudices. But I half-think I made the wrong call here, and that the model would be more useful if I had filtered out more 'ordinary language'.
