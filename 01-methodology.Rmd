# Methodology {#methodology-chapter}

The point of this chapter is to explain how I chose the model that the book is based around. But to understand the choices that I made, it helps to know a little bit about what a Latent Dirolecht Algorithm (LDA) does.

The inputs to the model are some texts, and a number. The model doesn't care about the ordering of words in the texts, so really the input isn't texts, but a list of lists of ordered pairs. Each ordered pair is a word and a number. In the version I'm using, the outer list is a list of philosophy articles. And each element of that list is a list of words in that article, along with the number of times the word appears.

Along with that, you give the model a number. This is the number of _topics_ that you want the model to divide the texts into. I'll call this number $t$ in this introduction. And intuitively there is a function $T$ that maps articles into the $t$ topics. 

What the model outputs is, for our purposes, a pair of probability functions - one for articles, and one for words.

The probability function for articles gives, for each article $a$ and topic number $n \in \{1, \dots, t\}$, a probability for $T(a) = n$. That is, it gives a probability that the article is in that topic. Notably, it doesn't identify the topics with any more than numbers. I'm going to give names to the topics - this one is [Kant](#topic32), this one is [Composition and Constitution](#topic89), etc. - but the model doesn't do that. For it, the topics really are just integers between 1 and $t$.

The probability function for words gives, for each word $w$ from any of the articles, and topic number $n \in \{1, \dots, t\}$, the probability that a randomly chosen word from the articles in that topic is $w$. So in the Kant topic, the probability that a randomly chosen word is 'Kant' is about 0.14. 

That number feels absurdly high, but it makes sense for a couple of reasons. One is that to make the models compile in even semi-reasonable time, I filtered out a lot of words. What's it's really saying is that the word 'Kant' produces about 1/7 of the tokens that remain. The other is that what it's really giving you here is the probability that a random word in an article is 'Kant' conditional on the probability of that article being in the [Kant](#topic32) is 1. And in fact the model is never that confident. Even for articles that you or I would say are unambiguously articles about Kant, the model is rarely more than 40% confident that that's what they are about. And this is for a good reason. Most articles about Kant in philosophy journals are, naturally enough, about Kantian philosophy. And any part of Kantian philosophy is, well, philosophy. So the model has a topic on [Beauty](#topic08), and when it sees an article on Kantian aesthetics, it gives some probability the correct classification of that article is in the topic on Beauty. So the word probabilities are quite abstract things - they are something like word frequencies in a caricature of an article in a given topic, and all real articles are mixtures of these caricatures.

The way the model works is by building these pairs of functions, checking that they cohere, and recursively refining them in places that they don't cohere. The model wants, as much as possible, the word frequencies in the input to match the modelled frequencies you get by looking at the probability an article is in a particular topic, combined with the word frequencies for each of those topics. This is strictly speaking impossible; there aren't enough degrees of freedom. But it can minimise the error, and it does so recursively.

The process involved is slow. I was able to build all the models I'll discuss on personal computers, but it takes some processing time. The particular model I'm primarily using took about 20 hours to build, but I ran through many more hours than that building other models to compare it to.

And the process is very path dependent. The algorithm, like many algorithms, has the basic structure of pick a somewhat random starting point, then look for a local equilibrium. That's incredibly dependent on how you start, and somewhat dependent on how you travel.

The point of this chapter is to describe how I chose the inputs to the model I ended up using, and then how I set various parameters within the model. The parameters are primarily, in terms of the metaphor of the previous paragraph, the starting point of the search, and how long the search should go before we decide we're at something close enough to an equilibrium.

The inputs are more complex. Very roughly, the inputs I used are the frequently occurring contentful words from research articles in twelve important philosophy journals. I'll start by talking about how and why I selected the particular twelve journals that I did.

## Selecting the Twelve Journals

This is a study about the trajectory of topics across leading philosophy journals. But presumably most people reading this aren't interested in journals as such; they are interested in the trajectory of philosophy. So it is important to select, as far as possible, journals that accurately reflect what's going on in philosophy.

An obvious idea would be to just use generalist journals, because they will reflect what's generally happening in philosophy. But this turns out to be a bad idea, since there really aren't any generalist journals in philosophy. Perhaps because the journals in moral and political philosophy, and in philosophy of science, are so good, the so-called 'generalist' journals tend to under-represent work in those fields. Or, perhaps more precisely, they don't always reflect the cutting edge work in those fields.

In [previous work](http://tar.weatherson.org/2017/04/26/citation-patterns-across-journals/), I noted how little attention the leading generalist journals had paid to two of the most important late twentieth century articles, Elizabeth Anderson's [What is the Point of Equality?](https://philpapers.org/rec/ANDWIT), and Peter Machamer, Lindley Darden and Carl Craver's [Thinking about Mechanisms](https://philpapers.org/rec/MACTAM). Note that these papers each have over 2500 Google Scholar citations, but they have barely been mentioned in the leading generalist journals. So to get an accurate picture of philosophy, you need to include at least some specialist journals.

As a reminder, here are the journals that I've included.

<br>
```{r journaltablereprise, echo=FALSE, cache=TRUE}
  kable(journals_summary, 
        col.names = c("Journal", "First Year", "Number of Articles"), 
        align=c("l", "c", "c")
  )
```
<br>

As you can see, there are two moral and political journals, _Ethics_ and _Philosophy and Public Affairs_, and two philosophy of science journals, _Philosophy of Science_ and _British Journal for the Philosophy of Science_. I could possibly have got by with just one of each. But I thought _Ethics_ and _P&PA_ brought in different fields of philosophy, and so they were both worth including. And that meant it would be good to balance them with two philosophy of science journals. This had the side benefit that I didn't have to decide which of those two philosophy of science journals was more representative.

But if I had those four 'specialist' journals, I needed enough 'generalist' journals that what I had felt representative of philosophy as a whole. Partially to get the balance right, and partially to make the graphs look nice, it felt like I needed eight more journals. I started with the current 'big four' journals.

- Mind
- The Philosophical Review
- Journal of Philosophy
- Noûs

I added Analysis because I wanted to be very sensitive to trends, and it feels (to me at least), like Analysis is often ahead of the trends in the field. That leaves three more spots. Here were the criteria I used to fill those.

- The data for the journal had to be available through JSTOR's Data for Reseachers. This was non-negotiable since that was my data source. But it was unfortunate, since it ruled out the Australasian Journal of Philosophy, which would otherwise have been perfect.
- The journal had to have been publishing for a long time. It wouldn't help balance much to add a journal that launched in the 1990s. This ruled out Philosophical Studies. That's a bit of a shame since you can't tell the story of 21st century philosophy without it. But it just doesn't have enough history for these purposes.
- The journal had to not be too idiosyncratic. I wanted this to tell us something about the field, not just about those journals. This ruled out The Monist, which was very idiosyncratic in its early years. In recent years it is idiosyncratic in a good way; highlighting work the others sometimes overlook. But pre-WWII it is barely a philosophy journal in any recognisable sense.
- The journal had to not be too much a philosophy of science journal, since the aim is to balance the two philosophy of science journals we have.
- The journal had to primarily publish in English, since the analysis tools I'm using simply don't work for cross-linguistic data sets. The last two criteria ruled out Synthese and Erkenntnis.

After all that, I was left with

- Philosophy and Phenomenological Research. This has slightly more non-English work than is ideal for current purposes, but I thought adding a little continental philosophy from its early years was worthwhile. And it became such an important journal that it felt wrong to leave it out.
- Proceedings of the Aristotelian Society. Note that I'm including the supplementary volumes here.^[And including them causes a few headaches. The articles in the supplementary volumes often have respondants. When they do, the metadata often lists both the original article and the reply article as co-authored. This is bizarre, though even more bizarre is that often the running head on the print article does the same thing. This can totally mess you up if you're ever calculating philosophical Erdös numbers. (Which you probably know, because you probably have calculated them.) This study isn't tracking authors, so it isn't too painful. But I am using auto-generated citations as a way of picking out articles, and they will sometimes look co-authored when they are really not. I'm not going to try fixing this; it's just a weirdness that we'll live with.] This is idiosyncratic in its early years; some secretaries of the Aristotelian Society have a bigger impact on this study than they do on the field. But without it you miss so much of British philosophy, including some themes in contemporary philosophy that aren't always covered in the other major journals.
- Philosophical Quarterly. For much of the twentieth century, this is much less prestigious than the other 11 journals I'm looking at. And this will become relevant when I look at the citation data. But it fits the other criteria very well. It adds a Scottish journal to the English and American journals I am otherwise looking at. It has slightly better history coverage than the other journals. And since I worked at St Andrews for so many years, I'm personally fond of it.

You might wonder why I didn't add any other specialist journals, along with ethics and philosophy of science. The reasons were a bit varied.

I think you probably do get a better sense of mid-century philosophy if you add one logic journal. But you can't do text mining on symbols. And in more recent years the sense in which the logic journals are primarily philosophy journals as opposed to mathematics journals has gotten weaker. So I left them out.

The twelve journals I have don't include as much history of philosophy as there is in the profession. But that's simply unavoidable if you're doing a study based on journals. History of philosophy is primarily a book discipline not a journal discipline. You can see this in the citation data. Pick almost any prominent figure in history of philosophy, and odds are that I'll have several journal articles with more citations than their most cited article. Though, of course, they will have several books that are much more widely cited than anything of mine. Just as importantly, when a history article is widely cited, it usually appears in one of the twelve journals I've already included. For this reason, the model that I end up working with does have a lot of history categories. We just have to remember that the absolute numbers of articles in each of these categories is not representative of how important the categories are in philosophy.

And the other specialist journals are either too new (e.g., Mind and Language, or Linguistics and Philosophy) or representative of too small a section of contemporary philosophy to be worth including. Aesthetics, for example, is an important philosophical field. (And it shows up in the model in an interesting way.) But including the Journal of Aesthetics and Art Criticism in the study would have made it look like aesthetics was one-thirteenth of the field. And that's misleading. So I stuck with these twelve.

## Selecting the Articles

Journals publish a lot, and I had to decide what to include and what to leave out. The aim was to include all and only research articles, but this was harder than it looks.

The metadata that JSTOR provides includes a tag for article kind. I only included articles with the tag "research-article", which does a reasonable job of getting rid of book reviews. But it turns out that it leaves in a lot.

I deleted all articles without a listed author. These were often editorials, corrections and the like.

After that, I started working through various words in titles that indicated something was not actually a research article. So I deleted all articles with these titles.

- Descriptive Notices
- Editorial
- Letter to Editor
- Letter
- Introduction

The first four are clear enough. The last was mostly a problem for special issues, but there were enough special issues of one kind or another to make it worthwhile. Then I deleted any articles that had the following phrases anywhere in the title.

- Correction
- Foreword
- Introductory Note
- Errat
- Erata
- Abstract of C
- Abstracts of C
- To the Editor
- Corrigenda
- Obituary
- Congress

The last is the only one that really needs comment. All the articles I found with this in the title were reports on one or other Philosophy Congress, not genuine research articles. Maybe there was a political philosophy article that referenced the United States Congress in its title, and should not have been excluded, but I didn't see it.

Since text mining only works within a single language, I excluded all the articles whose listed language in the metadata was anything other than English. And I manually excluded, when I saw them, articles whose title was not in English, and which seemed like non-English articles.

That left me with `r nrow(articles)` articles to work with.

## Selecting the Words {#stop-words}

The JSTOR data excludes a few stop words (like 'the' and 'and'), and words with 1 or 2 characters. On the other hand, it takes non-letters to be word breaks. So _doesn't_ would be split into _doesn_ and _t_, and the second rejected as too short. And hyphenated words are split as well. It turned out that this made _est_ into a reasonably common word. But I didn't want to include all the words for various reasons.

It seems common in text mining to exclude a more expansive list of 'stop words' than JSTOR leaves out. I was playing around with making my own list of stop words, but I decided it would be more objective to use the commonly used list from the 'tm' package. They use the following list of stop words.

```{r stop_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- common_words[1]
for (i in 2:length(common_words)){
  sw <- paste0(sw, ", ", common_words[i])
}
cat("-", sw)
```

I excluded all these words from the analysis. The intuition here is that including them would mean that the analysis is more sensitive to stylistic ticks than to content, and in practice that seemed to be right. The models did look more reflective of substance than style with the stop words excluded. In principle I'm not sure it was right to exclude all those quantifiers from the end of the list. But it doesn't seem to have hurt the analysis.

The stop words list from tm includes a lot of contractions. I wrote a small script to extract the parts of those contractions before the apostraphe, and excluded them too. The parts after then apostraphe were always 1 or 2 letters, so they were already excluded.

I've also looked through the list of the 5000 most common words in the data set to see what shouldn't be there, and the rest of this section comes from what was cut on the basis of that.

In some cases, JSTOR's source for the text was from the LaTeX code for the article, so there was a lot of LaTeX junk in the text file. I'm sure I didn't clean out all of this, but to clean out a lot of it, I deleted the following words.

```{r latex_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- latex_words[1]
for (i in 2:length(latex_words)){
  sw <- paste0(sw, ", ", latex_words[i])
}
cat("-", sw)
```

I'm a bit worried that excluding 'document' meant I lost some signal about historical articles in the LaTeX noise. But this was unavoidable. 

Also note that 'anid' is not a LaTeX term, but it was worthwhile to exclude it here. Something about how the text recognition software JSTOR uses interacted with 19th and early 20th century articles meant that several words, especially 'and', got coded as 'anid'. But this was the OCR verison of a typo, and best deleted. (There were a few more of these that were not in the 5000 most common words that on reflection I wish I'd cut too. But I don't think they make a huge difference to the analysis given how rare they are.)

Somewhat relunctantly, I deleted a bunch of spellings out of Greek letters for the same reason; they were mostly from LaTeX code. This meant deleting the following words.

```{r greek_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- greek_words[1]
for (i in 2:length(greek_words)){
  sw <- paste0(sw, ", ", greek_words[i])
}
cat("-", sw)
```

I'm sure this lost some signal. But there was so much LaTeX noise that it was unavoidable.

Next I deleted a few honorifics, in particular:

```{r honorifics, echo=FALSE, cache=TRUE, results='asis'}
sw <- gendered_words[5]
for (i in 6:length(gendered_words)){
  sw <- paste0(sw, ", ", gendered_words[i])
}
cat("-", sw)
```

These just seemed to mark the article as being old, not anything about the content of the article. I didn't need to exclude 'mr.' or 'dr.' since they were already excluded as too short.

Although I was trying to exclude foreign language articles, I also excluded a bunch of foreign words. One reason was that it was a check on whether I missed any foreign language articles. Another was that if I didn't do this, then articles that had extensive quotation from foreign languages would be seen by the text miner as in their own distinctive class merely in virtue of having non-English quotations. And that seemed wrong. So to fix it, I excluded these words.

```{r foreign_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- foreign_words[1]
for (i in 2:length(foreign_words)){
  sw <- paste0(sw, ", ", foreign_words[i])
}
cat("-", sw)
```

Finally, I excluded a bunch of words that seemed to turn up primarily in bibiographies, or in text citations. Including them seemed to just make the model be more sensitive to the referencing style of the journal, rather than the content. But here the deletions really did cost some content, because some of the words really were philosophically relevant. But I deleted them because they seemed to be turning up more often in bibliograhies than in text.

```{r ref_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- ref_words[1]
for (i in 2:length(ref_words)){
  sw <- paste0(sw, ", ", ref_words[i])
}
cat("-", sw)
```

The surprising one there is 'compliation'. But it most often appears because some journals have a footer saying "Journal compilation © ...".

Then to speed up processing, I deleted any word that appeared in any article three times or fewer. This did lose some content. But it sped up the processing a lot. Some of the steps I'll describe below took several days computing time. Without this restriction they would have taken several weeks. And words that appear 1-3 times in an article shouldn't be that significant for determining its content.

## Building a Model

So at this stage we have a list of `r nrow(articles)` to include, and a list of several hundred words to exclude. JSTOR provides text files for each article that can easily be converted to a two column spreadsheet. The first column is a word, the second column is the number of times the word appears. I added a third column, for the code number of the article, and then merged all the spreadsheets for each article into one giant spreadsheet. (Not for the last time, I used code that was very closely based on code that [John Bernau](https://www.johnabernau.com/about/) built for a similar purpose.) Now I had a file that was 137MB large, and had the word counts of all the words in all the articles.

I filtered out the words in all the lists above, and all the words that appeared in an article 1-3 times. And I filtered out all the articles that weren't on the list of `r nrow(articles)` research articles. This was the master word list I'd work with.

I turned that word list, which at this stage looked like a regular spreadsheet, into something called a document-term-matrix using the 'cast_dtm' command from Julia Slige and David Robinson's package [tidytext](https://www.rdocumentation.org/packages/tidytext/versions/0.1.3). The DTM format is important only because that's what the [topicmodels](https://cran.r-project.org/web/packages/topicmodels/index.html) package (written by Bettina Grün and Kurt Hornik) takes as input before producing an LDA model as output.

I'm not going to go over the full details of how a Latent Dirichlet Allocation (LDA) model is built, because the description that [Grün and Hornik provide](https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf) is better than what I could do. (I'll just note that I'm using the default VEM algorithm.) 

The basic idea is to use word frequency to estimate which words go in which topics. This makes some amount of sense. Every time the word 'Rawls' appears in an article, that increases the probability that the article is about political philosophy. And every time the word 'Bayesian' appears, that increases the probability that the article is about formal epistemology. These aren't sure-fire signs, but they are probabilistic signs, and by adding up all these signs, we can work out the probability that the article is in one topic rather than another.

But what's striking about the LDA method is that we don't specify in advance what the topics are. We don't tell it, "Hey, there's this thing called political philosophy, and here are some keywords for it." Rather, the algorithm itself comes up with the topics. This works a little bit by trial and error. The model starts off guessing at a distribution of articles into topics, then works out what words would be keywords for each of those topics, then sees if, given those keywords, it agrees with its own (probabilistic) assignment of articles into topics. It almost certainly doesn't, since the assignment was random, so it reassigns the articles, and repeats the process. And this process repeats until it is is reasonably satisfied with the (probabilistic) sorting. At that point, it tells us the assignment of articles, and keywords, to topics. (Really though, go see the link above for more details if you want to understand the math.)

The output provides topics, and keywords, but not any further description of the topics. They are just numbered. It might be that topic 52 has a bunch of articles about liberalism and democracy, broadly construed, and has words like 'Rawls', 'liberal', 'democracy', 'democratic', as keywords, and then we can recognise it as political philosophy. But to the model it's just topic 52.

At this stage there are three big choices the modeller has.

1. How many topics to divide the articles into.
2. How satisfied the model should be with itself before it reports the data.
3. What random assignment should be used to initialise the algorithm.

Although the algorithm can sort the articles into any number of topics one asks it to, it cannot tell you what makes for a natural number of topics to use. (There is a caveat to this that I'll get to.) That has to be hand-coded into the request for a model. And it's really the biggest decision that we have to make. The next section discusses how I eventually made it.

## Choosing the Number of Topics {#choose-topic-number}

The model building algorithm automates most of the work; it even chooses what the topics are. But the one thing it doesn't do is choose how many topics there are. You have to specify that in advance. And it's one of the biggest choice points there is.

In principle, you can give it as few as two topics to work with. If you ask the model to divide all the articles into two groups, it will usually divide them into something like ethics articles and something like M&E articles. I say 'usually' because it's a fairly random process. And about a quarter of the time, it will find some other way of dividing the articles in two, such as earlier or later, or perhaps something that keys off the distinctiveness of philosophy of science. But there isn't a lot of information here.

The topic models package itself comes with a measure that's intended to be used for this purpose. The 'perplexity' function asks the model, in effect, how confused it is by the data once it has built the model.^[You can ask it this about the data that was used to build the model, or hold back some of the data from the model building stage and use it on the held back data. The second probably makes more sense theoretically, but it didn't make a huge difference here.] The thought is that once you've got too many topics, the perplexity score won't change as you add more topics. That's a sign that you've reached a  natural limit. But it didn't help here. As far as I could tell, I could have had something like 400 topics and the perplexity score would still have been falling every time I added more topics. Philosophers are just too idiosyncratic, and you really need to get very very fine-grained topics before the computer is comfortable thinking it has the classifications of articles into topics right.

But a model with 400 topics wouldn't help anyone. (I did build one such model, and the rest of this paragraph is about why I'm not using it.) On it's own, it's too fine-grained to be useful. I don't think anyone would actually read it closely. To make the model human-readable, I'd have to bundle the 400 topics into something like the familiar categories: ethics, metaphysics, philosophy of science, etc. But when I tried to do that, I found just as many edge cases as clear cases. The only data that would come out of this approach that would be legible to humans would be a product of my choices not the underlying model. And the aim was to get my prejudices out of the system as much as possible.

So I needed something more coarse-grained than the model with lowest perplexity, but obviously more fine grained than simply two topics. I ended up doing a lot of trial and error, and looking at how the models came up with different numbers of topics. (This feels like the thing that most people using these tools end up doing.) 

When I looked at the models that were outputted with different numbers of topics, I was generally looking at these four factors. The first two factors push you towards more and more topics. The next two were designed to put downwards pressure on the number of topics.

First, how often did the model come up with topics that simply looked disjunctive? The point of the model is to group the articles into _n_ topics, and hopefully each of these topics has a sensible theme. But sometimes the theme is a disjunction - i.e., the topic consists of papers from philosophical debate X and papers from mostly unrelated debate Y. There are always some of these. Some debates are distinctive enough that the papers within that topic always cluster together - the model can tell that it shouldn't be separating them - but small enough (in these twelve journals) that the model doesn't want to use up a valuable topic on just that debate. There were three of these that almost always came up: feminism, Freud, and vagueness. If you build a model out of these journals with, say, 40 topics, then it is almost certain that three of the topics you'll end up are simply disjunctive, with one of the disjuncts being one of these three topics. My favourite was an otherwise sensible model that decided one of the topics in philosophy consisted of papers on material constitution, and papers on feminist philosophy. Now there are links there - some important feminist theories spend a lot of effort on carefully distinguishing causation from constitution - but it's really a disjunctive topic. And the fewer topics you have, the more disjunctive topics you get. So it's good to get rid of disjunctions.

Second, how often did the model make divisions that cross-cut familiar disciplinary boundaries? Some such divisions are unavoidable, and the model I use ends up with a lot of them. But in the first instance I'd prefer a model that separates out papers on the metaphysics of causation from papers on the semantics of counterfactuals to one that puts them together. The debates are obviously closely related - but if I can have them separated so I can track the relative frequency of articles in metaphysics vs articles in philosophy of language more closely. So I'd rather models that split them up.

Third, how often did the model divide up debates not in terms of what question they were asking, but in terms of what answers they were giving (or at least taking seriously). For instance, sometimes the model would decide to split up work on causation into, roughly, those papers that did and those that did not take counterfactuals to be central to understanding causation. This tracked pretty closely (but not perfectly) the division into papers before and after [Lewis (1973)](https://philpapers.org/rec/LEWC). (Though, amusingly, models that made this division usually put Lewis's own paper into the pre-Lewisian category; which makes sense since most of that paper is about theories of causation that had come before.) This seemed bad - we want a division into _topics_, and different answers to the same question shouldn't count.

Fourth, how often did the model make divisions that only specialists would understand? A bunch of models I looked at divided up, for instance, the philosophy of biology articles along dimensions that I, a non-specialist, couldn't see reason behind. The point of this is not that there are no real divisions there, or that the model was in any sense wrong. It's rather that I want the model to be useful to people across philosophy, and if non-experts can't see what the difference is between two topics just by looking at the headline data about the topic, then it isn't serving its function.

To be sure, I think I got worse at detecting this feature as I worked on the project. The more of these models I looked at, and the more reading of the underlying articles I did to understand why they were making the divisions they were making, the less good I got at deciding which divisions seemed arbitrary to outsiders. So I suspect I didn't enforce this criteria as strictly as I might have when building the models.

Still, after a lot of trial and error, it seemed like the best balance between these four criteria was hit at around 60 topics. This isn't to say it was perfect. For one thing, even with a fixed number of topics, different model runs produce very different models, and as I'll discuss in [the next section](#model-seed-choice), we have to choose between them. For another, the optimal balance between these criteria would come at different points in different fields. So perhaps at 48 topics you'd see a pretty good balance between these criteria within ethics (broadly construed), but it might be double that before you saw the right balance in philosophy of mind. So there are a lot of trade-offs, as you might expect given that we're trying to detect trends in the absence of anything like clear boundary lines.

But you might notice at this stage something odd. I said that we got the best balance at around 60 topics. Yet the model I've based the book on has 90 topics. How I got to that model involves yet more choices. I think each of these are defensible, but the reason this chapter is so long is that there are a lot of choice points along the way, and I think it's worthwhile to lay them all out.

## Choosing Between The Models {#model-seed-choice}

Even once setting the number of topics, there are still a lot of ways that the model can change. Building a model starts with a somewhat random assignment of words and articles to topics, followed by a series of steps (themselves each involving a degree of randomisation) towards a local equilibrium. But there is a lot of path dependency in this process, as there always is in finding a local equilibrium.

Rather than walk through the mathematics of why this is so, I find it more helpful to think about what the model is trying to achieve, and why it is such a hard thing to achieve. Let's just focus on one subject matter in philosophy, friendship, and think about how we could classify it if we're trying to divide all of philosophy up into 60-90 topics.

It's too small a subject matter to be its own topic. We'll do best if we have the topics be roughly equal size, and discussions that are primarily about friendship are, I'd guess, about 0.001 to 0.002 of the articles in these twelve journals. It's an order of magnitude short of being its own topic. So it has to be grouped in with neighbouring subjects. But which ones? For some subjects, the problem is that there aren't enough natural neighbours. This is why the models never quite know what to do with vagueness, or feminism, or Freud. But here the problem is that there are too many.

One natural enough thing to do is to group papers on friendship with papers on love, and both of them with papers on other emotions, or other reactive attitudes. That gives you a nice set of papers about aspects of the mental lives of humans that are central to actually being human, but not obviously well captured by simple belief-desire models. 

Another natural thing to do is to group papers on friendship with papers on families, and perhaps include both of them in broader discussions of ways in which special connections to particular others should be accounted for in a good ethical theory. Again, you get a reasonably nice set of papers here, with the general theme of special connections to others.

Or yet another natural thing to do is to group papers on friendship with papers on cooperation. And once you're thinking about cooperation, the natural paper to center the topic around is Michael Bratman's very highly cited paper [Shared Cooperative Activity](https://philpapers.org/rec/BRASCA). From there there are a few different ways you could go. You could expand the topic to Bratman's work on intention more broadly, and the literature it has spawned. Or you could expand it to include other work on group action, and even perhaps on group agency. (I teach that Bratman paper in a course on groups and choices, which is centered around game theory. Though I think getting from friendship to game theory in a single one of our 60-90 topics would be a step too far.)

Which of these is right? Well, I saw all of them when I ran the algorithm enough times. And they all seem like sensible choices to me. How should we choose which model to use when different models draw such different boundaries within the space of articles? A tempting thought is to see which one looks most like what one thinks philosophy really looks like, and choose it. But now we're back to imposing our prejudices on the model, rather than letting the model teach us something about the discipline.

A better thing to do is to run the model a bunch of times, and find the one that is most representative. This is what I did, though there are two problems. 

The first is 'run a bunch of times' is easier said than done. On the computers I was using (pretty good personal computers), it took about 8 hours to come up with a model with 60 topics. So running a bunch of them to find an average was a bit of work. (The University of Michigan has a good unit for doing intensive computing jobs like this. But I kept feeling I was close enough to being done that running things on my own devices was less work than setting up an account there. This ended up being a bad mistake.) But I could just leave them run overnight every night for a couple of weeks, and eventually I had 16 60-topic models to average out.

The models are distinguished by their _seed_. This is a number that you can specify to seed the random number generator. The intended use of it is to make it possible to replicate work like this that relies on randomisation. But it also means that we can run a bunch of models, then make slight changes to the one that seems most representative. And that's what I ended up doing. The seeds I used at this stage were famous dates from the revolutions of 1848. And to get ahead of ourselves, the model the book is based around has seed value 22031848, the date of both the end of the Five Days of Milan, and of the start of the Venetian Revolution.^[Why 1848 and not some other historical event? Well, I had originally been using dates from the French Revolution. But I made so many mistakes that I had to start again. In particular, I didn't learn how many words I needed to filter out, and how many articles I needed to filter out, until I saw how much they were distorting the models. And by that stage I had so many files with names starting with 14071789 and the like that I needed a clean break. So 1848, with all its wins and all its many losses, it was.] 

The second is that it isn't obvious how to average them. At one level, what the model produces is a giant probability function. And there is a lot of literature on how to merge probability functions into a single function, or (more or less equivalently), how to find the most representative of a set of probability functions. But this literature assumes that the probability functions are defined over (more or less) the same possibility spaces. And that's precisely what isn't true here. When you build one of these models, what you're left with is a giant probability function all right. But no two model runs give you a function over the same space. Indeed, the most interesting thing about any model is what space it decides is most relevant. So the standard tools for merging probablity functions don't apply.

What I did instead was look for two things. The model doesn't just say, this article goes in this topic. It says that this article goes in this topic with probability _p_. Indeed, it gives non-zero probabilities to each article being in each topic. So one thing you can look at for a model is which articles does it think have the highest probability of being in any given topic. That is, roughly speaking, which articles does it think are the paradigms of the different topics it discovers. Then across a range of models, you can ask, how much does this model agree with the other models about which are the paradigm articles. So, for instance, you can find the 10 articles with the highest probability of being in each of the 60 topics. And then you can ask, of the 600 articles that this model thinks are the clearest instance of a particular topic, how many of them are similarly in the 600 articles that other models think are the paradigms of a particular topic.

The models don't just give you probabilistic judgments of an article being in a particular topic, they give you probabilistic judgments of a word being in an article in that topic. So the model might say that the probability of the word 'Kant' turning up in an article in topic 25 is 0.1, while the probability of it turning up in most other topics is more like 0.001. That tells you that topic 25 is about Kant, but it also tells you that the model thinks that 'Kant' is a keyword for a topic. Since some words will turn up frequently in a lot of topics no matter what, you have to focus here not just on the raw probabilities (like the 0.1 above), but on the ratio between the probability of a word being in one topic and it being in others. That tells you how characteristic the word is of the topic. And again you can use this trick to find the 600 characteristic words of a particular model, and ask how often those 600 words are characteristic words of any model at all. There is a lot of overlap here - the vast majority of models have a topic where 'Aristotle' is chaacteristic word in this sense, for example. But there are also idiosyncracies, and the models with the fewest idiosyncracies seem like better bets for being more representative.

The problem was that these two approaches (and a couple of variations of them that I tried) didn't really pick out a unique model. It told me that three of them were better than the others, but not really which of those three was best. So I chose one in particular. Partially this was because I could convince myself it was a bit better on representativeness grounds, though honestly the other two would have done just as well. Partially it was because it did better on the four criteria from the previous section. But largely because the flaws it had all seemed to go one way; they were all flaws where the model failed to make distinctions I felt it should be making. The other models had a mix; some missing distinctions, but also some needless distinctions. And I felt at the time that having all the errors go one way was a good thing. All I had to do now was run the same model with slightly more topics, and I'd have a really good model. And that sort of worked, though it was more complicated than I'd hoped.

## Two Refinements {#refinements-section}

So now I had a model, with 60 topics, that looked good but not quite right. And, by design, there was a natural way to fix the problems; just add topics. It turns out that if you keep the seed number the same, and just give the model more topics to play with, it makes very few changes. Or, to be a bit more precise, it makes very few changes apart from permuting the numbers. So if you build two models with the same seed, and the second has one more topic than the first, for the vast majority of topics in the first model, there will typically be a 'matching' topic in the second model. And by 'matching' topic here I mean that the correlation between the probabilities the models give to articles being in those topics is very very high, above 0.99 or so. The matching models won't always have the same number, so it isn't always easy to find them. But by simply looking at the correlations between any pairs of topics (one from each model) they usually jumped out.

That meant it was possible every time a few topics were added to simply look at the new topics, and ask if they were improvements or not. In an earlier attempt at this project, one that was fatally undermined by not filtering out enough latex and bibliographic words, this had led to a clear optimum arising around 70 topics. And that's what I expected this time. But it didn't happen.

Instead what happened was that as I kept adding models, it kept (a) finding relative sensible new topics to add, and (b) not splitting up the topics I really hoped it would split. This was something of a disappointment - the project would have been more manageable for me if the model had found an optimum number of topics in the low 70s or lower. But it simply didn't; by the standards I'd set before looking at the models, they just kept getting better as the number of topics got higher.

Eventually I settled on 90 topics. There was a bit more than I wanted, and I could have gone even higher. But it was starting to get a little more fine-grained than I wanted - we already have three distinct topics in philosophy of biology, for example. Still, the model runs where I asked for 96 topics and then for 100 topics weren't clearly worse than the one with 90 (by the standards I'd set myself). So stopping here was somewhat arbitrary.

Once I had the 90 topic model, it still wasn't perfect. There were a few places where it looked like the model had put some things in very odd spots. Some of this remains in the finished product - the model bundles together some work on probability and coherence with historical work on Hume. But at this stage there were more of these overlaps than I liked.

So I relied on one last feature of the topicmodels package. The algorithm doesn't stop when it reaches an equilibrium; it stops when it sees insufficient progress towards equilibrium. One thing you can do is refine what counts as 'insufficient', but I found this hard to control. A similar approach is to start not with a random distribution, but with a finished model, and then ask it to approach equilibrim from that starting point. It won't go very far; the model was finished to start with. But it will end up with a model that it likes slightly better. (It will, for example, have a lower perplexity score.) I'll call the resulting model a _refinement_.

The refinement process takes a model as input and returns a model as output, so it can be iterated. And at this stage I had a clever thought. Since the refinement process improves the model, and it can be iterated, I should just iterate it as often as I can to get a better and better model. At the back of my mind I had two worries at this point. One was that this was a bit like tightening a string, and if you do it too much it will just snap. The other was that this was a silly worry, and that metaphorical reasoning about strings has no place when thinking about mathematical models of large text libraries.

Reader, it snapped.

After 100 iterations, the model ended up making an interesting, and amusing, mistake. 

One signature problem with the kind of text mining I'm doing is that it can't tell the difference between a change of vocabulary that is the result of a change in subject matter, and a change of vocabulary that is the result of a change in verbal fashions. If you build these kind of models with almost any parameter settings, you'll get a distinctive topic (or two) for [ordinary language philosophy](#topic24). Why? Because the language of the ordinary language philosophers was so distinctive. That's not great, but it's unavoidable. Ideally, that would be the only such topic. And one of the reasons I filtered out so many words was to avoid having more such topics.

But it turns out that there is another period with a somewhat distincive vocabulary: the twenty-first century. It's not as distinctive enough to really confuse most of these models. But it is just distinctive enough that if you run refinements iteratively for, let's say, four days while you're away at a conference, the model will find this distinctive language. So after 100 iterations, we ended up with a model that wasn't a philosophical topic at all, but was characterised by [the buzzwords of recent philosophy](buzzwords-section).

Still, it turns out the refinements weren't all a bad idea. After 15 refinements, the model had separated out some of the disjunctive categories I'd hoped it would, and was only starting to gt thrown by the weird language of very recent philosophy. So that's the model I ended up using - the one with random number seed 22031848, 90 topics, and 15 iterations of the refinement process.

## The Output

The result of all this is a model with two giant probability functions. These in turn create 180 conditional probability functions (2 for each of the 90 topics), and it's those that we'll use. In this section I'll talk through what those functions look like with first a worked example, and then some graphs about how well the models perform at their intended task

The worked example involves David Makinson's article [The Paradox of the Preface](https://philpapers.org/rec/MAKTPO-9). The input to the model looks like this.

<br>
```{r preface-words}
preface_words <- word_list %>%
  filter(document == "10.2307_3326519") %>%
  arrange(-wordcount) %>%
  select(word, wordcount)

kable(preface_words, 
      col.names = c("Word", "Wordcount"), 
      caption = "Words in The Paradox of the Preface",
      digits = c(0, 0)) %>% 
  kable_styling(full_width = F)
```
<br>

That is, the word 'rational' appears 14 times, 'beliefs' appear 11 times, and so on. This is a list of all of the words in the article, excluding the various stop words described above, and the words that appear one to three times.

The model gives a probability to the article being in each of 90 topics. For this article, as for most articles, it just gives a residual probability to the vast majority of topics. So for 83 topics, the probability it gives to the article being in that topic is about 0.0003. The seven topics it gives a serious probability to are:

<br>
```{r preface-topics}
preface_topics <- relabeled_gamma %>%
  select(document, topic, gamma) %>%
  filter(document == "10.2307_3326519") %>%
  arrange(-gamma) %>%
  select(topic, gamma) %>%
  filter(gamma > 0.02)

kable(preface_topics, 
      col.names = c("Topic", "Probability"), 
      caption = "Topic Probabilities for The Paradox of the Preface",
      digits = c(0, 4)) %>% 
  kable_styling(full_width = F)
```
<br>

I'm going to spend a lot of time in [the next chapter](#all-90-topics) on what these topics are. For now I'll just refer to them by number.

The model also gives a probability to each word turning up in a paradigm article for each of the topics. So for those nineteen words that the model saw as input, we can look at how frequently the model thinks a word should turn up in each of these 7 topics.

<br>
```{r preface-large-table}
preface_large_table <- relabeled_topics %>%
  select(-date) %>%
  filter(term %in% preface_words$word) %>%
  filter(topic %in% preface_topics$topic) %>%
  arrange(topic) %>%
  mutate(beta = as.character(signif(beta, 2)))

preface_wide_table <- preface_large_table %>%
  pivot_wider(id_cols = term, names_from = "topic", names_prefix = "t", values_from = beta)

kable(preface_wide_table, 
      col.names = c("Word", "Topic 4", "Topic 15", "Topic 37", "Topic 39", "Topic 59", "Topic 76", "Topic 81"), 
      caption = "Word Frequencies for topics in The Paradox of the Preface")
```
<br>

But the model doesn't think that "The Paradox of the Preface" is a paradigm case of any one of these topics; it thinks it is a mix of seven. So we can work out what it thinks the word frequences in that article should be by taking weighted means of these columns, with the weights given by the topic probabilities. And we get the following results.

<br>
```{r preface-cross-check}
overall_sum <- sum(preface_words$wordcount)

preface_check_table <- preface_large_table %>%
  inner_join(preface_topics, by = c("topic")) %>%
  group_by(term) %>%
  mutate(beta = as.numeric(beta)) %>%
  summarise(proj = weighted.mean(beta, gamma)) %>%
  inner_join(preface_words, by = c("term" = "word")) %>%
  mutate(f = wordcount/overall_sum) %>%
  select(term, wordcount, f, proj) %>%
  arrange(-wordcount)

kable(preface_check_table,
      col.names = c("Word", "Wordcount", "Measured Frequency", "Modelled Frequency"),
      digits = c(0, 0, 4, 4),
      caption = "Measured and Modelled Frequencies for The Paradox of the Preface")
```
<br>

The modelled frequency of 'rational' is given by multiplying, across seven topics, the probability of the article being in that topic, by the expected frequency of the word given it is in that topic. And the same goes for the other words. What I'm giving here as the measured frequency of a word is not its frequency in the original article; it is its frequency among the words that survive the various filters I described above. In general that will be two to three times as large as its original frequency.

The aim is that the two columns here would line up. And of course they don't. In fact, the model doesn't end up doing very well with this article; it is still a long way from equilibrium.

```{r preface-graph, fig.cap = "Modelled and Measured Frequency for Makinson (1965)"}
cross_check_graph <- function(x){
  temp_gamma <- relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document"))
  
  temp_words <- word_list %>%
    filter(document == x)
  
  total_words = sum(temp_words$wordcount)
  
  temp_words <- temp_words %>%
    mutate(f = wordcount/total_words)
  
  temp_topics <- relabeled_topics %>%
    select(-date) %>%
    filter(term %in% temp_words$word) %>%
    inner_join(temp_gamma, by = "topic") %>%
    group_by(term) %>%
    summarise(proj = weighted.mean(beta, gamma)) %>%
    arrange(-proj) %>%
    inner_join(temp_words, by = c("term" = "word"))
  
  salient_words <- temp_topics %>%
    mutate(outlier = f + proj) %>%
    arrange(-outlier) %>%
    slice(1:5)
  
  high_number <- max(salient_words$f, salient_words$proj)
  
print(  ggplot(temp_topics, aes(x = f, y=  proj)) + 
    spaghettistyle +
    geom_point(size = 0.5, alpha = 0.5) +
    coord_fixed(xlim = c(0, high_number * 1.02), ylim = c(0, high_number * 1.02)) +
    labs(caption = temp_gamma$citation[1], x = "Measured Word Frequency", y = "Modelled Word Frequency") +
    ggrepel::geom_text_repel(data = salient_words, aes(label = term))
)
}

cross_check_graph("10.2307_3326519")
```

On that graph, every dot is a word type. The x-axis represents the frequency of that word type in the article (after excluding the stop words and so on), and the y-axis represents how frequently the model thinks the word 'should' appear, given its classification of the article into 90 topics, and the frequency of words in those topics. Ideally, all the dots would be on the 45 degree line coming north-east out of the origin. Obviously, that doesn't happen. It can't really, because, to a very rough approximation, I've only given the model 90 degrees of freedom, and I've asked it to approximate over 32,000 data points.

Actually, this is one of the least impressive jobs the model does. I measured the correlations between measured and modelled word frequency, i.e., what this graph represents, for 600 highly cited articles. Among those 600, this was the 23rd lowest correlation between measured and modelled frequency. But in many cases, that correlation was very strong. For example, here are the graphs for three more articles where the model manages to understand what's happening.

```{r good-graph-1, fig.cap = "Modelled and Measured Frequency for Davidson (1990)"}
cross_check_graph("10.2307_2026863")
```

```{r good-graph-2, fig.cap = "Modelled and Measured Frequency for Edgington (1995)"}
cross_check_graph("10.2307_2254793")
```

```{r good-graph-3, fig.cap = "Modelled and Measured Frequency for Dworkin (1996)"}
cross_check_graph("10.2307_2961920")
```

There are some articles that it doesn't manage as well, typically articles with unusual words. (It also does poorly with short articles, like "The Paradox of the Preface".)

```{r bad-graph-1, fig.cap = "Modelled and Measured Frequency for Thomson (1998)"}
cross_check_graph("10.2307_2671962")
```

```{r bad-graph-2, fig.cap = "Modelled and Measured Frequency for Elster (1990)"}
cross_check_graph("10.2307_2381783")
```

```{r bad-graph-3, fig.cap = "Modelled and Measured Frequency for Fara (2005)"}
cross_check_graph("10.2307_3506173")
```

A few different things are going on here. In Elster's article, the model doesn't expect any philosophy article to use the word 'revenge' as much as he does. In Fara's article, the model lumps articles about modality (especially possible worlds) in with articles on dispositions. (This ends up being [Topic 80](#topic80).) And so it expected that Fara will talk about worlds, but he doesn't. Thomson's article has both of these features. The model is surprised that anyone is talking about clay so much. And it expects that a metaphysics article like Thomson's will talk about properties more than Thomson does.

So it isn't perfect, but as we saw above, it does pretty well with some cases. The papers I've shown so far are pretty much outliers though; here is what more typical articles look like.

```{r medium-graph-1, fig.cap = "Modelled and Measured Frequency for Lewis (1979)"}
cross_check_graph("10.2307_2215339")
```

```{r medium-graph-2, fig.cap = "Modelled and Measured Frequency for Lakoff and Johnson (1980)"}
cross_check_graph("10.2307_2025464")
```

```{r medium-graph-3, fig.cap = "Modelled and Measured Frequency for Kelly (2003)"}
cross_check_graph("10.2307_20140564")
```

It's not perfect, but the general picture is that the model does a pretty good job of modelling 32000 articles using 90 probability functions. And, more importantly from the perspective of this book, the way it models them ends up grouping like articles together. And that's what I'll use for describing trends in the journals over their first 138 years.

## Strengts and Weaknesses {#strengths-and-weaknesses-section}

The benefit of using this kind of modelling is that it allows you to take every article into account. This is the history of philosophy (in these journals) without any gaps whatsoever.

And this is no small feat. Remember that there are `r nrow(articles)` articles that we're looking at. Let's say that you could dedicate 8 hours a day, 5 days a week, just to reading these articles, and that you could on average read an article per hour. Some, to be sure, would take less than an hour even to read closely, but this is a fairly optimistic assumption for some of the longer articles. That would mean `r round(nrow(articles)/40, 0)` weeks of just to read through them all. If you take 2 weeks a year off, you would take `r round(nrow(articles)/2000, 0)`  years just to do the reading. And at the end of that time, you'd at best have some sketchy notes on the articles, not anything you can use for an analysis.

If you want to analyse all the articles, if you want to really have no gaps, then the only way to do it is by machine.

But there are a number of downsides to this algorithmic approach, all of which come from the fact that the machine is just doing string recognition. The algorithm doesn't know any semantics, just syntax. And this causes some complications. I'll mention five here, along with a brief discussion of how badly they impacted the model I ended up using.

One problem that I expected to find more was that the algorithm would run together different uses of the same word. But there was less of this than I feared. It seems, for example, to understand the difference between how 'function' is used in philosophy of biology, to how it is used in logic and mathematics. It didn't run together the different uses of 'realism', or 'internalism'/'externalism', like I would have expected. There is a hint of running together 'scepticism' in the sense most relevant to epistemology with other kinds of philosophical scepticism. (Someone who is a free will sceptic doesn't say we don't know whether free will exists, but that we know it doesn't.) But maybe this isn't too much of a problem, since the views aren't that separate.

The one time that this particular model seems to have gotten confused over the two related meanings of a word concerned 'free'. Topic 35 is a mishmash of work on free will, with work on political freedom. Now you might think this isn't too bad, since the subjects are somewhat connected. But it's not optimal, and we'll eventually work out a way to separate out free will and political freedom. Still, it's nice that something that was potentially a problem turned out not to be too bad.

A second problem comes from the reverse direction. Sometimes the differences in topics just come from a change in terminology. You can see this most clearly, I think, in the logic topics in the model. The changing terminology isn't entirely philosophically neutral, so separating out sequents from syllogisms, or implications from validities, isn't completely useless. But it isn't the most important distinction to make either. But this isn't a particularly serious problem. At some level we can think of the topics as like variable names. And like variable names, they sometimes pick out the same thing. That's fine, we just have to remember that it happens.

A third, and related, problem, comes from when the model makes fine-grained distinctions within a subject. It doesn't really happen here, but a lot of model runs ended up separating out work on causation that didn't discuss counterfactuals (like Mackie's work) from post-Lewisian work where counterfactuals are front and center. (Amusingly, it would often put Lewis's original paper in the pre-counterfactual category, since it spends so much time discussing non-counterfactual theories of causation.) Even in principle this isn't the biggest problem, because again you can think of these topics as distinct variables with a common referent. But in practice it doesn't really seem to have been a big problem in this model run.

A potentially bigger problem is the converse, which I already discussed when talking about choosing the number of topics. Sometimes the topics are just disjunctive. For example, topic 37 ends up being half about sets, and half about the grue paradox. Now there is a connection of sorts here - Nelson Goodman is kind of important to both literatures. But really this shouldn't be a single topic. As I already noted, this is a hard problem to fix. If you increase the number of topics, the model becomes harder to read, and you're just as likely to split a coherent topic (like causation) as to split a disjunctive topic.

I did three things here to address these disjunctive topics. One, that I've already mentioned, was to keep running refinements until the worst of the disjunctiveness was polished away. (Before the refinements, some papers on probabilistic epistemology got classified in with papers on Hume, and I don't know what the computer was thinking.) A second is to use very clear labels for the topics, like "Sets and Grue", to indicate that it is a disjunctive topic. And a third is to run a further analysis on articles in that topic to divide up the sets articles from the grue articles. Eventually there ended up being 10 topics where I felt this kind of split was worthwhile. Though to be sure, in many cases is was to separate out ethics work from social and political work, and you may feel this isn't a division that was worth making in the first place.

The fifth and final problem is that the algorithm can't tell changes of topic apart from changes in style. If it becomes a requirement on all right-thinking philosophers to express onself more or less exclusively in monosyllables, as seems to have been the case in mid-century Britain, then the algorithm will think that there is a new topic that is being discussed right then. I'm exaggerating of course about mid-century Britain, but there is a trend that matters, and that I'll talk much more about later. 

Or imagine what would happen if every philosopher all at once decided that you shouldn't respond to objections with a new theory that has distinctive consequences, but instead you should respond to *worries* with a new *account* that has distinctive *commitments*. Well, the model will think that there is this cool new subject about 'worries', 'accounts', and 'commitments', and that you're all talking about it. And if this stylistic change happens all at once across philosophy, the model will think that the generalist journals, the philosophy of science journals, and the moral and political journals, are all obsessed all of a suddent with the worry/account/commitment subject. Of course, philosophy couldn't be so caught up chasing trends that something like this would all happen at once, could it? Could it? Let's return to this issue [at the very end](#buzzwords-section), and see how bad things got.

## Regrets {#regrets-section}

So that's the methodology I used. Now that I've written the whole thing up, there are a few things I wish I'd done differently. I don't so strongly wish this that I decided to scrap the whole project and start again.^[I went through several cycles of building a model, writing it up, seeing mistakes that way, and restarting the process. This is the model that survived.] But I hope that others will learn from what I've done, and to that end I want to be upfront about my mistakes.

First, I should have filtered even more words. There are three kinds of words I wish I'd been more aggressive about filtering out.

- There are some systematic OCR errors in the early articles. I caught 'anid', which appears over 3000 times. (It's almost always meant to be 'and', I think.) But I missed 'aind', which appears about 1500 times. And there are other less common words that are also OCR errors and should be filtered out.
- I caught a lot of latex words, but somehow missed 'rightarrow', as well as a few much rarer words.
- And I caught a lot of words that almost always appear in bibliographies, headers or footers, but missed 'basil' (which turns up on a table later) and 'noûs' (though I caught 'nous').

In general I could have been way more aggressive filtering words like these out.

But second, I think it was a mistake to filter out words that appear 1-3 times in articles. This actually makes perfect sense for long articles, and for some long articles you could get rid of words that appear 4 or 5 times as well. But it's too aggressive for short articles. I needed some kind of rule like filtering out words that appear less than 1 time in 2000 in the article. It is important, I think, to filter out the words that appear just once, or else you have to be perfect in catching OCR errors and weird latex code. But after that you need some kind of sliding scale.

The next three things are much more systematic, though also less clearly errors.

The third problem was that my model selection was too stepwise, and not holistic enough. I found the best 60 topic model I could find. Then I increased the topics on it (eventually to 90) until the topics looked as good as they could get. Then I ran refinements on it until the refinements looked like they were damaging the model. Then I split some of the topics up for categorisation. What I didn't do at any step was look back and ask, for example, how would the other 60 topic models look if I applied these adjustments to them?

Now there was a reason for that. Each of those adjustments cost quite a lot of my time, and even more computer time. Doing the best you can at each step and then locking in the result makes the process at least a bit manageable. But I should (a) have been a bit more willing to revisit earlier decisions, and (b) more forward looking when making each of those intermediate decisions. I was a bit forward looking at one point; one of my criteria for choosing between 60 topic models was a preference for unwanted conflations over unwanted splits. And that was because I knew I could fix conflations various ways. But I should have done more of this. And maybe I could have stuck much closer to 60 topics if I had.

The fourth problem was that I didn't realise how bad a topic [Arguments](#topic55) would turn out to be. For the purposes of the kind of study I'm doing, it's really important that the topics really be _topics_ in the ordinary sense, and not tools or methods. Now this is hard in philosophy, because philosophy is so methodologically self-conscious that there are articles that really are about all the tools and methods you might care about. But I wish I'd avoided making one of them a topic. (I'll come back in section \@ref(raw-weight-count) to a formal method one can use for avoiding these kinds of topics.)

The fifth problem, if it is a problem, is that I wasn't more aggressive about expanding the list of stop words. This model as a topic on [Ordinary Language Philosophy](#topic24). Actually, _all_ the models I built had a topic like this (at least once they had at least 15 or so topics.) But the keywords characteristic of this topic are words that really could have been included on a stop words list. They are words like 'ask' and 'try'. And one side-effect of this is that the model keeps thinking a huge proportion of the articles in the data set are maybe kind of Ordinary Language articles.

Another way to put this is that the boundary between a stop word and a contentful word (in this context) is pretty vague. And given that Ordinary Language Philosophy was a thing that happened, and that affected how everyone (at least in the UK) was writing for a while, there is a good case for taking a very expansive understanding of what the stop words were.

The choice I made was to not lean on the scales at all, and just use the most common off-the-shelf list of stop words. And there was a good reason for that; I wanted the model to not simply replicate my prejudices. But I half-think I made the wrong call here, and that the model would be more useful if I had filtered out more 'ordinary language'.
