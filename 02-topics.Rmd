# The 90 Topics {#all-90-topics}

In this chapter I go through all 90 topics that the model generates. I'll present a bunch of automatically generated facts about each topic, then say something about either the history of the topic, or how it fits into the larger model. In this introduction I'll explain how each of the automatically generated facts is in fact generated, using the example of [Topic 21](#topic21).

As you can see in the sidebar, each topic has a number and a name. The numbers are taken from the age of the articles in the topic. Lower numbers pick out older topics. So as we work through the 90, we'll move closer and closer to the present of philosophy. The names are things that I supplied. And the statistics that follow are mostly the things I was looking at when I gave each topic its name.

I've put each topic into one (or sometimes two) **categories**. These are the familiar disciplines of contemporary philosophy: metaphysics, ethics, history and the like. For Early Modern, the categorisation was easy.

```{r example-category}
jjj <- 21
if(!the_categories$cat_num[jjj] == 13){
  cat_nam <- the_categories$cat_name[jjj]
}
if(the_categories$cat_num[jjj] == 13){
  temp_category_tibble <- tibble(
    topic = (jjj*100+1):(jjj*100+2))
  temp_category_tibble <- temp_category_tibble %>%
    inner_join(the_categories, by = "topic")
  cat_nam <- paste(temp_category_tibble$cat_name, sep = "/", collapse = "/")
}

cat("**Category**: ", cat_nam, "\n\n")
```

For each topic, the model generates something like a probability of a word turning up given that an article is (certainly) in that topic. We can use that to generate keywords for each topic. But there are a couple of hitches.

The first thing I tried was to identify the keywords for a topic with the words that have the highest probability of turning up in articles in that topic. But that gives you the same keywords for most of the topics. Indeed, for every topic it gives you keywords that were borderline cases of being [stop words](#stop-words).

The second thing I tried was to identify the keywords with those words where the ratio between the probability the word turns up in this topic with the probability it turns up in an arbitrary article. This is better, but still not right. Doing this makes all the keywords be incredibly rare words, that essentially never turn up in any other topic. (Occasionally this would make 'weatherson' a keyword, for example, though not usually where I expected.)

So what I settled on was to use that ratio, but only quantify over the words that are at least reasonably common across the data. Roughly, they are words that turn up at least once in every 20000 words (excluding stop words).^[More carefully, for any word $w$ and model $t_k$ the model provides something like $\Pr(w | t_k)$, the probability of word turning up in an article in that topic. We're looking for the words that maximise $\frac{\Pr(w | t_k)}{\sum \Pr(w | t_i)}$, where the sum is over the 90 topics, and the constraint is that the average value of $\Pr(w | t_i)$ is at least $\frac{1}{20000}.] Applied to Early Modern, that gives us the following keywords.

```{r example-keywords}
distinctive <- distinctive_topics[(jjj-1)*15 + 1, 1]
for (jj in 2:15){
  distinctive <- paste(distinctive, distinctive_topics[(jjj-1)*15 + jj, 1], sep = ", ")
}

cat("**Keywords**: ", distinctive, "\n\n")
```

Next I'll look at the size of the topic. There are two ways of looking at this, and since I'm going to be using them a lot, it's worthwhile going over them at some length.

The model assigns each article a probability of being in each topic. So for each article there is a topic with maximal probability. (In principle there could be ties, but in practice that doesn't seem to happen.) I'll follow a fairly standard practice and say that an article is **in a topic** if that topic has maximal probability. And then the number of articles in a topic is the number of articles such that this topic gets higher probability for that article than any other topic. We can count these up to get a sense of the size of the topic.

```{r example-raw-count}
require(toOrdinal)

cat("**Number of Articles**: ", overall_stats$r_count[jjj], " \n <br>", sep="")
cat("**Percentage of Total**: ", 100*overall_stats$r_percent[jjj], "%\n <br> ", sep="")
cat("**Rank**: ", toOrdinal(overall_stats$r_rank[jjj]), "\n\n", sep="")
```

So there are 398 articles that are in Early Modern (in this sense). Hopefully it isn't too surprising now why I called this Early Modern. I don't know what 'vii' is doing there; arguably it should have been filtered out. That's slightly more than average, since there are about 360 articles in the average topic. It's 1.2% of the total; as you can calculate, the average topic would have 1.1%. And if we order the topics from largest to smallest, it makes Early Modern the 32nd largest of the topics.

But this isn't the only way to measure the size of a topic. The model gives a probability of being in Early Modern to every article in the data set. We can simply add those probabilities up to get another way to measure topic size. Formally, this is the way to calculatte the _expected_ number of articles in that topic given the probability distribution, though I don't think thinking of these numbers as expected values is particularly helpful. If we do that, summing the probabilities of being in early modern across all articles, we get the following statistics.

```{r example-weighted-count}
cat("**Weighted Number of Articles**: ", overall_stats$w_count[jjj], "\n <br>", sep="")
cat("**Percentage of Total**: ", 100*overall_stats$w_percent[jjj], "%\n <br>", sep="")
cat("**Rank**: ", toOrdinal(overall_stats$w_rank[jjj]), "\n\n", sep="")
```

I'll call this calculation the **weighted number** of articles in the topic. As I said, mathematically it's just the formula for expected value calculation, but I'm going to use it more like a weighted sum, hence the name. By this measure, Early Modern looks a little smaller. It's now only the 49th largest topic, and is under 1%. This is one of the larger gaps between the two ways of measuring the size of a topic; mostly they go together. (Though there is going to be one [special case](#topic55) where they come dramatically apart.)

Next I'll look at some facts about the dates of articles in that topic.

```{r example-dates}
cat("**Mean Publication Year**: ", overall_stats$mean_y[jjj], "\n <br>", sep="")
cat("**Weighted Mean Publication Year**: ", overall_stats$wy[jjj], "\n <br>", sep="")
cat("**Median Publication Year**: ", overall_stats$median_y[jjj], "\n <br>", sep="")
cat("**Modal Publication Year**: ", overall_stats$modal_y[jjj], "\n\n", sep="")
```

The first, third and fourth statistics there are easy to understand. I simply took the 398 articles in Early Modern, and found the mean, median and modal publication dates for them. The second is only a little trickier. I calculated the weighted average of the publication year of all articles in the data set, where the weights are given by the probability of being in Early Modern. As happens here, the first three numbers usually end up being very similar. The fourth can be quite random, and usually leans toward the present since there are more articles published now than in the past.

The last statistics I'll look at, before going on to some graphs, concern how close Early Modern is to various neighbours. I'll present these then explain them.

```{r example-neighbours}
cat("**Topic with Most Overlap**: ",
    the_categories$subject[closest_neighbour$othertopic[jjj]], 
    " (", 
    round(closest_neighbour$g[jjj],4),
    ")\n <br>",
    sep="")
cat("**Topic this Overlaps Most With**: ",
    the_categories$subject[closest_neighbour_inverse$topic[jjj]], 
    " (", 
    round(closest_neighbour_inverse$g[jjj],4),
    ")\n <br>",
    sep="")
cat("**Topic with Least Overlap**: ",
    the_categories$subject[furthest_neighbour$othertopic[jjj]], 
    " (", 
    round(furthest_neighbour$g[jjj],5),
    ")\n <br>",
    sep="")
cat("**Topic this Overlaps Least With**: ",
    the_categories$subject[furthest_neighbour_inverse$topic[jjj]], 
    " (", 
    round(furthest_neighbour_inverse$g[jjj],5),
    ")\n",
    sep="")

opts_knit$set(eval.after = "fig.cap") # Need this for next chunk
```

Recall that each of the 398 articles in Early Modern is also assigned a probability of being in each of the other 89 topics. So I calculated the average probability of being in each of the 89 topics among these 398 articles. And the first line here reports that the highest average probability was for [Idealism](#topic02). It isn't huge - just 4.6%, but given we're only looking at non-maximal probabilities, and it's a 90-way partition, this isn't that small. The third line reports that the lowest of these average probabilities was for [Formal Epistemology](#topic84). Usually there is a group of 10-20 topics that have vanishingly small mean probabilities here, and it's a bit random which of them the model picks out.

We can also look through each of the other topics and ask, of the articles in those topics, what is the probability the model gives to them being Early Modern articles? If you do all 89 calculations, it turns out the answer is the topic about the [Ontological Argument](#topic29). This is one of the smaller topics, but the articles in it have on average a probability of just over 2% of being Early Modern Articles. On the other hand, the articles in [Game Theory](#topic75) have a mean probability of being Early Modern articles of only 0.01%. I suspect this is because the model separates out Early Modern from [Social Contract Theory](#topic31), and any paper on the intersection of 17th/18th century philosophy with game theory ends up classifed as a Social Contract paper.

That's enough statistics to get us started, let's move onto some nice graphs.

```{r examplefacet, fig.cap=paste(the_categories$subject[jjj], "Articles in Each Journal")}
jjj <- 21
source('topic_comments/topic_summary_facet_graph.R')
```

As you can see, this has years on the x-axis, and ratios on the y-axis, and 12 'facets'. What each dot represents is average probability for an article in a particular year-journal pair being in this topic. Let's work through that with an example.

Look in the facet for _Philosophical Review_. There are two dots that are much higher than the rest, both of them around 0.15. The left-hand one, which is just under 0.15, is from the early 1930s. The right-hand one, just over 0.15, is from 1999. And I'm going to talk through this one for a bit. Here are the 11 articles in _Philosophical Review_ that year, along with their probability of being in topic 21.^[The meta-data includes messy things like "Sleighjr", and I just haven't corrected them - I'm just going with what JSTOR feeds me.]

<br>
```{r prexample1}
phil_review_1999 <- relabeled_gamma %>%
  filter(topic == 21, year == 1999, journal == "Philosophical Review") %>%
  inner_join(articles, by = "document") %>%
  arrange(fpage) %>%
  select(citation, gamma)

kable(phil_review_1999, col.names = c("Article", "Probability of being Early Modern"), caption = "Articles in Philosophical Review, 1999")
```
<br>

If you sum these 11 probabilities, you get `r sum(phil_review_1999$gamma)`. And dividing by 11 to get the average, you get `r mean(phil_review_1999$gamma)`. And that's where the dot I was pointing out comes from.

Note that this might feel a little light. Topic 21 is, more or less, early modern metaphysics and epistemology. And that looks like it should be 3 or 4 out of the 11 topics, which is much more than 0.15. What's gone on?

One thing that's happened is somewhat inevitable when dealing with history. Griffin's article is definitely about Leibniz, which is why you see a probability well above 0, but it's also about modals and counterfactuals. And we have another topic that's all about [Modality](#topic80), so a bunch of the probability went there. (Indeed, it is in that topic because that probability was maximal.) And it's about God, and we have two topics about God in the model, and some of the probability went there. This is the general case. History of philosophy articles involve a lot of philosophy, and whatever kind of philosophy they involve, the model will want to put them with other philosophy articles discussing those points.

But how does that explain Murdoch's article? Surely that's an early modern article. And it surely is. The simplest thing to say there is that when you've got 90 exclusive hypotheses, ending up with a probability of 0.6 for one of them is actually a lot. And some of the remaining probability also makes sense. We have a topic for the [Ontological Argument](#topic29), and some of the probability for Murdoch's article goes there. And we have a topic on [Arguments](#topic55), and the articles in that topic are mostly about how to understand circularity, so some of the probability goes there. And there is a topic for [Ordinary Language Philosophy](#topic24), and it turns out there are a lot of articles the model gives non-trivial probability to being in it. This topic causes some complications, because it's as much a style as a topic, and I'll come back to it a lot in what follows.

So that's how the facet graphs are constructed. Apart from the _Mind_ facet,the graphs do not start all the way to the left edge, because those journals didn't start publishing until after _Mind_ did. The color of the dots is taken from the color the topic has in the big graphs we will do later when all 90 topics are presented on one graph.

Note that the scale for these graphs will top out at either 0.2 or the highest value graphed, whatever is highest. I sort of wanted them to all be on a common scale, so you could easily eyeball how active a topic is. But that meant so many graphs were just squiggles barely above the x-axis. Instead I made most of them (76 to be precise) have a common scale, and the few outliers get their own scale. But I'll note this when it matters.

Next, I'll do the same graph but for all the journals at once. So here's what it looks like for topic 21. 

```{r exampleoverall, fig.cap=the_categories$subject[jjj], fig.height= 5.2}
cat("<br>")
source('topic_comments/topic_summary_overall_graph.R')
cat("<br>")
```

The big difference here is that I set the default upper limit to the y-axis at 0.05. There are only a handful of cases where that needs to be raised, but they require raising it by a lot.

After these graphs, there are two tables of the articles that are in the topic. Here is the first.

```{r examplecharart}
cat("<br>")
source('topic_comments/topic_summary_char_art.R')
temp_dt
```

This table actually lists all 398 articles in Early Modern. By default it displays 10 at a time. You can move through the list by the numbers at the bottom, or extend it using the dropdown menu in the top left.

The search box in the upper right will search for any text in the citation. This is helpful either for finding title words (e.g., 'Spinoza'), or author names (e.g., 'Curley').

The year and citation columns are self-explanatory. The probability column gives the probability that the article is actually in Early Modern. This is helpful to check when you see an article that looks misclassified. If the number is under about 0.25, that means the model is fairly undecided about what to do with the article, and it ended up here for want of somewhere better to put it.

The table is sortable by any of those three columns. But it's default sort order is by what I'll call typicality. This is the product of its probability of being in the topic, with the log of its length in pages. I'm using this complicated formula because for most topics, the high probability articles are short discussion notes where the model doesn't see anything to offset its initial judgment about where the article should go. Using this (totally made up) typicality measure as the initial sort meant that the articles that turned up here were more familiar, and gave me a better sense of what was, well, typical for the topic.

The second table is a list of highly cited articles in the topic.

```{r example-t21e}
cat("<br>")
source('topic_comments/topic_summary_high_cites.R')
high_table
```
<br>

I used [Publish or Perish](https://harzing.com/resources/publish-or-perish) to download the 50 most cited articles from each of our 12 journals according to Google Scholar. I do _not_ stand by these lists as being particularly accurate. I tried a couple of obvious ways to download the data, and they had obvious shortcomings, which I corrected. And then I thought the lists I had were good enough for illustrative purposes, so I stopped. But there were so many obvious things to correct that I'm sure there are non-obvious things to correct. But the point is not to do a citation study; it's to list some articles you've probably heard of that are in the topic.

The order of this list is most to least Google Scholar citations. Note that it is not the 600 most cited articles in the 12 journals; the 50th most cited Journal of Philosophy articles is cited more than practically any Philosophical Quarterly article. ^[With one [notable exception](https://philpapers.org/rec/JACEQ).] But again, the point is not to measure how well cited the topics are; it's to list some familiar articles in the topics. And I felt that spreading around the journals was best for that purpose.

Google Scholar isn't particularly reliable at distinguishing between articles with the same title, year and publication venue. So you see the occasional doubling up like this. As I said, I don't stand behind the accuracy of these lists; they are there to illustrate the topic.

You'll often see some very low probabilities turning up in these tables. Sometimes the most influential articles in philosophy are ones that don't neatly fit into one topic or another. Indeed, one of the themes of this book is that by highlighting where the most work has been done, we can see more clearly what areas are left open. They'll probably be the areas that produce highly cited articles of the next 138 years.

```{r child = 'topic_comments/topic01.Rmd'}
```

```{r child = 'topic_comments/topic02.Rmd'}
```

```{r child = 'topic_comments/topic03.Rmd'}
```

```{r child = 'topic_comments/topic04.Rmd'}
```

```{r child = 'topic_comments/topic05.Rmd'}
```

```{r child = 'topic_comments/topic06.Rmd'}
```

```{r child = 'topic_comments/topic07.Rmd'}
```

```{r child = 'topic_comments/topic08.Rmd'}
```

```{r child = 'topic_comments/topic09.Rmd'}
```

```{r child = 'topic_comments/topic10.Rmd'}
```

```{r child = 'topic_comments/topic11.Rmd'}
```

```{r child = 'topic_comments/topic12.Rmd'}
```

```{r child = 'topic_comments/topic13.Rmd'}
```

```{r child = 'topic_comments/topic14.Rmd'}
```

```{r child = 'topic_comments/topic15.Rmd'}
```

```{r child = 'topic_comments/topic16.Rmd'}
```

```{r child = 'topic_comments/topic17.Rmd'}
```

```{r child = 'topic_comments/topic18.Rmd'}
```

```{r child = 'topic_comments/topic19.Rmd'}
```

```{r child = 'topic_comments/topic20.Rmd'}
```

```{r child = 'topic_comments/topic21.Rmd'}
```

```{r child = 'topic_comments/topic22.Rmd'}
```

```{r child = 'topic_comments/topic23.Rmd'}
```

```{r child = 'topic_comments/topic24.Rmd'}
```

```{r child = 'topic_comments/topic25.Rmd'}
```

```{r child = 'topic_comments/topic26.Rmd'}
```

```{r child = 'topic_comments/topic27.Rmd'}
```

```{r child = 'topic_comments/topic28.Rmd'}
```

```{r child = 'topic_comments/topic29.Rmd'}
```

```{r child = 'topic_comments/topic30.Rmd'}
```

```{r child = 'topic_comments/topic31.Rmd'}
```

```{r child = 'topic_comments/topic32.Rmd'}
```

```{r child = 'topic_comments/topic33.Rmd'}
```

```{r child = 'topic_comments/topic34.Rmd'}
```

```{r child = 'topic_comments/topic35.Rmd'}
```

```{r child = 'topic_comments/topic36.Rmd'}
```

```{r child = 'topic_comments/topic37.Rmd'}
```

```{r child = 'topic_comments/topic38.Rmd'}
```

```{r child = 'topic_comments/topic39.Rmd'}
```

```{r child = 'topic_comments/topic40.Rmd'}
```

```{r child = 'topic_comments/topic41.Rmd'}
```

```{r child = 'topic_comments/topic42.Rmd'}
```

```{r child = 'topic_comments/topic43.Rmd'}
```

```{r child = 'topic_comments/topic44.Rmd'}
```

```{r child = 'topic_comments/topic45.Rmd'}
```

```{r child = 'topic_comments/topic46.Rmd'}
```

```{r child = 'topic_comments/topic47.Rmd'}
```

```{r child = 'topic_comments/topic48.Rmd'}
```

```{r child = 'topic_comments/topic49.Rmd'}
```

```{r child = 'topic_comments/topic50.Rmd'}
```

```{r child = 'topic_comments/topic51.Rmd'}
```

```{r child = 'topic_comments/topic52.Rmd'}
```

```{r child = 'topic_comments/topic53.Rmd'}
```

```{r child = 'topic_comments/topic54.Rmd'}
```

```{r child = 'topic_comments/topic55.Rmd'}
```

```{r child = 'topic_comments/topic56.Rmd'}
```

```{r child = 'topic_comments/topic57.Rmd'}
```

```{r child = 'topic_comments/topic58.Rmd'}
```

```{r child = 'topic_comments/topic59.Rmd'}
```

```{r child = 'topic_comments/topic60.Rmd'}
```

```{r child = 'topic_comments/topic61.Rmd'}
```

```{r child = 'topic_comments/topic62.Rmd'}
```

```{r child = 'topic_comments/topic63.Rmd'}
```

```{r child = 'topic_comments/topic64.Rmd'}
```

```{r child = 'topic_comments/topic65.Rmd'}
```

```{r child = 'topic_comments/topic66.Rmd'}
```

```{r child = 'topic_comments/topic67.Rmd'}
```

```{r child = 'topic_comments/topic68.Rmd'}
```

```{r child = 'topic_comments/topic69.Rmd'}
```

```{r child = 'topic_comments/topic70.Rmd'}
```

```{r child = 'topic_comments/topic71.Rmd'}
```

```{r child = 'topic_comments/topic72.Rmd'}
```

```{r child = 'topic_comments/topic73.Rmd'}
```

```{r child = 'topic_comments/topic74.Rmd'}
```

```{r child = 'topic_comments/topic75.Rmd'}
```

```{r child = 'topic_comments/topic76.Rmd'}
```

```{r child = 'topic_comments/topic77.Rmd'}
```

```{r child = 'topic_comments/topic78.Rmd'}
```

```{r child = 'topic_comments/topic79.Rmd'}
```

```{r child = 'topic_comments/topic80.Rmd'}
```

```{r child = 'topic_comments/topic81.Rmd'}
```

```{r child = 'topic_comments/topic82.Rmd'}
```

```{r child = 'topic_comments/topic83.Rmd'}
```

```{r child = 'topic_comments/topic84.Rmd'}
```

```{r child = 'topic_comments/topic85.Rmd'}
```

```{r child = 'topic_comments/topic86.Rmd'}
```

```{r child = 'topic_comments/topic87.Rmd'}
```

```{r child = 'topic_comments/topic88.Rmd'}
```

```{r child = 'topic_comments/topic89.Rmd'}
```

```{r child = 'topic_comments/topic90.Rmd'}
```

