# Looking Outwards {#lookingoutward}

## Classic Books {#classic-book-section}

## Philosophers' Imprint {#imprint-section}

I've often speculated in the book about what would happen if we ran the model forward beyond the end of JSTOR's moving wall. The purpose of this section is to look at what happens if we run the model forward in one very particular way. I downloaded all the articles published in 2019 by the open access journal [Philosophers' Imprint](https://www.philosophersimprint.org). I extracted the text from these articles using Jeroen Ooms's package [pdftools](https://docs.ropensci.org/pdftools/). And then I applied the model to these articles using the posterior function in [topicmodels](https://cran.r-project.org/web/packages/topicmodels/index.html). Here are the primary topics for each of the 54 articles.

```{r get-imprint-articles}
# Load the titles from the CSV
require(readr)
imprint_titles <- read_csv("imprint-titles.csv")

# Adjust the number to the code that was used in the LDA
imprint_titles <- imprint_titles %>%
  mutate(document = paste0("imprint-",number)) %>%
  mutate(citation = paste0(author, ", \"", title, "\""))

# Merge with the list of top gammas for each article
imprint_top_gamma_display <- inner_join(imprint_titles, imprint_top_gamma, by = "document") %>%
  inner_join(the_categories, by = "topic") %>%
  arrange(topic) %>%
  select(citation, subject, gamma, everything())
```

```{r display-article-summary}

require(DT)

datatable(imprint_top_gamma_display %>%
        select(citation, subject, gamma), 
          colnames = c("Article", "Topic", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:2))),
          caption = "Topic Assignments for Articles in Philosophers' Imprint 2019"
          )%>%
  formatSignif('gamma',3) %>%
  formatStyle(1:3,`text-align` = 'left')
```

For each article I've shown which topic the model thinks the article most likely falls in. And these are, I think, mostly sensible classifications. But I've also shown the probability the model gives to that article being in just that topic. And given that these are maximal, they might look surprisingly low.

As we saw in the [previous section](#classic-book-section), the model is extremely cautious when making out of sample classifications. So even for what looks to us like a fairly clear case, like say Janum Sethi's [Two Feelings in the Beautiful: Kant on the Structure of Judgments of Beauty](http://hdl.handle.net/2027/spo.3521354.0019.034), the model is only 30% sure of its judgment. Now I guess it isn't surprising that the model is a little hesitant about whether to classify this with article on Kant or articles on Beauty, but the uncertainty you see in out-of-sample applications goes well beyond this. Here, for instance, is the list of all the topics it thinks Sethi's article might be in. (As has been the standard through this book, I've cut the table off at 2%.)

```{r individual-article-imprint-kable}
imprint_kable <- function(x){
temp_gamma <- imprint_gamma %>%
  filter(document == x, gamma > 0.02) %>%
  select(topic, gamma) %>%
  inner_join(the_categories, by = "topic") %>%
  select(subject, gamma) %>%
  arrange(-gamma)

capt <- paste0(filter(imprint_top_gamma_display, document == x)$citation[1],
               ", ",
               "Philosophers' Imprint, 2019, vol. 19, number",
               ", ",
               filter(imprint_top_gamma_display, document == x)$number[1]-1900
)


kable(temp_gamma, 
      col.names = c("Subject", "Probability"), 
      caption = capt) %>% 
  kable_styling(full_width = F)
}
```

<br>
```{r sethi-kable}
imprint_kable("imprint-1934")
```
<br>

The top three look reasonable, though the numbers are too low, especially for Kant. This is just what happens in these out-of-sample applications. I have no idea why the model thought [Dewey and Pragmatism](#topic32) was there. But the story about [Norms](#topic90) is a bit more interesting.

It turns out that the language of twenty-first century philosophy is rather different from the language of twentieth-century philosophy. This is like the way in which the language of mid-century British philosophy was rather different to the language of other places and times. And the Norms topic picks up some of these newly fashionable words. I'll come back to this at much greater length in section \@ref(buzzwords-section).

It isn't only Sethi's paper that is part of this linguistic change. There are many papers that are primarily in Norms, even though they aren't really all about normative philosophy. Some of that is washed away when you use the weighted counts, but far from all of it. Here is the table of the weighted sum of each of the topics over the 54 articles. (That is, it's the sum, over the 54 articles, of the article being in that topic.) The table is long, so it's split up, sortable, and searchable.

```{r imprint-topic-summary}
imprint_summary_DT <- inner_join(imprint_summary, the_categories, by = "topic") %>%
  arrange(-g) %>%
  select(topic, subject, g)

require(DT)

datatable(imprint_summary_DT, 
          colnames = c("Topic Number", "Topic", "Weighted Sum"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:2))),
          caption = "Weighted Sum of Topic Probabilities for Philosophers' Imprint 2019"
          )%>%
  formatSignif('g',2) %>%
  formatStyle(1:3,`text-align` = 'left')
```

How does Norms get to 4.8 like that? Lots of small pieces it turns out.

```{r imprint-norms-summary}
imprint_norms_DT <- inner_join(imprint_titles, imprint_gamma, by = "document") %>%
  filter(topic == 90) %>%
  inner_join(the_categories, by = "topic") %>%
  arrange(-gamma) %>%
  select(citation, gamma, everything())


require(DT)

datatable(imprint_norms_DT %>%
            select(citation, gamma), 
          colnames = c("Article", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:1))),
          caption = "Articles in Philosophers' Imprint 2019 - Probability of being in Topic 90"
          )%>%
  formatSignif('gamma',3) %>%
  formatStyle(1:2,`text-align` = 'left')
```

This is really surprising, and I suspect a sign that something's gone wrong. Either there are artefacts from the way I did the text extraction, or this new style of language is really spreading everywhere. I suspect both, but I'm not going to investigate this here. What I'd rather look at, because I think it tells us something more about where philosophy is heading, is the topic that was second on our above list: [Other History](#topic04).

This topic had looked rather dead in recent journals, but it turns up second overall here. And just eyeballing the article list that might not be too surprising. There are articles in Su√°rez (two of them!), William King, Amo, and Mary Shepherd. But this isn't entirely why Other History pops up so high here.

```{r imprint-history-summary}
imprint_norms_DT <- inner_join(imprint_titles, imprint_gamma, by = "document") %>%
  filter(topic == 4) %>%
  inner_join(the_categories, by = "topic") %>%
  arrange(-gamma) %>%
  select(citation, gamma, everything())


require(DT)

datatable(imprint_norms_DT %>%
            select(citation, gamma), 
          colnames = c("Article", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:1))),
          caption = "Articles in Philosophers' Imprint 2019 - Probability of being in Topic 4"
          )%>%
  formatSignif('gamma',3) %>%
  formatStyle(1:2,`text-align` = 'left')
```

This makes the lack of attention to 'Other History' in the journals up to 2013. James, Wittgenstein and Nietzche are hardly obscure figures. If articles on them are enough to keep the Other History scoreboard ticking over, it's really telling that that scoreboard had ground to a halt.

Apart from this topic, the results are largely as I expected. The contemporary topics mostly keep growing. [Vagueness](#topic86), which peaked in the early 2000s, is an exception. And Imprint does less philosophy of science than some journals, so the recent philosophy of science topics aren't showing up much. But otherwise the new topics are still growing. And the old topics are still mostly shrinking, but with two big exceptions. One, the resurgence of interest in historical figures outside of the huge names like Plato, Descartes and Kant, I've already mentioned. The other is that there is a paper on [Idealism](#topic02).

<br>
```{r idealism-imprint-paper-summary}
imprint_kable("imprint-1944")
```
<br>

Now on the one hand, that's only a 10.1% probability of being in Idealism. On the other hand, it doesn't look like a mistake for the model to put this article in Idealism. And to my eyes, there were no articles after the mid-1990s that were correctly classified that way.

I don't want to say that one article in Imprint is a trend, even if it is something we hadn't seen in the last 7000 or so articles in the original data set. But it is interesting, and something to watch for over upcoming years. It is so striking to me that Idealism went from being so big to so small, and some regression to the mean seems like it wouldn't be a surprise.


## Buzzwords {#buzzwords-section}

One of the methods I used for building the model was to run repeated [refinements](#refinements-section) of the model, to try to make it better track actual philosophical topics. At the time I did this, I was worried that this would have bad consequences. It felt like tightening the strings. And while this is generally a good idea, if you make things too tight, they snap. After a few refinemnts, I started to think this was a silly bit of reasoning by analogy. These aren't actually strings, so they can't actually snap, right? Well, after 100 iterations of the refinement script, I got a topic whose distribution looked like this.

```{r bad-gamma-absolute}

jjj <- 90
ggplot(data = bad_gamma_year_journal %>% drop_na(), aes(x = year, y = g))  +
  geom_point(size = 0.15, colour = hcl(h = (jjj-1)*(360/cats)+15, l = 65, c = 100)) +
  theme(legend.position="none") +
  labs(x = "Year", y = "Weighted Number of Articles", title = paste0("Articles from topic ",jjj," in each Journal")) +
  facet_wrap(~journal, ncol = 3, labeller = as_labeller(journal_short_names))
```

Those are remarkable graphs - it seems that this topic is getting to be a bigger and bigger deal in all the journals. I had not seen anything like this; after 1970 it's almost impossible to get the 'generalist' journals, the philosophy of science journals and the ethics journals moving in the same direction. Maybe it's a function of the journals publishing more articles over time. We could check this by looking at what proportion of the journals are made up by this topic, not the absolute number of (expected) articles.^[Most of the graphs in chapter \@ref(all-90-topics) are proportional, not the absolute graph I just showed you.]

```{r bad-gamma-relative}
jjj <- 90
ggplot(data = bad_gamma_year_journal %>% drop_na(), aes(x = year, y = y))  +
  geom_point(size = 0.15, colour = hcl(h = (jjj-1)*(360/cats)+15, l = 65, c = 100)) +
  theme(legend.position="none") +
  labs(x = "Year", y = "Weighted Number of Articles", title = paste0("Articles from topic ",jjj," in each Journal")) +
  facet_wrap(~journal, ncol = 3, labeller = as_labeller(journal_short_names))
```

It's slightly less steep, especially in _Philosophy of Science_. But the generalist journals - except _Analysis_ - and _British Journal for the Philosophy of Science_ are still rising rapidly. So let's look at what articles are primarily in this topic.

<br>
```{r bad-gamma-top-articles}

kable(bad_gamma %>% slice(1:10) %>% select(citation, gamma), col.names = c("Article", "Topic Probability"))
```
<br>

This is very confusing in three different ways.

1. Although the topic seems concentrated in the twenty-first century, two of the top ten articles are from a fair way ago - including the top one.
2. If you exclude that top article, no article has a topic probability of over 0.4. This is true even though for some journal-year pairs, the average topic probability is over 0.1. It feels like every article must get a reasonable probability of being in this topic.
3. Relatedly, there doesn't seem to be any thematic unity to the articles here. What would you even call the 'topic' which has these ten articles as paradigm.

For comparison, the original [topic 90](#topic90) had a top 10 list that looked a little more sensible.

<br>
```{r bad-gamma-compare-top-articles}

t <- relabeled_articles %>%
  filter(topic == 90) %>%
  arrange(-gamma) %>%
  slice(1:10) %>%
  select(citation, gamma)

kable(t, col.names = c("Article", "Topic Probability"))
```
<br>

The papers are more recent, the probabilities are higher, and there is some more unity to the topic. It's about normativity and objectivity, very broadly construed, with a bit of a focus on Brandm. And you can still see hints of that in the new top 10 list, but it's gotten blurrier. The Rosen article that's at nearly 70% in the original topic has dropped to around 20% in the new topic, reflecting the lack of thematic unity.

Maybe we can get a bit better look at this new weird topic by looking at its keywords. Remember that an LDA model assigns each word a probability of being in a paradigm article in the topic. We can compare that to the frequency of that word in the whole data set to get a sense of what's characteristic of the topic. (Again, it's necessary to restrict attention to the 5000 most common words here to prevent too much focus on words that appear just a handful of times.) And here's what we get. The second column here is the ratio of the probability of the word being in a (paradigm) article in this topic to the word's overall frequency.

<br>
```{r bad-beta-top-ratio}
kable(bad_beta %>% select(word, y) %>% slice(1:20) %>% mutate(y = round(y, 1)), col.names = c("Word", "Ratio")) %>% 
  kable_styling(full_width = F)
```
<br>

This I think is the clue to what's happened. The 'topic' here is just the distinctive vocabulary of twenty-first century philosophy. The topic appears in all journals because these words have become more and more prevalent in all journals.

I'll come back to direct evidence for this hypothesis in a minute, but first I wanted to show you what a table like this looks like for normal topics. Here's the same table for topic 90 in the original model.

<br>
```{r good-beta-top-ratio}
kable(good_beta %>% select(word, y) %>% slice(1:20) %>% mutate(y = round(y, 1)), col.names = c("Word", "Ratio")) %>% 
  kable_styling(full_width = F)
```
<br>

You can see some of the twenty-first century vocabulary, like 'challenge' and 'accounts' turning up at the bottom. But this is what things mostly should look like for a topic about normativity and objectivity. 

Just to get a sense of the appropriate scale for what's being measured here, here's what the top of the table for the [Kant topic](#topic32) looks like.

<br>
```{r kant-beta-top-ratio}
kable(kant_beta %>% select(word, y) %>% slice(1:10) %>% mutate(y = round(y, 1)), col.names = c("Word", "Ratio")) %>% 
  kable_styling(full_width = F)
```
<br>

That makes sense; the word 'Kant' is 163 times more likely to appear in a paradigm Kant article than in philosophy in general. A ratio like this of 163 is high, but having the highest ratio be 15 is a sign something has gone wrong. The only topic that is really like this in the original model is [ordinary language philosophy](#topic 24).

<br>
```{r olp-beta-top-ratio}
kable(olp_beta %>% select(word, y) %>% slice(1:20) %>% mutate(y = round(y, 1)), col.names = c("Word", "Ratio")) %>% 
  kable_styling(full_width = F)
```
<br>

Like the new topic 90, this topic in the original model really tracked a style not a content. It's really striking that the distinctive words in ordinary language philosophy are so much shorter than the distinctive words in contemporary philosophy.^[I wanted to include here some graphs about average word lengths over time, but they don't really show very much. There is a very gentle increase, focussed on the philosophy of science journals, but the distinctively short keywords don't really track anything about average word length.] But it's probably time to start actually proving that words like 'commitment', 'challange' and 'approach' are distinctively twenty-first century words. So rather than just look at the outputs of complicated models, I'm going to end with some simple graphs of word frequency over time. 

The data set I'm using for the graphs to follow is the word lists as provided by JSTOR. So that excludes some stop words, and all one and two letter words, but not all the other words that I filtered out before building the LDA. I'll start with some graphs of the keywords from this new topic 90.

```{r buzzwords-graphs}
word_frequency_graphs(c("account", "accounts", "claim", "claims"))
word_frequency_graphs(c("role", "appeal", "project", "focus"))
word_frequency_graphs(c("commitment", "commitments", "proposal", "proposals"))
word_frequency_graphs(c("worry", "worries", "challenge", "challenges"))
word_frequency_graphs(c("typically", "relevant", "practices"))
```

Not all of these words are shooting upwards, but many of them are. It's striking to me that you don't really need trendlines here to see a trend. At this rate we'll soon see articles made up of just the words 'account', 'typically', 'relevant' and 'challenge', plus perhaps their plurals.

So this is why I just used 15 refinements of the model rather than 100. The language of early twenty-first century philosophy is distinctive enough that if you push a text-based analysis too hard, it ends up just tracking form rather than content.

But didn't we have this already back in [ordinary language philosophy](#topic 24)? We did, though fortunately the binary sort helped find a couple of natural topics within it. Still, it would be nice to confirm that these words really were being used more frequently in mid-century. So let's look at the same graphs for the keywords from ordinary language philosophy.

```{r olpwords-graphs}
word_frequency_graphs(c("ask", "answer", "question", "said"))
word_frequency_graphs(c("really", "perhaps", "course", "certainly"))
word_frequency_graphs(c("quite", "sort", "seem", "much"))
word_frequency_graphs(c("think", "want", "get"))
word_frequency_graphs(c("anything", "something", "things"))
```

The first three have roughly the pattern I was expecting, but the last two don't. I think there is a sense in which some of the stylistic changes that the ordinary language philosophers brought in persisted. And there is also a sense in which they were the last holdouts against the move to a more scientific philosophy. As is so often the case, it helps to look at a distinctive era as both the end of what came before it, and the start of what came after it.

There is another puzzle that I left open above that I want to return to. How could we square the low ratio between the maximal and average topic probabilities for some journal-year pairs? The obvious answer is that every article is in the topic to some non-trivial degree. Let's see how true that is. So for a few journal-year pairs, I'm going to go through every article and list the probability that it is in this new topic. I'll start with _Philosophical Review_ in 2004.

<br>
```{r bad-gamma-pr-2004}
kable(bad_gamma %>% filter(year == 2004, journal == "Philosophical Review") %>% select(citation, gamma), col.names = c("Article", "Topic Probability"))
```
<br>

The Roth paper really is about commitments, so it isn't surprising that it's a little higher than the others. But look how much this spreads around other articles. The model thinks there is something that all but one of these articles have seriously in common. And I think there isn't anything substantive (as opposed to stylistic) this could be. Let's move on to _Ethics_ in 2010.

<br>
```{r bad-gamma-ethics-2010}
kable(bad_gamma %>% filter(year == 2010, journal == "Ethics") %>% select(citation, gamma), col.names = c("Article", "Topic Probability"))
```
<br>

I don't know why all the stars are there in the title; that's just what came from JSTOR. And we get the same pattern; almost all the articles are in the topic at a 5% probability or higher. To the extent that there was anything substantive in the topic, it was in normative ethics, so maybe that's not too surprising. But let's see what happens when we do the same thing for _British Journal for the Philosophy of Science_ in 2011.

<br>
```{r bad-gamma-bjps-2011}
kable(bad_gamma %>% filter(year == 2011, journal == "British Journal for the Philosophy of Science") %>% select(citation, gamma), col.names = c("Article", "Topic Probability"))
```
<br>

Here we do get more articles that are clearly excluded. The difference between the last six articles is unimportant. Once you get below 0.1%, the probabilities are functions of how confident the model is in its central classifications. But it's still striking how many of these are above 1.1%. There are 90 topics, so if the model had no idea it would put each probability at 1.1%. The vast majority of the articles here are above that.

There is another study I could imagine running here, but it would take so long that I'm going to leave it to later work. Repeatedly refining the model broke because of the distinctive language of twenty-first century philosophy. There are two possible explanations for that.

1. There has been a linguistic revolution over the last generation, and philosophers now write in a very different style to how they wrote a generation ago.
2. This is an artifact of model building, and if you stopped the model at any time, and ran the same study I did, you'd get results like this. That is, doing what I did will get you weird results whenever there is linguistic drift, and there is always linguistic drift.

I actually could test these by running the study I did for this book but stopping in, say, 1993. But I don't think spending several hundred hours processing time on teasing apart these two explanations would be worthwhile.

That's in part because this question will resolve itself over time naturally. Hopefully more studies like mine (or preferably better designed studies than mine) will be run on data that goes through 2020 and beyond. Those will tell us even more about where philosophy is going, and answer several questions that I've left open as pleasant side-effects.