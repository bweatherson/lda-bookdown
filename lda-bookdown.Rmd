--- 
title: "A History of Philosophy Journals"
subtitle: "Volume 1: Evidence from Topic Modeling, 1876-2013"
author: "Brian Weatherson"
date: "Marshall M. Weinberg Professor of Philosophy <br> University of Michigan, Ann Arbor"
documentclass: book
link-citations: yes
site: bookdown::bookdown_site
description: Building models of the trends in philosophy journals using LDA.
bibliography: topic.bib
nocite: '@*'
always_allow_html: true
---

# Introduction {-}

```{r packages, echo=FALSE, message = FALSE, warning = FALSE, cache=FALSE}
knitr::opts_knit$set(eval.after = c("fig.cap", "fig.alt"))
knitr::opts_chunk$set(dpi = 288)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.height = 8.2)
knitr::opts_chunk$set(fig.width = 7.5)
knitr::opts_chunk$set(results = 'asis')
require(tidyverse)
require(tidytext)
require(topicmodels)
require(knitr)
require(kableExtra)
require(corrr)
require(svglite)
library(ggplot2); theme_set(theme_minimal())
require(DT)
require(english)
require(tools)
require(ggtext)
#library(webshot)
#install_phantomjs()
require(extrafont)
#loadfonts()
options(dplyr.summarise.inform = FALSE)
```

```{r loader, cache=TRUE}
# Loads all the RData
# Cached because this takes forever each step
journal_short_names <- c(
  "Analysis" = "Analysis",
  "British Journal for the Philosophy of Science" = "BJPS",
  "Ethics" = "Ethics",
  "Journal of Philosophy" = "Journal of Philosophy",
  "Mind" = "Mind",
  "Noûs" = "Noûs",
  "Philosophical Review" = "Philosophical Review",
  "Philosophy and Phenomenological Research" = "PPR",
  "Philosophy and Public Affairs" = "P&PA",
  "Philosophy of Science" = "Philosophy of Science",
  "Proceedings of the Aristotelian Society" = "Aristotelian Society",
  "The Philosophical Quarterly" = "Philosophical Quarterly"
)

# The list of commonly used words (could calculate this from below)
load("common_words.RData")
source("short_words.R")

# How many topics we are using
cats <- 90

# The big LDA we're using
load("t90t15.RData")

# Rename it to the name I primarily use (and so it isn't overridden by other loads)
thelda <- refinedlda

# The application of the LDA to the 2019 Imprint articles
load("imprint_lda.RData")

all_journals_gamma <- tidy(thelda, matrix = "gamma")

```


```{r word_list_loader, cache=TRUE}
# The long word list
# This isn't on github because it might be proprietary
# It's in a different module because (a) it isn't on github and (b) it is really slow
load("all_journals_word_list.RData")
```

```{r article_loader, cache=TRUE}
# Load the articles, and make a bunch of tidying up
# Fixed_articles is now a terrible misnomer - it only has a few fixes
# With caches the others are easy to do every time
# The list of articles
load("fixed_articles.RData")

articles <- articles %>% 
  mutate(across(.cols = title:auth4, sub, pattern = "O\\'([a-z])", perl=TRUE, replacement = "O\\'\\U\\1")) %>% 
  mutate(across(.cols = title:auth4, sub, pattern = "Mc([a-z])", perl=TRUE, replacement = "Mc\\U\\1")) %>% 
  mutate(across(.cols = title:auth4, sub, pattern = "([a-z]),Jr.", replacement = "\\1, Jr.")) %>% 
  mutate(across(.cols = title:auth4, sub, pattern = "([a-z])jr.", replacement = "\\1, Jr.")) 
  
  # mutate(across(.cols = title:auth4, str_replace, "Mca", "McA")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcb", "McB")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcc", "McC")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcd", "McD")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mce", "McE")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcf", "McF")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcg", "McG")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mch", "McH")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mci", "McI")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcj", "McJ")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mck", "McK")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcl", "McL")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcm", "McM")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcn", "McN")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mco", "McO")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcp", "McP")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcq", "McQ")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcr", "McR")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcs", "McS")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mct", "McT")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcu", "McU")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcv", "McV")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcw", "McW")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcx", "McX")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcy", "McY")) %>% 
  # mutate(across(.cols = title:auth4, str_replace, "Mcz", "McZ"))

articles <- articles %>% 
  mutate(title = str_replace_all(title, "[/*]", "")) %>% 
  mutate(auth1 = str_replace_all(auth1, "[/*†‡]", "")) %>% 
  mutate(lastlet = str_sub(auth1, -1)) %>% 
  mutate(auth1 = case_when((lastlet == "," | lastlet == " ") ~ str_sub(auth1, end = -2),
                              TRUE ~ auth1)) %>% 
  mutate(auth1 = case_when((str_sub(auth1, -4) == " and" | (str_sub(auth1, -3) == "And")) ~ str_sub(auth1, end = -4),
                              TRUE ~ auth1)) %>% 
  mutate(lastlet = str_sub(auth1, -1)) %>% 
  mutate(auth1 = case_when((lastlet == "," | lastlet == " ") ~ str_sub(auth1, end = -2),
                                       TRUE ~ auth1)) %>% 
  mutate(auth2 = str_replace_all(auth2, "[/*†‡]", "")) %>% 
  mutate(lastlet = str_sub(auth2, -1)) %>% 
  mutate(auth2 = case_when((lastlet == "," | lastlet == " ") ~ str_sub(auth2, end = -2),
                              TRUE ~ auth2)) %>% 
  mutate(auth2 = case_when((str_sub(auth2, -4) == " and" | (str_sub(auth2, -3) == "And")) ~ str_sub(auth2, end = -4),
                              TRUE ~ auth2)) %>% 
  mutate(lastlet = str_sub(auth2, -1)) %>% 
  mutate(auth2 = case_when((lastlet == "," | lastlet == " ") ~ str_sub(auth2, end = -2),
                              TRUE ~ auth2)) %>% 
  mutate(lastlet = str_sub(auth2, -1)) %>% 
  mutate(auth2 = case_when((lastlet == "," | lastlet == " ") ~ str_sub(auth2, end = -2),
                              TRUE ~ auth2)) %>% 
  mutate(auth3 = str_replace_all(auth3, "[/*†‡]", "")) %>% 
  mutate(lastlet = str_sub(auth3, -1)) %>% 
  mutate(auth3 = case_when((lastlet == "," | lastlet == " ") ~ str_sub(auth3, end = -2),
                              TRUE ~ auth3)) %>% 
  mutate(auth3 = case_when((str_sub(auth3, -4) == " and" | (str_sub(auth3, -3) == "And")) ~ str_sub(auth3, end = -4),
                              TRUE ~ auth3)) %>% 
  mutate(lastlet = str_sub(auth3, -1)) %>% 
  mutate(auth3 = case_when((lastlet == "," | lastlet == " ") ~ str_sub(auth3, end = -2),
                              TRUE ~ auth3)) %>% 
  mutate(lastlet = str_sub(auth3, -1)) %>% 
  mutate(auth3 = case_when((lastlet == "," | lastlet == " ") ~ str_sub(auth3, end = -2),
                              TRUE ~ auth3)) %>% 
  mutate(auth4 = str_replace_all(auth4, "[/*†‡]", "")) 

# Regenerate authall after all those changes
articles <- articles %>% 
    mutate(authall = case_when(
    is.na(auth2) ~ auth1,
    is.na(auth3) ~ paste0(auth1," and ", auth2),
    is.na(auth4) ~ paste0(auth1,", ",auth2," and ",auth3),
    TRUE ~ paste0(auth1, " et al")
  ))

# Change format to Michigan Publishing preferred style
articles <- articles %>% 
  mutate(adjlpage = case_when(floor(fpage/10) == floor(lpage/10) & fpage < 10000 ~ lpage - 10*floor(lpage/10),
                              floor(fpage/100) == floor(lpage/100) & fpage < 10000 ~ lpage - 100*floor(lpage/100),
                              TRUE ~ lpage)) %>% 
  mutate(citation = case_when(
    journal == "Philosophy of Science" & fpage > 10000 ~ paste0(authall,", ",year,", “", toTitleCase(title),",” ",journal," ",vol,":S",fpage-10000,"–S",lpage-10000,"."),
    journal == "Proceedings of the Aristotelian Society" & year - vol > 1905 ~ paste0(authall,", ",year,", “", toTitleCase(title),",” ",journal," (Supplementary Volume) ",vol,":",fpage,"–",adjlpage,"."),
#    TRUE ~ paste0(authall," (",year,") \"", title,"\" ",journal," ",vol,":",fpage,"-",lpage,".")
    TRUE ~ paste0(authall,", ",year,", “", toTitleCase(title),",” ",journal," ",vol,":",fpage,"–",adjlpage,".")
  )
  )


```

```{r gamma_setup, cache=TRUE}
# Gamma is the probability of article being in a topic
# This retrieves it, and rearranges the topics chronologically

# The list of highly cited articles
load("highly_cited_articles.RData")


all_journals_classifications <- all_journals_gamma %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  ungroup()

all_journals_titles_and_topics <- inner_join(all_journals_classifications, articles, by = "document")

year_topic_mean <- all_journals_titles_and_topics %>% ungroup() %>% 
  group_by(topic)  %>% 
  dplyr::summarize(date = mean(year)) %>% 
  mutate(rank = rank(date))

relabeled_articles <- merge(all_journals_titles_and_topics, year_topic_mean) %>% 
  select(-topic) %>% 
  dplyr::rename(topic = rank) %>% 
  mutate(mcitation = case_when(
    journal == "Philosophy of Science" & fpage > 10000 ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ ",vol,":S",fpage-10000,"–S",lpage-10000,"."),
    journal == "Proceedings of the Aristotelian Society" & year - vol > 1905 ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ (Supplementary Volume) ",vol,":",fpage,"–",adjlpage,"."),
#    TRUE ~ paste0(authall," (",year,") \"", title,"\" ",journal," ",vol,":",fpage,"-",lpage,".")
    TRUE ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ ",vol,":",fpage,"–",adjlpage,".")
  )
  )

high_cite_gamma <- merge(highly_cited, relabeled_articles, by = "document") %>% arrange(desc(Cites))


relabeled_gamma <- merge(all_journals_gamma, year_topic_mean) %>%
  as_tibble() %>%
  select(-topic) %>%
  dplyr::rename(topic = rank)

# Some code left over from before this was written in tidy
relabeled_gamma <- merge(relabeled_gamma, articles, by = "document") %>%
  select(document, gamma, topic, year, journal, length) %>%
  mutate(length = case_when(
                            is.na(length) ~ 1,
                            TRUE ~ length
                            ))
```

```{r import_categories, cache=FALSE}
# The big category csv
# Could type this in from R, but easier to edit as CSV
require(readr)
the_categories <- read_csv("category-summary-22031848-90-r15.csv")
```

```{r graphsetup, cache=TRUE}
# All the data for the big graphs in chapter 3
# Gotta build them first because some of them get drawn on for the individual topics in chapter 2
  article_demonimator <-  relabeled_articles  %>%
    group_by(year) %>%
    dplyr::summarise(d = n_distinct(document))

  page_demonimator <- relabeled_articles %>%
    group_by(year) %>%
    dplyr::summarise(d = sum(length, na.rm = TRUE))
  
  count_numerator <- relabeled_articles  %>%
    group_by(year, topic) %>%
    dplyr::summarise(y = n_distinct(document)) %>%
    ungroup() %>%
    complete(year, topic, fill = list(y = 0))

  count_ratio <- merge(count_numerator, article_demonimator) %>%
    mutate(y = y/d)
  
  page_count_numerator <- relabeled_articles %>%
    group_by(year, topic) %>%
    dplyr::summarise(y = sum(length, na.rm=TRUE)) %>%
    ungroup() %>%
    complete(year, topic, fill = list(y = 0))

  page_count_ratio <- merge(page_count_numerator, page_demonimator) %>%
    mutate(y = y/d)
  
  weight_numerator <- relabeled_gamma %>%    
    group_by(year, topic) %>%
    dplyr::summarise(y = sum(gamma)) %>%
    ungroup() %>%
    complete(year, topic, fill = list(y = 0))

  weight_ratio <- merge(weight_numerator, article_demonimator) %>%
    as_tibble() %>%
    mutate(y = y/d)
  
  page_weight_numerator <- relabeled_gamma %>%    
    group_by(year, topic) %>%
    mutate(gl = gamma * length) %>%
    dplyr::summarise(y = sum(gl, na.rm = TRUE)) %>%
    ungroup() %>%
    complete(year, topic, fill = list(y = 0))

  page_weight_ratio <- merge(page_weight_numerator, page_demonimator) %>%
    mutate(y = y/d)
  
  journalgamma <- relabeled_gamma  %>%
    group_by(year, topic, journal) %>%
    dplyr::summarise(gamsum = sum(gamma)) %>%
    ungroup() %>%
    complete(year, topic, journal, fill = list(gamsum = NA))
  
  yearjournalcount <- relabeled_articles %>%
    group_by(journal, year) %>%
    dplyr::summarise(annual = n_distinct(document))
  
  journalgamma_frequency <- merge(journalgamma, yearjournalcount) %>%
    mutate(gamfre = gamsum / annual) %>%
    complete(year, journal, topic, fill = list(gamfre = NA))  
```

```{r astopic, cache=FALSE}
# The topics are naturally numbers so they get continuous colors
# Turning them into factors makes the automatic coloring work
# I also use this to relabel and rearrange the journal titles
# Burp
  count_numerator$topic <- as.factor(count_numerator$topic)
  page_count_numerator$topic <- as.factor(page_count_numerator$topic)
  weight_numerator$topic <- as.factor(weight_numerator$topic)
  page_weight_numerator$topic <- as.factor(page_weight_numerator$topic)
  count_ratio$topic <- as.factor(count_ratio$topic)
  page_count_ratio$topic <- as.factor(page_count_ratio$topic)
  weight_ratio$topic <- as.factor(weight_ratio$topic)
  page_weight_ratio$topic <- as.factor(page_weight_ratio$topic)
  journalgamma_frequency$topic <- as.factor(journalgamma_frequency$topic)
  
journal_order <- c("Mind", "Proceedings of the Aristotelian Society", "Ethics", "Philosophical Review",  "Analysis","Philosophy and Public Affairs", "Journal of Philosophy", "Philosophy and Phenomenological Research", "Philosophy of Science", "Noûs",  "The Philosophical Quarterly", "British Journal for the Philosophy of Science")

journalgamma_frequency$journal <- factor(journalgamma_frequency$journal, levels = journal_order)
```

```{r keywordsetup, cache=TRUE}
# Generate two things for the topic summaries in chapter 2
# First, the keywords, which are a complicated thing to make, since the beta matrix is huge
# Second, the highly cited list, which is actually fairly easy -though have to check it is up to date
# Burp
  phil_topics <- tidy(thelda, matrix = "beta")

  relabeled_topics <- merge(phil_topics, year_topic_mean) %>%
    as_tibble() %>%
    select(-topic) %>%
    dplyr::rename(topic = rank)

  word_score <- relabeled_topics %>%
    group_by(term) %>%
    dplyr::summarise(sumbeta = sum(beta)) %>%
    arrange(desc(sumbeta))

 busy_topics <- merge(relabeled_topics, word_score) %>%
   filter(sumbeta > 0.00005 * cats) %>%
   mutate(score = beta/sumbeta) %>%
   arrange(desc(score))

 distinctive_topics <- busy_topics %>%
   group_by(topic) %>%
   top_n(15, score) %>%
   ungroup() %>%
   arrange(desc(-topic))

 short_keywords <- c()

 short_keywords <- tribble(
   ~topic, ~distinctive_words)
  

```

```{r overall_stats, cache=TRUE}
# Put all the stats for chapter 2 into one table that I can draw out
overall_stats <- tibble(
  the_topic = 1:90,
  r_count = 0,
  w_count = 0,
)


for (i in 1:90){
  overall_stats$r_count[i] <- nrow(relabeled_articles %>% filter(topic == i))
  overall_stats$w_count[i] <- round(sum(filter(relabeled_gamma, topic == i)$gamma), 1)
}

overall_stats <- overall_stats %>%
  mutate(r_percent = round(r_count / nrow(relabeled_articles), 3)) %>%
  mutate(r_rank = order(order(r_count, decreasing=TRUE))) %>%
  mutate(w_percent = round(w_count / nrow(relabeled_articles), 3)) %>%
  mutate(w_rank = order(order(w_count, decreasing=TRUE))) %>%
  mutate(w_over_r = w_count - r_count) %>%
  mutate(gap_rank = order(order(w_over_r, decreasing=TRUE))) %>%
  mutate(wy = 0)

temp_years <- relabeled_gamma %>%
  select(topic, gamma, year) %>%
  mutate(yg = gamma * year)

for (i in 1:90){
  overall_stats$wy[i] = round(sum(filter(temp_years, topic == i)$yg)/overall_stats$w_count[i], 1)
}

overall_stats <- overall_stats %>%
  add_column(mean_y = 0, median_y = 0, modal_y = 0)

for (i in 1:90){
  temp_years <- relabeled_articles %>% filter(topic == i)
  overall_stats$mean_y[i] = round(mean(temp_years$year), 1)
  overall_stats$median_y[i] = round(median(temp_years$year), 0)
  overall_stats$modal_y[i] = as.numeric(names(sort(-table(temp_years$year)))[1])
}
```

```{r first-cap, cache=TRUE}
# A function for capitalising first letter of string and leaving everything else as is
# Str_to_sentence makes the names lower case, so doesn't work for this purpose
require(stringr)
fcap <- function(x){
  paste0(
    str_to_upper(str_sub(x, 1,1)),
    str_sub(x, 2, nchar(x))
  )
}
```


```{r cross-topic, cache=TRUE}
# Average gamma in topic y for articles in topic x
# Useful quick-and-dirty overlap between topics measure
cross_topic <- function(x, y){
  t <- relabeled_articles %>% filter(topic == x)
  s <- relabeled_gamma %>% filter(topic == y, document %in% t$document)
  sum(s$gamma)
}
```

```{r cross-topic-two, cache=TRUE}
# Create tables from the cross-topic function
short_articles <- relabeled_articles %>%
  select(document, hometopic = topic)

cross_topic_tibble <- relabeled_gamma %>%
  inner_join(short_articles, by = "document") %>%
  group_by(topic, hometopic) %>%
  dplyr::summarise(g = mean(gamma)) %>%
  filter(!topic == hometopic) %>%
  select(topic = hometopic, othertopic = topic, g) %>%
  arrange(topic, othertopic)

closest_neighbour <- cross_topic_tibble %>%
  group_by(topic) %>%
  top_n(1, g)

closest_neighbour_inverse <- cross_topic_tibble %>%
  group_by(othertopic) %>%
  top_n(1, g) %>%
  arrange(othertopic)

furthest_neighbour <- cross_topic_tibble %>%
  group_by(topic) %>%
  top_n(1, -g)

furthest_neighbour_inverse <- cross_topic_tibble %>%
  group_by(othertopic) %>%
  top_n(1, -g) %>%
  arrange(othertopic)

```


```{r kable-for-article-probabilities, cache=TRUE}
# A function for making a quick table of an article's distribution over the 90 topics
# This is first used in chapter 2
individual_article <- function(x){
  temp_article <- relabeled_articles %>%
  filter(document == x)

temp_gamma <- relabeled_gamma %>%
  filter(document == x, gamma > 0.02) %>%
  select(topic, gamma) %>%
  inner_join(the_categories, by = "topic") %>%
  select(sub_lower, gamma) %>%
  mutate(sub_lower = fcap(sub_lower)) %>% 
  arrange(-gamma)

kable(temp_gamma, 
      col.names = c("Subject", "Probability"), 
      caption = paste0(temp_article$authall[1], ", “", temp_article$title[1], ".”"),
      digits = c(0, 4)) %>% 
  kable_styling(full_width = F)
}
```

```{r dt-for-author-articles, cache=TRUE}
author_dt <- function(x, y, z){
cat("<table style=\'margin-bottom:0px\'>",
    paste0("<caption>",
           paste0("(#tab:t",
                  z,
                  ")"
           ),
           paste0("Articles with author ",
                  y,
                  "."),
           "</caption>",
           "</table>", sep =" ")
)
datatable(relabeled_articles %>%
          filter(auth1 %in% x | auth2 %in% x | auth3 %in% x) %>%
          arrange(topic) %>%
         inner_join(the_categories, by = "topic") %>%
          select(year, citation, subject, gamma),           
          colnames = c("Year", "Article", "Subject", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:3)),
                         pageLength = 10
                         )#,
#          caption = htmltools::tags$caption(paste0("Articles with author ",y,"."), style = "font-weight: bold")
    )%>%
      formatSignif('gamma',4) %>%
      formatStyle(1:4,`text-align` = 'left')  
} 
```

```{r kable-for-author-articles, cache=TRUE}
author_kable <- function(x, y){
kable(relabeled_articles %>%
          filter(auth1 %in% x | auth2 %in% x | auth3 %in% x) %>%
          arrange(year) %>%
        mutate(gamma = round(gamma, 4)) %>%
         inner_join(the_categories, by = "topic") %>%
          select(year, mcitation, subject, gamma),           
          col.names = c("Year", "Article", "Subject", "Probability"), 
#          caption = paste0("Articles with Author ",y), style = "font-weight: bold")
          caption = paste0("Articles with author ",y,"."))
  } 
```

```{r words-by-year, cache=TRUE}
# Use this to get words to graph
# This gets used in chapter 7

article_year_tibble <- articles %>%
  select(document, year)

word_year_count <- all_journals_tibble %>%
  inner_join(article_year_tibble, by = "document") %>%
  group_by(year) %>%
  dplyr::summarise(a = sum(wordcount))

word_year_journal_count <- all_journals_tibble %>%
  inner_join(articles, by = "document") %>%
  group_by(year, journal) %>%
  dplyr::summarise(a = sum(wordcount))


```

```{r word-frequency-graphs, cache=TRUE}
# A pair of functions that turn a string of words into a graph of each of their frequencies over time
# Have to generate the data first, because would be enormous table to have it all stored
# These get used in chapter 7
word_year_frequency <- function(x){
  left_join(word_year_count, all_journals_tibble %>%
    filter(word == x) %>%
    left_join(article_year_tibble, by = "document") %>%
    group_by(year) %>%
    dplyr::summarise(c = sum(wordcount)),
    by = "year") %>%
    replace_na(list(c = 0)) %>%
    mutate(f = c / a) %>%
    mutate(term = x)
}

frequency_summary <- function(x){
  denom <- sum(all_journals_tibble$wordcount)
  numer <- sum(filter(all_journals_tibble, word == x)$wordcount)
  zz <- numer/denom
  tribble(
    ~term, ~the_mean,
    x, zz
  )
}


word_frequency_graphs <- function(x){
  t <- lapply(x, word_year_frequency) %>% bind_rows()
  h <- lapply(x, frequency_summary) %>% bind_rows()
ggplot(t, aes(x = year, y = f, color = term, group = term)) +
  freqstyle +
  geom_point(size = 0.6, alpha = 0.8) +
#  geom_hline(yintercept = h$the_mean, col = group) +
  stat_summary(fun = mean, 
               aes(x = 1950, yintercept = ..y.., group = term), 
               geom = "hline",
               linetype = "dashed",
               size = 0.2) +
  scale_x_continuous(minor_breaks = 10 * 1:201,
                     expand = expansion(mult = c(0.01, 0.01))) +
  scale_y_continuous(expand = expansion(mult = c(0.01, .03)),
                     minor_breaks = scales::breaks_pretty(n = 12),
                     breaks = scales::breaks_pretty(n = 3),
                     labels = function(x) ifelse(x > 0, paste0("1/",round(1/x,0)), 0)) +
  #  scale_y_continuous(labels = scale_inverter) +
  labs(x = element_blank(), y = "Word frequency") +
  theme(legend.title = element_blank())
}

word_frequency_graph_alt_text <- function(x){
  t1 <- paste0(
    "A scatterplot showing the frequency of the words ",
    paste(x, collapse=", "),
    ". "
  )
  for (ijk in 1:length(x)){
    temp <- word_year_frequency(x[ijk]) %>% 
      mutate(f = round(f * 1000000))
    temp_max <- temp %>% slice_max(f, n = 1)
    temp_min <- temp %>% slice_min(f, n = 1)
    t1 <- paste0(t1,
            paste0(
              "The word ",
              x[ijk],
              " appears, on average across the years, ",
              round(mean(temp$f)),
              " times per million words, and in the median year, it appears ",
              round(median(temp$f)),
              " times per million words. Its most frequent occurrence is in ",
              temp_max$year[1],
              " when it appears ",
              temp_max$f[1],
              " times per million words, and its least frequent occurrence is in ",
              temp_min$year[1],
              " when it appears ",
              temp_min$f[1],
              " times per million words. "
            ))
  }
  t1
}

word_year_journal_frequency <- function(x, y){
  left_join(word_year_journal_count %>% 
              filter(journal == y), 
            all_journals_tibble %>%
              filter(word == x) %>%
              left_join(articles, by = "document") %>%
              filter(journal == y) %>%
              group_by(year) %>%
              dplyr::summarise(c = sum(wordcount)),
            by = "year") %>%
    replace_na(list(c = 0)) %>%
    mutate(f = c / a) %>%
    mutate(term = x)
}

journal_word_frequency_graph_alt_text <- function(x, j){
  t1 <- paste0(
    "A scatterplot showing the frequency of the words ",
    paste(x, collapse=", "),
    " in the journal ",
    j,
    ". (All stats from now on just refer to that journal.) "
  )
  for (ijk in 1:length(x)){
    temp <- word_year_journal_frequency(x[ijk], j) %>% 
      mutate(f = round(f * 1000000)) %>% 
      ungroup()
    temp_max <- temp %>% slice_max(f, n = 1)
    temp_min <- temp %>% slice_min(f, n = 1)
    t1 <- paste0(t1,
            paste0(
              "The word ",
              x[ijk],
              " appears, on average across the years, ",
              round(mean(temp$f)),
              " times per million words, and in the median year, it appears ",
              round(median(temp$f)),
              " times per million words. Its most frequent occurrence is in ",
              temp_max$year[1],
              " when it appears ",
              temp_max$f[1],
              " times per million words, and its least frequent occurrence is in ",
              temp_min$year[1],
              " when it appears ",
              temp_min$f[1],
              " times per million words. "
            ))
  }
  t1
}
```

```{r word-era-graphs, cache=TRUE}
word_era_graphs <- function(x, y){
  word_freq_data <- era_words %>%
    filter(word %in% slice(common_words, 1:x)$word) %>%
    filter(epoch == y) %>%
    arrange(-f) %>%
    ungroup() %>%
    slice(1:5)
  print(word_frequency_graphs(word_freq_data$word))
}

word_era_graphs_alt_text <- function(x, y){
    word_freq_data <- era_words %>%
    filter(word %in% slice(common_words, 1:x)$word) %>%
    filter(epoch == y) %>%
    arrange(-f) %>%
    ungroup() %>%
    slice(1:5)
    word_frequency_graph_alt_text(word_freq_data$word)
}
```

```{r imprint-setup, cache=TRUE}
# Extract the gammas from imprint_lda and renumber chronologically
imprint_gamma<- as_tibble(imprint_lda$topics, rownames = NA) %>%
  rownames_to_column(var = "document") %>%
  pivot_longer(-document) %>%
  select(document, topic = name, gamma = value) %>%
  mutate(topic = as.integer(topic)) %>%
  inner_join(year_topic_mean, by = "topic") %>%
  select(document, topic = rank, gamma) %>%
  arrange(document, gamma)

# Sum gammas over the 54 Imprint articles
imprint_summary <- imprint_gamma %>%
  group_by(topic) %>%
  dplyr::summarise(g = sum(gamma))

# Top gamma for each article 
imprint_top_gamma <- imprint_gamma %>%
  group_by(document) %>%
  top_n(1, gamma)
```

```{r load-bad-lda, cache=TRUE}
# The LDA I use for the buzzwords section right at the end
# Everything goes wrong when this is cached and doesn't load
load("t90t100.RData")
```

```{r setup-bad-lda, cache=TRUE}

# Just replicate the normal setup, but with The Bad LDA
bad_gamma <- tidy(refinedlda, matrix = "gamma") %>%
  filter(topic == 6) %>%
  inner_join(articles, by = "document") %>%
  select(citation, gamma, journal, year) %>%
  arrange(-gamma)

bad_gamma_year <- bad_gamma %>%
  group_by(year) %>%
  summarise(g = sum(gamma))%>%
  inner_join(article_demonimator, by = "year") %>%
  mutate(y = g / d)
  
# Graph absolute and ratio

bad_gamma_year_journal <- bad_gamma %>%
  group_by(year, journal) %>%
  summarise(g = sum(gamma)) %>%
  inner_join(yearjournalcount, by = c("year", "journal")) %>%
  mutate(y = g / annual)

bad_gamma_year_journal$journal <- factor(bad_gamma_year_journal$journal, levels = journal_order)

# Graph absolute and ratio

bad_beta <- tidy(refinedlda, matrix = "beta") %>%
  filter(topic == 6) %>%
  filter(term %in% all_word_count$word) %>%
  select(word = term, beta) %>%
  inner_join(all_word_count, by = "word") %>%
  mutate(y = beta/total * sum(all_word_count$total)) %>%
  arrange(-y)
```

```{r comparison-beta-tables, cache=TRUE}
good_beta <- relabeled_topics %>%
  filter(topic == 90) %>%
  filter(term %in% all_word_count$word) %>%
  select(word = term, beta) %>%
  inner_join(all_word_count, by = "word") %>%
  mutate(y = beta/total * sum(all_word_count$total)) %>%
  arrange(-y)

kant_beta <- relabeled_topics %>%
  filter(topic == 32) %>%
  filter(term %in% all_word_count$word) %>%
  select(word = term, beta) %>%
  inner_join(all_word_count, by = "word") %>%
  mutate(y = beta/total * sum(all_word_count$total)) %>%
  arrange(-y)

olp_beta <- relabeled_topics %>%
  filter(topic == 24) %>%
  filter(term %in% all_word_count$word) %>%
  select(word = term, beta) %>%
  inner_join(all_word_count, by = "word") %>%
  mutate(y = beta/total * sum(all_word_count$total)) %>%
  arrange(-y)
```

```{r category_gamma_setup, cache=TRUE}
# I recreate category_gamma from inside the script
# Need to build all_dtm because gotta assign probabilities from within the binary sorts to all articles
word_list <- all_journals_tibble %>%
  filter(wordcount > 3) %>%
  filter(!word %in% short_words) %>%
  filter(document %in% articles$document)


all_dtm <- cast_dtm(word_list, document, word, wordcount)
```

```{r category_gamma_derive, cache=TRUE}
split_check <- filter(the_categories, cat_num == 13)$topic

category_gamma <- relabeled_gamma %>%
  select(document, topic, gamma)

for (i in split_check){
  load(paste0("binary_lda/lda_",i,".RData"))
  temp_lda <- posterior(binary_lda, all_dtm)
  temp_gamma<- as_tibble(temp_lda$topics, rownames = NA) %>%
    rownames_to_column(var = "document") %>%
    pivot_longer(-document) %>%
    select(document, btopic = name, bgamma = value)
  temp_old_gamma <- category_gamma %>%
    filter(topic == i) %>%
    inner_join(temp_gamma, by = "document") %>%
    mutate(topic = as.numeric(topic)) %>%
    mutate(btopic = as.numeric(btopic)) %>%
    mutate(topic = 100*topic + btopic) %>%
    mutate(gamma = gamma*bgamma) %>%
    select(-btopic, -bgamma)
 category_gamma <- category_gamma %>%
   filter(!topic == i) %>%
   bind_rows(temp_old_gamma)
}

category_gamma <- category_gamma %>%
  inner_join(the_categories, by = "topic")

articles_by_category <- category_gamma %>%
  group_by(document) %>%
  top_n(1, gamma) 
```

```{r category_setup, cache=TRUE}
category_gamma_graph <- category_gamma %>%
  inner_join(articles, by = "document") %>%
  select(journal, year, category = cat_name, gamma) %>%
  group_by(journal, year, category) %>%
  dplyr::summarise(g = sum(gamma)) %>%
  ungroup() %>%
  complete(journal, year, category, fill = list(g = NA))

category_year <- category_gamma_graph %>%
  group_by(category, year) %>%
  dplyr::summarise(y = sum(g, na.rm=TRUE))

year_denominator <- category_gamma_graph %>%
  group_by(year) %>%
  dplyr::summarise(d = sum(g, na.rm = TRUE))

category_frequency <- inner_join(category_year, year_denominator, by = "year") %>%
  mutate(f = y / d)
```

Anglophone philosophy in the twentieth century was centered, to an unprecedented extent, around journals: periodical publications that aimed to present (one vision of) the best philosophical work of the moment. By looking at the trends across these journals, we can see important trends in philosophy itself.

But looking at the journals is easier said than done. Most major journals have published thousands of articles. To get a guide to philosophy as a whole, and not just to one particular vision of it, it's necessary to look at several different journals, and tens of thousands of articles. This is impossible for any human to do.

Fortunately, it's not necessary to rely on humans. Two technological developments have made it practical to use computers to do at least some of the reading.

The first development was that JSTOR used optical character-recognition (OCR) software to create text versions of many archived journals. They combined this with the original electronic versions of recent issues to create a full library of the text of many leading journals. And, crucially, they made this library available to the general public.

The second development was that personal computers have gotten fast enough that it is (just barely) practical to run text-mining algorithms over libraries as large as the ones JSTOR provides on personal computers.^[_Practical_ here is a relative term; the models I primarily use here took eight to ten hours to complete on pretty good computers. But that's fine if a computer can be left running overnight.] So even without having to use tools beyond what's available in a typical university office, these algorithms can be used to see trends in the journal data. 

This study focuses on these twelve journals.

```{r journal-table, cache=FALSE}
options(dplyr.summarise.inform = FALSE)

journals_summary <- articles %>%
  dplyr::group_by(journal) %>%
  dplyr::summarise(fyear = min(year), art = n_distinct(document), .groups = "keep") %>% 
  ungroup() %>% 
  dplyr::mutate(journal = paste0("_",journal,"_"))

kable(journals_summary, 
      col.names = c("Journal", "First Year", "Number of Articles"), 
      align=c("l", "c", "c"),
      caption = "The twelve journals that this book talks about."
  )
    
```

That table shows the twelve journals I'm using, the year they started publication, and how many articles from each journal I'm analyzing. That doesn't include everything the journal published, since I'm only looking at the research articles they published in English. So I'm not looking at book reviews, but also not at editorials, introductions, corrections and the like. Because the text-mining algorithms really require a single language, I'm also excluding everything that was published in languages other than English.^[I had no idea how many articles in French, German and Spanish were published in these journals over the years.] But even still, there are a lot of articles to look at, and they include many of the most important works in philosophy over that time period.

The data comes from JSTOR's [Data for Researchers](https://jstor.org/dfr/), which provides, for each article in these journals, a file with a list of the words in the article and the number of times those words appear. Though as will become important in what follows, this data separates hyphenated words, excludes various common words like _and_ and _the_, and also excludes all one and two letter words.

JSTOR has a moving window, which means it doesn't make available the latest issues of all of the journals. When I started this project, the last year that I could get access to all issues of all twelve journals was 2013. So this study stops in 2013. I make a number of anecdotal observations about what's happened since 2013 during this book. And at the end I come back to one study on work from 2019. But this is primarily a history of the years 1876–2013.

I used the data from JSTOR to build a Latent Dirichlet Allocation (LDA) model of the journals using the [_topicmodels_](https://cran.r-project.org/package=topicmodels) package written by [Bettina Grün](http://ifas.jku.at/gruen/) and [Kurt Hornik](http://statmath.wu.ac.at/~hornik/), and described by them in @GrunHornik2011.

An LDA model takes the distribution of words in articles and comes up with a probabilistic assignment of each paper to one of a number of topics. The number of topics has to be set manually, and after some experimentation it seemed that the best results came from dividing the articles up into `r as.english(cats)` topics. And a lot of this book discusses the characteristics of these `r as.english(cats)` topics. But to give you a more accessible sense of what the data looks like, I'll start with a graph that groups those topics together into familiar contemporary philosophical subdisciplines, and displays their distributions in the twentieth and twenty-first century journals.^[I'm leaving the nineteenth century off this graph because it is odd in various ways, and best treated separately. I'll say much more about it as we proceed.]

```{r initial-graph-style, cache = FALSE}
facetstyle <-   theme_minimal() +
  theme(text = element_text(family="Lato"),
        plot.title = element_text(size = rel(1),
                                  family = "Lato",
                                  face = "bold",
                                  margin = margin(0, 0, 10, 0)),
        strip.text = element_blank(),
        panel.spacing.x = unit(-0.05, "lines"),
        panel.background = element_blank(),
        panel.spacing.y = unit(1, "lines"),
        axis.title.x = element_text(size = rel(1),
                                    margin = margin(t = 6, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(size = rel(1),
                                    margin = margin(t = 0, r = 8, b = 0, l = 0)),
        panel.grid.major.y = element_line(color = "grey85", size = 0.07),
        panel.grid.minor.y = element_line(color = "grey85", size = 0.03),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        legend.position="none")
spaghettistyle <- facetstyle +
  theme(panel.grid.major.y = element_line(color = "grey80", size = 0.08),
        panel.grid.minor.y = element_line(color = "grey85", size = 0.04),
        legend.text = element_text(size = rel(0.5)),
        plot.caption = element_text(size = rel(0.7))
        )
freqstyle <-   spaghettistyle +
  theme(legend.text = element_text(size = rel(0.75)),
          panel.grid.major.y = element_line(color = "grey85", size = 0.08),
        legend.position = "right")
chap_two_facet_labels <- tribble(
  ~journal, ~short_name,
  "Mind", "Mind",
  "Philosophical Review", "Philosophical Review",
  "Journal of Philosophy", "Journal of Philosophy",
  "Noûs", "Noûs",
  "Proceedings of the Aristotelian Society", "Aristotelian Society",
  "Analysis", "Analysis",
  "Philosophy and Phenomenological Research", "PPR",
  "The Philosophical Quarterly", "Philosophical Quarterly",
  "Ethics", "Ethics",
  "Philosophy and Public Affairs", "Philosophy & Public Affairs",
  "Philosophy of Science", "Philosophy of Science",
  "British Journal for the Philosophy of Science", "BJPS"
)

# Changing the text in the theme just changes the labels on the axis etc
# To change what's in the graph itself, you need to use these commands
# And I'm using a hack to get the titles for the graph appearing in the graph, so...
update_geom_defaults("text", list(family = "Lato"))
update_geom_defaults("label", list(family = "Lato"))
```

```{r first-facet-graph, fig.cap = "Proportion of articles in each category per year.", dev = 'png', fig.alt = alt_text, cache=TRUE}
tt <- filter(category_frequency, year > 1899) # Remove 19th Century from graph
category_graph_labels <- tt %>%
  group_by(category) %>%
  summarise(year = median(year)) %>% # Put label at middle horizontally
  mutate(f = max(tt$f) * 1.2) # Put label above the highest value on the graph
ggplot(tt, 
       aes(x = year, y = f, color=category, group=category)) +
  geom_text(data = category_graph_labels,
            mapping = aes(label = category),
            vjust = "inward", 
            hjust = "inward",
            fontface = "bold", 
            size = 3) +
  labs(x = element_blank(), y = "Proportion of articles", title = "Trends in Philosophical Categories") +
  geom_point(size = 0.15) + 
  facet_wrap(~category, ncol=3) +  
  scale_x_continuous(expand = expansion(mult = c(0.05, 0.05)),
                     breaks = 25 * 77:80) +
  scale_y_continuous(breaks = c(0.1,0.2),
                     expand = expansion(mult = c(0, .03))) +
  facetstyle

alt_text <- paste0(
  "Scatterplots showing the proportion of articles in each year that are in each of the 12 categories the book uses. The categories are ",
  paste(category_graph_labels$category, collapse = ", "),
  ". The frequencies are described briefly in the text below, then in much more detail in chapter 4. The key point here is that even though this is a scatterplot, it looks like a line graph. The year-to-year changes in how much each topic is represented are tiny."
)
```

These graphs aren't smoothed—this is just a scatterplot of how prevalent each category is over time. The continuity in the graphs comes from continuity in the underlying subject matter. Although philosophy changes over time, the changes tend to be small and smooth—at least at this level of resolution. 

There are, however, a few big trends that are visible even at this resolution. 

The categories of Ethics and Philosophy of Science have fairly steady rises over the graphs. What primarily drives that is that journals thought of as specialist journals, like _Ethics_ and _Philosophy of Science_, became more and more specialized over time. There is much more topic overlap between these journals and so-called generalist journals in the 1940s and 1950s than in the 1990s and 2000s. 

For much of the history of these journals, they publish approximately zero articles that look anything like contemporary epistemology. Edmund Gettier's famous article ["Is Justified True Belief Knowledge?"](https://philpapers.org/rec/GETIJT-4) [@Gettier1963] doesn't advance an existing debate; it starts a debate. But that debate doesn't really get going for another decade or more.

On the other hand, in the middle of that graph above is a chart that does not reflect anything in contemporary philosophy: Idealism. The extent to which Idealism dominated philosophy before World War I, and continued to be a huge presence between the wars, quite astounded me. And the model is using a fairly narrow definition of Idealism here. Idealist-influenced works on social and political philosophy, such as work that engages heavily with Bergson and Santayana, is another huge field, but it's included in social and political philosophy above.

The anecdote I'd always been told about the state of British philosophy in the early part of the century was that you could get a good sense of things by just looking at the issue of _Mind_ that included ["On Denoting"](https://philpapers.org/rec/RUSOD) [@Russell1905]. It's wedged between two big articles on Idealism. Indeed, the model classifies the articles just [before](https://philpapers.org/rec/RFAPVA-2) and just [after](https://philpapers.org/rec/GIBPAP) Russell's in the Idealism category. But I, at least, had no idea how dominated British philosophy was by Idealism, or how long this dominance lasted.

This was far from the only thing that surprised me about the data. Some of these surprises probably reflect my ignorance, but some of them may be of wider interest.

As you might have gathered from the quantity of work on Idealism, there just isn't much space for the work that we now think is most significant in late-nineteenth or early-twentieth century philosophy. Frege, Moore, Russell and even Wittgenstein have virtually zero impact on the journals at the time they publish. They do have an impact later, with the starting times of their influence being in more or less in reverse chronological order to when they actually wrote. But they are invisible in real time. This is especially striking for Moore, who does not seem to have used his influence as the editor of _Mind_ to steer the journal particularly in the direction of his work. The contrast with the generation of editors who came after him, on either side of the Atlantic, will be striking.

As well as Idealism, two other broadly antirealist schools make a major impact on the journals in the first half of the twentieth century: pragmatism and positivism. But the impacts differ greatly in size. Idealism has a much bigger impact than pragmatism, and pragmatism has a much bigger impact than positivism. Indeed, the main way that positivism is visible is that it has a very prominent decline phase. The 'one patch per puncture' period of attempts to save the verification principle is big enough that it is basically one of the `r as.english(cats)` topics the model finds. But this model doesn't find a ton of work defending positivism turning up as a distinctive topic, nor does it find a notable falling away in the fields (like metaphysics and mind) that positivists railed against. It's possible that the focus on journals that primarily publish in English explains why I didn't see a "rise of positivism" period, but its absence was striking.

If there is no early analytic/logical atomism visible, and positivism is a small presence that shows up mainly as it is dying, what fields that are part of the standard story of the history of analytic philosophy do show up? Well, the first big one is [ordinary language philosophy](#topic24). This is so big, especially in Britain, that it almost breaks the model. The big assumption that drives the kind of model I'm building is that there is a one-to-one mapping between classes of articles with a distinctive vocabulary, and classes of articles with a distinctive subject matter. That often holds true, but it breaks quite spectacularly in 1950s Britain. A new language, shorn of pomp and circumstance, takes over. And my poor model thinks that all the philosophers have moved on to a wholly new subject matter. But they largely have not—they are just discussing the old subjects using new words.

There is one notable exception to this though. During the ordinary language phase we do see a lot of articles that are about language itself. This is a new thing—we don't see any such articles before then. That's an exaggeration of course—"On Denoting" really was published—but it's true as a generalization. But all of a sudden in the middle of the century there is a spike in interest in [Wittgensteinian philosophy of language](#topic22). That spike falls away almost as quickly as it came, but it changes the field. The space that was taken up by Wittgensteinian philosophy of language is replaced by other work in philosophy of language, influenced in the first place by either Frege, Russell, Quine or Austin. 

There is something about this story that is repeated across the subjects. When one particular topic falls out of fashion, it is usually replaced by one from the same subdiscipline. That's how the very stable lines on the graphs above are generated, although most categories are made up of topics that see sharp rises and falls in the amount of attention they are getting. But philosophy of language is a bit of an exception. Before the Wittgensteinian boom it was invisible in the journal; afterwards it routinely accounted for 10 percent of the published articles. And that 10 percent figure stayed stable across huge shifts in what philosophers of language were talking about. What surprised me was that the stability here was the norm—the sudden appearance of a new field taking up 10 percent of the journal space was what was unusual.

But what interests me as much as these big-picture trends are the little trends underlying them. What particular topics do philosophers talk about, and when do they talk about them? The answer to the latter question is almost always several years later than I had expected. To take one dramatic example, I associate work on [wide content](#topic85) with Kripke and Putnam's work from the early 1970s, and hence with the 1970s as a whole. But it turns out this work is practically invisible in the 1970s journals. ["Meaning and Reference"](https://philpapers.org/rec/PUTMAR-2) [@Putnam1973] shows up, but almost nothing else does until a decade or more later. And that's the general pattern; if you associate a topic with its most famous papers, you'll be misled about when it primarily shows up in the journals.

So I'll spend some time in what follows looking at when familiar topics show up. But I'll also be looking at what shows up that isn't part of contemporary philosophy. I've spent a bit of time talking about Idealism, but it isn't the only thing missing from current journals. There used to be much more work on, broadly construed, [philosophy of history and of sociology](#topic10) than there is now. This has one particular impact that intersects with my other interests. Anglophone philosophers nowadays do spend a bit of time on philosophically significant work from the late eighteenth century. But not much of the work that they look at comes from Philadelphia, United States, or Paris, France, let alone Cap-Haïtien, Haiti. Although it's not like midcentury philosophers paid any attention to Cap-Haïtien in the late eighteenth century either, but they did think a bit more about the philosophical importance of what happened, and what was written, at that time in Philadelphia and Paris. And that seems like a good idea. Also, hopefully one of the benefits of the kind of retrospective I'm writing is it encourages people to look back and see what else we used to spend more time on, and could profitably spend more time on in the future.

## Plan {-}

The book has nine chapters, and it loosely divides into three parts, with three chapters in each part.

The first part concerns the ninety topics that the model divides the articles into. Chapter \@ref(methodology-chapter) is about how and why I made the choices I did in providing the inputs to the model. Chapter \@ref(all-90-topics) goes through each of these ninety topics one at a time. As well as producing some automated statistics and graphs for each topic, and listing which articles are in each topic, I make some small comments about the topic. These are mostly about the content of the topic and its place in philosophical history, though I also spend a lot of time talking about how the model made its division into topics. Most of the comments here are short, though occasionally I decide that one topic needs 2500 words or more. Chapter \@ref(summary-graphs) is about my attempts to make a legible graph of all ninety topics through time. I run through a lot of different representations of the data; most of them are failures, but some of them are more interesting failures than others.

The second part looks at what happens if instead of making a ninety-way division into somewhat novel topics, a twelve-way division into familiar topics is tried. I divide the articles up into common contemporary categories, like epistemology, ethics, metaphysics, and philosophy of science, and I look at the trends in these categories. Chapter \@ref(categorychapter) goes over the trends in the twelve categories. Chapter \@ref(sortingchapter) goes over how I made these divisions, with a special focus on where the categories do, and do not, run into each other. And Chapter \@ref(epistemologychapter) looks at one of these categories: epistemology. This is in part for self-interested reasons; it's the field I work in. But it's in part because the data about epistemology were so surprising. Epistemology, as it is currently practiced, is basically invisible in the journals before World War II. And the most famous part of contemporary epistemology, the so-called Gettier problem about the relationship between knowledge, truth, justification, and belief, plays a surprisingly small role in recent years.

The third part looks at further applications of the model. The first six chapters had used years as the main unit of temporal measurement. In chapter \@ref(eraschapter), I use more coarse-grained measures. First I look at the trends over five "eras" in philosophy, and then over twelve decades. One benefit of doing things this way is that as well as looking at what the model says, I can look at trends in the underlying data directly, and see how well the model is tracking reality. In chapter \@ref(outliers), I look at outliers along various dimensions, both to see where extreme events have been happening and to put some stress on the model. If its most outlandish claims are true, and I think several of them are, then there is more confidence in its more mundane claims. And in chapter \@ref(lookingoutward), I try to look beyond the model. I use the model to compare the early years of the journals with some famous books that were published around the same time. Then I look at how articles in _Philosophers' Imprint_ in 2019 compare to what is seen before 2013. (The big story is the resurgence of interest in historical figures outside the standard canon.) And finally I look at something that almost blew up the project: the very distinctive vocabulary of twenty-first-century philosophy.

## Website Instructions {-}

This book isn't meant to be read cover to cover; it's meant to be picked through like the proverbial box of chocolates. To make this easier, there is a full table of contents on the sidebar. If you click on any of the chapter headings, it takes you to that chapter, and expands to list all the sections in that chapter. Then you can go directly to the section.

The sidebar is also scrollable separately from the rest of the text. If you put the mouse in the sidebar and scroll, it will move the sidebar not the main text. This is particularly important in chapter 2, where there are more sections than will fit on most screens.

But the sidebar takes up quite a bit of real estate, especially on a tablet. So if you want to hide it, either hit _s_, or click the <i class="fa fa-align-justify"></i> button in the top left. If the sidebar is hidden, doing either of those things will restore it. If you're reading on a phone, the sidebar should be hidden by default.

Next to <i class="fa fa-align-justify"></i>, the <i class="fa fa-search"></i> icon brings up a search box for searching the book. Though note that this only searches the text of the book; it doesn't search the various tables. Clicking 'f' also brings up, or hides if it is already present, the search box.

The <i class="fa fa-font"></i> icon lets you adjust the appearance of the book. You can set the background to white (by default), sepia or dark. Unfortunately, this doesn't change the appearance of the graphs, which will be white no matter what you pick. So I don't love how it looks with different colors, but the option is there. You can also change the font so the body of the book is in the serif font that's used in the titles. And you can increase or decrease the font size.

The <i class="fa fa-eye"></i> icon takes you to the GitHub source for the page you're reading. Except in chapter 2, the source documents are split by chapter not section. So if you click on it right now, it will take you to the source file for the whole introductory chapter. But you should still be able to find the part you are looking for quickly.

The <i class="fa fa-info"></i> icon provides some basic information about keyboard shortcuts you can use in the book.

The <i class="fa fa-twitter"></i> icon takes you to Twitter with a pregenerated tweet about how wonderful this book is.

If you use any other social network, the <i class="fa fa-share-alt"></i> icon pulls up a list of other social networks you can share information about the book on. I don't suspect I'll end up with a lot of shares on LinkedIn or Weibo, but just in case that's the social media you most use, it's there.

You can move forward or back between sections using the left and right arrow keys. This is especially useful in long sections where the arrows on the screen might not be immediately visible.

## Acknowledgements {-}

This book relies on resources that a lot of people have made available, usually for free.

The raw data comes from JSTOR's [Data for Research](https://www.jstor.org/dfr/) program. It wouldn't really be possible without the work they did to make that set available.

I initially transformed the JSTOR Ddta into something that could be read in R via some scripts from John Bernau. His paper [Text Analysis with JSTOR Archives](https://doi.org/10.1177%2F2378023118809264) describes some techniques for modeling trends in sociology journals using the JSTOR archives.

Once I had the data in R, I analyed it using the **topicmodels** [package](https://cran.r-project.org/web/packages/topicmodels/index.html) by Bettina Grün and Kurt Hornik.

The idea for analyzing this data using topicmodels to analyze the data in this way comes from [What Is This Thing Called Philosophy of Science? A Computational Topic-Modeling Perspective, 1934–2015](https://doi.org/10.1086/704372), by Christophe Malaterre, Jean-François Chartier, and Davide Pulizzotto [-@Malaterre2019].

As well as those sources, I learned a lot about how to use the **topicmodels** package from [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com) by Julia Silge and David Robinson, and from some articles in Towards Data Science, including those by [Shashank Kapadia](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0) [-@Kapadia2019] and by [Farren tang](https://towardsdatascience.com/beginners-guide-to-lda-topic-modeling-with-r-e57a5a8e7a25). [-@tang2019].

The citation data I use here are from [Google Scholar](http://scholar.google.com), and I accessed them via ["Publish or Perish"](https://harzing.com/resources/publish-or-perish) [@Harzing2007]. Happily, a Mac version of Publish or Perish came out recently; in the past I had set up PC emulators just so I could run it.

Most of the graphics in this book are based on things I learned from Kieran Healy, either from his book [_Data Visualization: A Practical Introduction_](https://kieranhealy.org/publications/dataviz/) [@Healy2019] or from his [blog](https://kieranhealy.org/blog/).

The whole book was put together using the **bookdown** package, primarily built by Yihui Xie [-@bookdown]. This in turn is built on the **Pandoc** language, which was originally built by John MacFarlane. And the code uses tools from the **tidyverse** package throughout, which was originally built by Hadley Wickham. 

The design of the book is modeled on that of [_rstudio4edu_](https://rstudio4edu.github.io/rstudio4edu-book/) by Desirée De Leon and Alison Hill[-@DeLeonHall2019], and I've used a lot of their CSS code under the hood here.

And my daughter Nyaya has helped catch a lot of errors, though given the way I write, this is an endless task.

## GitHub {-}

I've created a GitHub repository for this book. It's at

> https://github.com/bweatherson/lda-bookdown

Most of the code and data you need to recreate this book is there. The exception is that the data files that I downloaded from JSTOR are not there. That's for two reasons. One is that it wasn't completely clear that the license for them would allow redistribution. The other was that they would have been too big to put on GitHub anyway. So to recreate everything I'm doing here, you'll need to download them directly. 

In the ```notes``` directory of the GitHub repository there are some R scripts for converting downloaded JSTOR files into things that can be managed in R. A lot of the things I'm doing here are based on code that's in the code for the book. But it takes about twenty-four to thirty-two hours (on a reasonably fast personal computer) to build the model, so I don't build it anew each time I compile the book. The next section describes how to build a model like the one I'm using.

If you want to go directly to the code for the chapter you're in, the eye icon in the top bar will take you there.

## Replication Instructions {-}

One of the aims of this book is to encourage others to use similar tools to produce better books and papers. This book has many flaws, some of which come from me not knowing as much as I did when I started the project as I do now, and some of which are just my limitations. I'm sure others can do better. So I've tried throughout to be as clear as possible about my methodology, so as to provide an entry point for anyone who wants to set out on a similar project.

This section contains step-by-step instructions for how to build a small-scale model of the kind I'm using. The next chapter discusses the many choice points on the way from small-scale models like this to the large-scale model I use in the book, but it's helpful to have a small example to start with. I'm going to download the text for issues of the _Journal of Philosophy_ in the 1970s, and build a small (ten-topic) model on them. These instructions assume basic familiarity with R, and especially with **tidyverse**. If you don't have that basic familiarity, a good getting-started guide for basic familiarity is Jenny Bryan's [STAT 545: Data wrangling, exploration, and analysis with R](https://stat545.com), especially chapters 1, 2, 5, 6, 7, 14 and 15. OK, I assume readers are familiar with the basics of R, it's time to do some basic text mining.

Go to https://jstor.org/dfr and set up a free account.

Download the [list of journals JSTOR has](https://www.jstor.org/kbart/collections/all-archive-titles?contentType=journals&fileFormat=xlsx). That link will take you to the Excel file; if you're dedicated to plain text there is [also a text version available](https://www.jstor.org/kbart/collections/all-archive-titles?contentType=journals&fileFormat=csv), but it isn't a lot of fun to use. The key part of that file is the column **title_id**. That gives you the code you need to refer to each journal.

Back at https://jstor.org/dfr go to "Create a Dataset" and use "jcode:(jphilosophy)", or whatever the title_id for your desired journal is, to get a data set.

![Finding all _Journal of Philosophy_ articles](instruct_1.png)

We are going to restrict dates, so let's just do the 1970s. Type in the years you want, for us it's 1970 to 1979, and click "Update Results".

![Restricting to the 1970s](instruct_2.png)

Click "Request Dataset". You need the metadata and the unigrams, and you need to give it a name (but you won't use it at any time).

![What data to get](instruct_4.png)

Once again, click "Request Dataset". You'll get this somewhat less than reassuring popup.

![Uh oh—waiting time](instruct_3.png)

But despite it saying that it may take up to two hours, in fact you normally get the data in minutes, even seconds. You'll get an email (at the address you used for registration) saying it's ready. Clicking the link in that email will get you a zip file. And in that zip file there are two directories: ```ngram1``` and ```metadata```.

We want to put these somewhere memorable. I'll put the first in ```data/ngram/jphil``` and the second in ```data/metadata/jphil```. (So **data** is a subdirectory of my main working directory. And it has two subdirectories in it, ```ngram``` and ```metadata```. And each of those have a subdirectory for each journal being analyzed.) It's good to keep the ngrams and the metadata in separate places, and it will be very useful (actually essential) to the code I'm about to run to use the same directory name for where a particular journal's metadata is, and where its words are.

There is a hitch here that I should be able to figure out in R, but I couldn't. As things come out, I ended up with names that didn't have spaces in them. So the author of ["Should We Respond to Evil with Indifference"](https://philpapers.org/rec/WEASWR) was BrianWeatherson, not Brian Weatherson. There was probably a way to fix this at the importing stage, but doing so required more understanding of XML files than I have. So instead I came up with a hack. In each journal directory under inside ```metadata```, go to the terminal and run this command:

```
find . -name '*.xml' -print0 | xargs -0 sed -i "" "s/<surname>/<surname> /g"
```

This adds a space before each surname. So in the surname field, that article goes from having the value "Weatherson" to having the value “ Weatherson”. And now it can be concatenated with "Brian" to produce a reasonable looking name. It's not elegant, but it works.

The next two steps are taken almost entirely from John A. Bernau's paper ["Text Analysis with JSTOR Archives"](https://doi.org/10.1177%2F2378023118809264) [@Bernau2018]. I've tinkered with the scripts a little, but if you go back to the supporting documents for his paper, you can see how much I've literally copied over. 

Anyway, here's the script I ran to convert the metadata files, which are in XML format, into something readable in R. The following file is called ```extract_metadata.R``` on the GitHub page. If you're working with more journals, you have to add the extra journals into the tibble near the start. The first column should be the name you gave to the directories for the journal's data; the second should be the name you want to appear in any part of the project being read by humans.

```{r echo = T, eval = F}
# Parsing out xml files
# Based on a script by John A. Bernau 2018

# Install / load packages
require(xml2)
require(tidyverse)
require(plyr)
require(dplyr)

# Add every journal that you're using here as an extra line
journals <- tribble(
  ~code, ~fullname,
  "jphil", "Journal of Philosophy",
)

all_metadata <- tibble()

journal_count <- nrow(journals)

for (j in 1:journal_count){
  
  # Identify path to metadata folder and list files
  path1 <- paste0("data/metadata/",journals$code[j])
  files <- list.files(path1)
  
  # Initialize empty set
  final_data <- NULL
  
  # Using the xml2 package: for each file, extract metadata and append row to final_data
  for (x in files){
    path <- read_xml(paste0(path1, "/", x))
    
    # File name - without .xml to make it easier for lookup purposes
    document <- str_remove(str_remove(x, ".xml"),"journal-article-")
    
    # Article type
    type <- xml_find_all(path, "/article/@article-type") %>% 
      xml_text()
    
    # Title
    title <- xml_find_all(path, xpath = "/article/front/article-meta/title-group/article-title") %>% 
      xml_text()
    
    # Author names
    authors <- xml_find_all(path, xpath = "/article/front/article-meta/contrib-group/contrib") %>%
      xml_text()
    auth1 <- authors[1]
    auth2 <- authors[2]
    auth3 <- authors[3]
    auth4 <- authors[4]
    
    # Year
    year <- xml_find_all(path, xpath = "/article/front/article-meta/pub-date/year") %>% 
      xml_text()
    
    # Volume
    vol <- xml_find_all(path, xpath = "/article/front/article-meta/volume") %>% 
      xml_text()
    
    # Issue
    iss <- xml_find_all(path, xpath = "/article/front/article-meta/issue") %>% 
      xml_text()
    
    # First page
    fpage <- xml_find_all(path, xpath = "/article/front/article-meta/fpage") %>% 
      xml_text()
    
    # Last page
    lpage <- xml_find_all(path, xpath = "/article/front/article-meta/lpage") %>% 
      xml_text()
    # Language
    lang <-  xml_find_all(path, xpath = "/article/front/article-meta/custom-meta-group/custom-meta/meta-value") %>%  
      xml_text()
    
    # Bind all together
    article_meta <- cbind(document, type, title, 
                          auth1, auth2, auth3, auth4, year, vol, iss, fpage, lpage, lang)
    
    final_data <- rbind.fill(final_data, data.frame(article_meta, stringsAsFactors = FALSE))
    
    # Print progress 
    if (nrow(final_data) %% 250 == 0){
      print(paste0("Extracting document # ", nrow(final_data)," - ", journals$code[j]))
      print(Sys.time())
    }
  }
  
  # Shorter name
  fd <- c()
  fd <- final_data
  
  # Adjust data types
  fd$type <- as.factor(fd$type)
  fd$year <- as.numeric(fd$year)
  fd$vol <- as.numeric(fd$vol)
  fd$iss <- str_replace(fd$iss, "S", "10") # A hack for special issues
  fd$iss <- as.numeric(fd$iss)
  
  # We are going to replace S with some large number, and then undo it a few lines later
  fd$fpage <- str_replace(fd$fpage, "S", "1000") 
  fd$lpage <- str_replace(fd$lpage, "S", "1000")
  # Convert to numeric (roman numerals converted to NA by default, but the S files should be preserved)
  fd$fpage <- as.numeric(fd$fpage)
  fd$lpage <- as.numeric(fd$lpage)
  fd <- fd %>%
    mutate(
      fpage = case_when(
        fpage > 1000000 ~ fpage - 990000,
        fpage > 100000 ~ fpage - 90000,
        TRUE ~ fpage
      )
    )
  fd <- fd %>%
    mutate(
      lpage = case_when(
        lpage > 1000000 ~ lpage - 990000,
        lpage > 100000 ~ lpage - 90000,
        TRUE ~ lpage
      )
    )
  fd$fpage[fd$fpage == ""] <- NA
  fd$lpage[fd$lpage == ""] <- NA
  
  # Create length variable
  fd$length <- fd$lpage - fd$fpage + 1
  
  # Convert to tibble  
  fd <- as_tibble(fd)
  
  # Filter out things that aren't research-article, have no author
  fd <- fd %>%
    arrange(desc(-length)) %>%
    filter(type == "research-article") %>%
    filter(is.na(auth1) == FALSE)
  
  # Filter articles that we don't want  
  fd <- fd %>%
    filter(!grepl("Correction",title)) %>%
    filter(!grepl("Foreword",title)) %>%
    filter(!(title == "Descriptive Notices")) %>%
    filter(!(title == "Editorial")) %>%
    filter(!(title == "Letter to Editor")) %>%
    filter(!(title == "Letter")) %>%
    filter(!(title == "Introduction")) %>%
    filter(!grepl("Introductory Note",title)) %>%
    filter(!grepl("Foreword",title)) %>%
    filter(!grepl("Errat",title)) %>%
    filter(!grepl("Erata",title)) %>%
    filter(!grepl("Abstract of C",title)) %>%
    filter(!grepl("Abstracts of C",title)) %>%
    filter(!grepl("To the Editor",title)) %>%
    filter(!grepl("Corrigenda",title)) %>%
    filter(!grepl("Obituary",title)) %>%
    filter(!grepl("Congress",title))
  
  # Filter foreign language articles. Can't filter on lang = "eng" because some articles have blank
  fd <- fd %>%
    filter(!lang == "fre") %>%
    filter(!lang == "ger")
  
  # Convert file to character to avoid cast_dtm bug
  fd$document <- as.character(fd$document)
  
  # Add a column for journal name
  fd <- fd %>%
    mutate(journal = journals$fullname[j])
  
  # Put the metadata for this journal with metadata for other journals
  all_metadata <- rbind(fd, all_metadata) %>%
    arrange(year, fpage)
}

save(all_metadata, file  = "my_journals_metadata.RData")

# The rest of this is a bunch of tweaks to make the metadata more readable

my_articles <- all_metadata

# Get Rid of All Caps
my_articles$title <- str_to_title(my_articles$title)
my_articles$auth1 <- str_to_title(my_articles$auth1)
my_articles$auth2 <- str_to_title(my_articles$auth2)
my_articles$auth3 <- str_to_title(my_articles$auth3)

#Get rid of messy spaces in titles
my_articles$title <- str_squish(my_articles$title)
my_articles$auth1 <- str_squish(my_articles$auth1)
my_articles$auth2 <- str_squish(my_articles$auth2)
my_articles$auth3 <- str_squish(my_articles$auth3)

# Note that this sometimes leaves us with duplicated articles in my_articles
# The following is the fix duplication code
my_articles <- my_articles %>% 
  rowid_to_column("ID") %>%
  group_by(document) %>%
  top_n(1, ID) %>%
  ungroup()

# Making a list of authors; uses 'et al' for 4 or more authors
my_articles <- my_articles %>%
  mutate(authall = case_when(
    is.na(auth2) ~ auth1,
    is.na(auth3) ~ paste0(auth1," and ", auth2),
    is.na(auth4) ~ paste0(auth1,", ",auth2," and ",auth3),
    TRUE ~ paste0(auth1, " et al")
  ))

# Code for handling page numbers starting with S, and for just listing last two digits in last page when that's all that is needed
my_articles <- my_articles %>% 
  mutate(adjlpage = case_when(floor(fpage/100) == floor(lpage/100) & fpage < 10000 ~ lpage - 100*floor(lpage/100),
                              TRUE ~ lpage)) %>% 
  mutate(citation = case_when(
    journal == "Philosophy of Science" & fpage > 10000 ~ paste0(authall," (",year,") \"", title,"\" ",journal," ",vol,":S",fpage-10000,"-S",lpage-10000,"."),
    journal == "Proceedings of the Aristotelian Society" & year - vol > 1905 ~ paste0(authall," (",year,") \"", title,"\" ",journal," (Supplementary Volume) ",vol,":",fpage,"-",adjlpage,"."),
#    TRUE ~ paste0(authall," (",year,") \"", title,"\" ",journal," ",vol,":",fpage,"-",lpage,".")
    TRUE ~ paste0(authall,", ",year,", \"", toTitleCase(title),",\" _",journal,"_ ",vol,":",fpage,"–",adjlpage,".")
  )
  )

# Remove Errant Articles
# This is used to remove duplicates, articles that aren't in English but don't have a language field, etc.
# Again, this isn't very elegant, but you just have to look at the list of articles and see what shouldn't be there
errant_articles <- c(
  "10.2307_2250251",
  "10.2307_2102671",
  "10.2307_2102690",
  "10.2307_4543952",
  "10.2307_2103816",
  "10.2307_185746",
  "10.2307_3328062"
)

# Last list of things to exclude
my_articles <- my_articles %>%
  filter(!document %in% errant_articles) %>%
  filter(!lang == "spa")

save(my_articles, file="my_articles.RData")
```

The main thing I added to this was the ugly code for handling articles with **S** in their page number. This doesn't matter for the _Journal of Philosophy_ in the 1970s. But two other journals have page numbers that look like S17, S145, etc. Treating these as numbers was a bit of a challenge, and the ugly code above is an attempt to handle it. As you can see, I've written distinct lines in for the two journals that I was looking at that did this; if you look at more journals you'll have to be careful with this.

The other thing I did is right near the end, which is the **mutate** command that introduces the citation field. That's a really helpful way of referring to articles in a familiar, human, and readable way. If you prefer a different citation format, that's the line you want to adjust.

We now have a tibble, called **my_articles** that has the metadata for all the articles. It's somewhat helpful in its own right; I use the large one I generated from all twelve journals for looking up citations. But we also need the words. For this I use another script that I built off one from Bernau's paper.

This is called ```extract_words.R``` on the GitHub page. And again, if you want to use more journals, you'll have to extend that tibble at the start.

```{r echo = T, eval = F}
# Read ngrams
# Based on script by John A. Bernau 2018
require(tidyverse)
require(quanteda)

# Journal List
journals <- tribble(
  ~code, ~fullname,
  "jphil", "Journal of Philosophy",
)

jlist <- journals$code

# Initialise huge tibble
huge_tibble <- tibble(filename = character(), word = character(), wordcount = numeric())

for (journal in jlist){
  # Set up files paths
  path <- paste0("data/ngram/",journal)
  n_files <- list.files(path)
  
  # Connecting Words to Filter out
  source("short_words.R")
  
  big_tibble <- tibble(filename = character(), word = character(), wordcount = numeric())
  
  for (i in seq_along(n_files)){
    # Remove junk to get codename
    codename <- str_remove(str_remove(n_files[i], "-ngram1.txt"),"journal-article-")
    
    # Get metadata for it
    meta <- my_articles %>% filter(document == codename)
    
    # If it is in article list, extract text
    if(nrow(meta) > 0){
        small_tibble <- read.table(paste0(path, "/", n_files[i]))
        small_tibble <- small_tibble %>%
          dplyr::rename(word = V1, wordcount = V2) %>%
          add_column(document = codename, .before=1) %>%
          mutate(digit = str_detect(word, "[:digit:]"),
                 len = str_length(word)) %>% 
          filter(digit == F & len > 2) %>% 
          filter(!(word %in% short_words)) %>% 
          select(-digit, -len)
        big_tibble <- rbind(big_tibble, small_tibble)
    }
    if (i %% 250 == 0){
      print(paste0("Extracting document # ", journal, " - ", i))
      print(Sys.time())
    }
  }
  huge_tibble <- rbind(huge_tibble, big_tibble)
}

# Adjust data types
my_wordlist <- as_tibble(huge_tibble)
my_wordlist$document <- as.character(my_wordlist$document)
my_wordlist$word <- as.character(my_wordlist$word)

save(my_wordlist, file  = "my_wordlist.RData")
```

Now with a tibble of all the articles, and another with all the words in each article, it's time to go to work. The next file is called ```create_lda.R``` on the GitHub page, and if you're doing a big project, it could take some time to run. This particular script takes less than a minute to run on my computer. But the equivalent step in the main project took over eight hours on a pretty powerful laptop.

```{r echo = T, eval = F}
require(tidytext)
require(topicmodels)
require(tidyverse)

# This is redundant if you've just run the other scripts, but here for resilience
load("my_wordlist.RData")
load("my_articles.RData")

source("short_words.R")

# Filter out short words and words appearing 1-3 times
in_use_word_list <- my_wordlist %>%
  filter(wordcount > 3) %>%
  filter(!word %in% short_words) %>%
  filter(document %in% my_articles$document)

# Create a Document Term Matrix 
my_dtm <- cast_dtm(in_use_word_list, document, word, wordcount)

# Build the lda
# k is the number of topics
# seed is to allow replication; vary this to see how different model runs behave
# Note that this can get slow - the real one I run takes 8 hours, though if you're following this script, it should take seconds
my_lda <- LDA(my_dtm, k = 10, control = list(seed = 22031848, verbose = 1))

# The start on analysis - extract topic probabilities
my_gamma <- tidy(my_lda, matrix = "gamma")

# Now extract probability of each word in each topic
my_beta <- tidy(my_lda, matrix = "beta")
```

The big step is the one that calls the LDA command. That builds the topic model. From here, your job is to just do analysis. But just to demonstrate what this finds, here is a quick look at what we found. 

```{r analyse-dummy-lda, cache=TRUE}
load("my_lda.RData")
my_gamma <- tidy(my_lda, matrix = "gamma")

topic_three <- my_gamma %>%
  filter(topic == 3) %>%
  arrange(-gamma) %>%
  slice(1:10) %>%
  inner_join(articles, by = "document") %>%
  select(citation)

kable(topic_three,
      col.names = "Article",
      caption = "Top articles in topic 3 in example LDA.")
```

These are the ten articles that our little example LDA gives the highest probability to being in topic 3. What's topic 3? I guess ethics, from the look of those articles. We could check this by seeing which words have the highest probability of turning up in the topic.

```{r analyse-dummy-lda-words, cache=TRUE}
load("my_lda.RData")
my_beta <- tidy(my_lda, matrix = "beta")

topic_three_words <- my_beta %>%
  filter(topic == 3) %>%
  arrange(-beta) %>%
  slice(1:10) %>%
  select(term)

kable(topic_three_words,
      col.names = "Word",
      caption = "Topic 3 words.") %>%
   kable_styling(full_width = F)
```

And that seems to back up my initial hunch that this is about ethics, or at least about morality. I'm going to stop the illustration here, because to go any further would mean doing serious analysis on a model that probably doesn't deserve serious attention. But hopefully I've said enough here that anyone who wants to can get started on their own analysis.

## Volumes {-}

The portentous "Volume 1" in the subtitle is because I have a number of ideas for how to think about the history of philosophy journals. My first project along these lines involved looking at citation patterns, and a version of that focusing on early twenty-first century journals should be volume 2. When I started working on this what I really wanted to understand was the revolution in philosophy (at least as it appeared in journals) between 1968 an 1975, and maybe there will be a volume 3 if I figure out something to say about that period. But volume 2 is barely started, and volume 3 is for now vaporware. The subtitle is to leave options open, not to announce further work.

The current version of this book has been compiled on:

```{r include-date}
Sys.Date()
```

With the following configuration:

<details>
    <summary>Show configuration</summary>
```{r session-info}
xfun::session_info()
```
</details>

```{r articles-with-word, cache=TRUE}
articles_with_word <- function(x){
t <- all_journals_tibble %>%
  filter(word == x) %>%
  inner_join(relabeled_articles, by = "document") %>%
  arrange(-wordcount) %>%
  top_n(10, wordcount) %>%
  inner_join(the_categories, by = "topic") %>%
  select(mcitation, subject, wordcount)
kable(t, 
      col.names = c("Article", "Subject", "Word Count"), 
      caption = paste0("Articles in which the word \'",x,"\' appears most often.")
)
}
```

```{r empty-block-end-of-index}
# I love placeholders
```

<!--chapter:end:index.Rmd-->

# Methodology {#methodology-chapter}

The point of this chapter is to explain the choices I made in building the model that the book is based around. But to understand the choices that I made, it helps to know a little bit about what a Latent Dirilecht Algorithm (LDA) does.

The inputs to the model are some texts and a number. The model doesn't care about the ordering of words in the texts, so really the input isn't texts but a list of lists of ordered pairs. Each ordered pair is a word and a number. In the version I'm using, the outer list is a list of philosophy articles. And each element of that list is a list of words in that article, along with the number of times the word appears.

Along with that, you give the model a number. This is the number of _topics_ that you want the model to divide the texts into. I'll call this number $t$ in this introduction. And intuitively there is a function $T$ that maps articles into the $t$ topics. 

What the model outputs is, for our purposes, a pair of probability functions: one for articles and one for words.

The probability function for articles gives, for each article $a$ and topic number $n \in \{1, \dots, t\}$, a probability for $T(a) = n$; that is, it gives a probability that the article is in that topic. Notably, it doesn't identify the topics with any more than numbers. I'm going to give names to the topics—this one is [Kant](#topic32); this one is [composition and constitution](#topic89), etc.—but the model doesn't do that. For it, the topics really are just integers between 1 and $t$.

The probability function for words gives, for each word $w$ from any of the articles, and topic number $n \in \{1, \dots, t\}$, the probability that a randomly chosen word from the articles in that topic is $w$. So in the Kant topic, the probability that a randomly chosen word is _Kant_ is about 0.14. 

That number feels absurdly high, but it makes sense for a couple of reasons. One is that to make the models compile in even semireasonable time, I filtered out a lot of words. What's it's really saying is that the word _Kant_ produces about 1/7 of the tokens that remain. The other is that what it's really giving you here is the probability that a random word in an article is _Kant_ conditional on the probability of that article being in the [Kant](#topic32) is 1. And in fact the model is never that confident. Even for articles that might be considered to be clearly articles about Kant, the model is rarely more than 40 percent confident that that's what they are about. And this is for a good reason. Most articles about Kant in philosophy journals are, naturally enough, about Kantian philosophy. And any part of Kantian philosophy is, well, philosophy. So the model has a topic on [beauty](#topic08), and when it sees an article on Kantian aesthetics, it gives some probability the correct classification of that article is in the topic on Beauty. So the word probabilities are quite abstract things—they are something like word frequencies in a certain kind of stereotyped article. What the model really wants to do is find $t$ stereotypes such that each real article is a linear mixture of the stereotypes. 

The way the model approaches this goal is by building two probability functions, checking how well they cohere, and recursively refining them in places that they don't cohere. One probability function is the probability, for each article, that it is in one or other of the ninety topics. So it might say this article is 0.4 likely to be in topic 32, 0.3 likely to be in topic 68, and so on down to some vanishing probability that it is in topic 89. The other probability function is the probability, for each topic, of a given word appearing. So it might say that given the article is in topic 32, there is a 0.15 likelihood that a randomly selected word in the article is _Kant_, an 0.05 likelihood that a randomly selected wordis _ideal_, and so on, down to a vanishingly small likelihood that the word is, say, _Weatherson_. Combining those functions, we get the probability, for each actual article, that a randomly selected word in it is _Kant_ or _ideal_ or _Weatherson_ or any other word. And we can check that calculated probability against the actual frequency of each word in the article. I'll call the calculated probabilities the _modeled frequencies_, and say that the goal of the model is to have the modeled frequencies of words in articles match the actual frequencies of the words in the articles. A perfect match here is impossible to achieve; there aren't enough degrees of freedom. But the model can minimize the error, and it does so recursively.

The process involved is slow. I was able to build all the models I'll discuss on personal computers, but it takes some processing time. The particular model I'm primarily using took about twenty hours to build, but I ran through many more hours than that building other models to compare it to.

And the process is very path dependent. The algorithm, like many algorithms, has the basic structure of pick a somewhat random starting point, then look for a local equilibrium. That's incredibly dependent on how you start and somewhat dependent on how you travel.

The point of this chapter is to describe how I chose the inputs to the model I ended up using, and then how I set various parameters within the model. The parameters are primarily, in terms of the metaphor of the previous paragraph, the starting point of the search, and how long the search should go before we decide one is something close enough to an equilibrium.

The inputs are more complex. Very roughly, the inputs I used are the frequently occurring substantive words from research articles in twelve important philosophy journals. I'll start by talking about how and why I selected the particular twelve journals that I did.

## Selecting the Twelve Journals

This is a study about the trajectory of topics across leading philosophy journals. But presumably most people reading this aren't interested in philosophy journals as such; they are interested in the trajectory of philosophy. So it is important to select, as far as possible, journals that accurately reflect what's going on in philosophy.

An obvious idea would be to just use generalist journals, because they will reflect what's generally happening in philosophy. But this turns out to be a bad idea, since there really aren't any generalist journals in philosophy. Perhaps that's because the journals in moral and political philosophy, and in philosophy of science, are so good, that so-called generalist journals tend to under-represent work in those fields. Or, perhaps more precisely, they don't always reflect the cutting-edge work in those fields.

In [previous work](http://tar.weatherson.org/2017/04/26/citation-patterns-across-journals/), I noted how little attention the leading generalist journals had paid to two of the most important late twentieth-century articles, Elizabeth Anderson's ["What is the Point of Equality?"](https://philpapers.org/rec/ANDWIT), and Peter Machamer, Lindley Darden and Carl Craver's ["Thinking about Mechanisms"](https://philpapers.org/rec/MACTAM). These papers each have over 2500 Google Scholar citations, but they have barely been mentioned in the leading generalist journals. An accurate picture of recent philosophy has to include the literatures these papers spawned, and those literatures on the whole aren't found in generalist journals. So to get an accurate picture of philosophy, you need to include at least some specialist journals.

As a reminder, here are the journals that I've included:

```{r journaltablereprise, echo=FALSE, cache=TRUE}
  kable(journals_summary, 
        col.names = c("Journal", "First Year", "Number of Articles"), 
        align=c("l", "c", "c")
  )
```

As you can see, there are two moral and political journals, _Ethics_ and _Philosophy and Public Affairs_ (_P&PA_), and two philosophy of science journals, _Philosophy of Science_ and the _British Journal for the Philosophy of Science_. I could possibly have gotten by with just one of each. But I thought _Ethics_ and _P&PA_ brought in different fields of philosophy, and so they were both worth including. That meant it would be good to balance them with two philosophy of science journals. This had the side benefit of my not having to decide which of those two philosophy of science journals was more representative.

But if I had those four "specialist" journals, I needed enough "generalist" journals that what I had felt representative of philosophy as a whole. Partially to get the balance right and partially to make the graphs look nice, it felt like I needed eight more journals. I started with the current "big four" journals.

- _Mind_
- _The Philosophical Review_
- _Journal of Philosophy_
- _Noûs_

I added _Analysis_ because I wanted to be very sensitive to trends, and _Analysis_ is often ahead of the trends in the field. That leaves three more spots. Here were the criteria I used to fill those:

- The data for the journal had to be available through JSTOR's Data for Researchers. This was not negotiable since that was my data source. But it was unfortunate, since it ruled out the _Australasian Journal of Philosophy_, which would otherwise have been perfect.
- The journal had to be active for a long time. It wouldn't help balance much to add a journal that didn't exist during the timeframe I'm looking at. This ruled out _Philosophical Studies_. That's a bit of a shame since the story of twenty-first century philosophy can't be told without _Philosophical Studies_. But it just doesn't have enough history for the purposes of this study.
- The journal could not be too idiosyncratic. I wanted it to tell something about the field, not just about those journals. This ruled out _The Monist_, which was very idiosyncratic in its early years. In recent years, it is idiosyncratic in a good way; highlighting work the others sometimes overlook. But before World War II it is barely a philosophy journal in any recognizable sense.
- The journal could not be a philosophy of science journal, since the aim is to balance the two philosophy of science journals I have.
- The journal had to primarily publish in English, since the analysis tools I'm using simply don't work for cross-linguistic data sets. The last two criteria ruled out _Synthese_ and _Erkenntnis_.

After all that, I was left with:

- _Philosophy and Phenomenological Research_: This has slightly more non-English work than is ideal for current purposes, but I thought adding a little continental philosophy from its early years was worthwhile. And it became such an important journal that it felt wrong to leave it out.
- _Proceedings of the Aristotelian Society_: Note that I'm including the supplementary volumes here.^[Including them also causes a few headaches. The articles in the supplementary volumes often have respondents. When they do, the metadata often lists both the original article and the reply article as coauthored. This is bizarre, though even more bizarre is that often the running head on the print article does the same thing. This can totally mess up the calculating of philosophical Erdös numbers which one would probably know if they have calculated them). This study isn’t tracking authors, so it isn’t too painful. But I am using auto-generated citations as a way of picking out articles, and they will sometimes look coauthored when they are really not. I’m not going to try fixing this; it’s just a weirdness that I’ll live with.] This is idiosyncratic in its early years; some secretaries of the Aristotelian Society have a bigger impact on this study than they do on the field. But without it, so much of British philosophy is missed, including some themes in contemporary philosophy that aren’t always covered in the other major journals.
- _Philosophical Quarterly_. For much of the twentieth century, this is much less prestigious than the other eleven journals I'm looking at, and this will become relevant when I look at the citation data. But it fits the other criteria very well. It adds a Scottish journal to the English and US journals I am otherwise looking at. It has slightly better history coverage than the other journals, and since I worked at St Andrews for so many years, I'm personally fond of it.

One might wonder why I didn't add any other specialist journals, along with journals in ethics and in philosophy of science. The reasons were a bit varied.

I think there is a better sense of midcentury philosophy if one logic journal is added. But text mining can't be done on symbols. And in more recent years, the sense in which the logic journals are primarily philosophy journals as opposed to mathematics journals has gotten weaker. So, I left them out.

The twelve journals I have don;t include as much history of philosophy as there is in the profession. But that's simply unavoidable if doing a study based on journals. History of philosophy is primarily a book discipline rather than a journal discipline. This can be seen in the citation data. Pick almost any prominent figure in history of philosophy and odds are that I'll have several journal articles with more citations than their most cited article. The prominent figure picked will almost surely have several books that are more widely cited than any of my articles. The point isn't that historians of philosophy are never cited but that they rarely have highly cited articles. Just as importantly, when a history article is widely cited, it usually appears in one of the twelve journals I've already included. For this reason, the model that I end up working with does have a lot of history categories. Just remember that the absolute numbers of articles in each of these categories is not representative of how important the categories are in philosophy.

And the other specialist journals are either too new (e.g., _Mind and Language_, or _Linguistics and Philosophy_) or representative of too small a section of contemporary philosophy to be worth including. Aesthetics, for example, is an important philosophical field. (And it shows up in the model in an interesting way.) But including the _Journal of Aesthetics and Art Criticism_ in the study would have made it look like aesthetics was 1/13 of the field, and that's misleading. So I stuck with these twelve.

## Selecting the Articles

Journals publish a lot, and I had to decide what to include and what to leave out. The aim was to include all and only research articles, but this was harder than it looks.

The metadata that JSTOR provides includes a tag for article kind. I only included articles with the tag "research-article", which does a reasonable job of getting rid of book reviews. But it turns out that it includes a lot of things that are not really research articles. It functions in the JSTOR metadata as something of a generic article kind, one that applies if nothing else seems right. So we have to manually edit out a bunch of articles.

I deleted all articles without a listed author. These were often editorials, corrections and the like.

After that, I started working through various words in titles that indicated something was not actually a research article. So I deleted all articles with these titles:

- Descriptive Notices
- Editorial
- Letter to Editor
- Letter
- Introduction

The first four are clear enough. The last was mostly a problem for special issues, but there were enough special issues of one kind or another to make it worthwhile. Then I deleted any articles that had the following phrases anywhere in the title:

- Correction
- Foreword
- Introductory Note
- Errat
- Erata
- Abstract of C
- Abstracts of C
- To the Editor
- Corrigenda
- Obituary
- Congress

The last is the only one that really needs comment. All the articles I found with this in the title were reports on one or another philosophy congress, not genuine research articles. Maybe there was a political philosophy article that referenced the United States Congress in its title and should not have been excluded but I didn't see it.

Since text mining only works within a single language, I excluded all the articles whose listed language in the metadata was anything other than English. And I manually excluded, when I saw them, articles whose title was not in English and which seemed like non-English articles.

That left me with `r nrow(articles)` articles to work with.

## Selecting the Words {#stop-words}

The JSTOR data excludes a few stop words (like _the_ and _and_), and words with one or two characters. On the other hand, it takes nonletters to be word breaks. So _doesn't_ would be split into _doesn_ and _t_ and the second rejected as too short. And hyphenated words are split as well. It turned out that this made _est_ into a reasonably common word. But I didn't want to include all the words for various reasons.

It seems common in text mining to exclude a more expansive list of stop words than JSTOR leaves out. I was playing around with making my own list of stop words, but I decided it would be more objective to use the commonly used list from the **tm** package. They use the following list of stop words:

```{r stop_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- common_words[1]
for (i in 2:length(common_words)){
  sw <- paste0(sw, ", ", common_words[i])
}
cat("-", sw)
```

I excluded all of these words from the analysis. The intuition here is that including them would mean that the analysis is more sensitive to stylistic ticks than to content, and in practice that seemed to be right. The models did look more reflective of substance than style with the stop words excluded. In principle I'm not sure it was right to exclude all those quantifiers from the end of the list, but it doesn't seem to have hurt the analysis. I'll come back to this point at the end of the chapter, but it is possible I should have been more aggressive in filtering out stop words.

The stop words list from **tm** includes a lot of contractions. I wrote a small script to extract the parts of those contractions before the apostraphe, and excluded them too. The parts after then apostrophe were always one or two letters, so they were already excluded.

I've also looked through the list of the five thousand most common words in the data set to see what shouldn't be there, and the rest of this section comes from what was cut on the basis of that.

In some cases, JSTOR's source for the text was from the LaTeX code for the article, so there was a lot of LaTeX junk in the text file. I'm sure I didn't clean out all of this, but to clean out a lot of it, I deleted the following words.

```{r latex_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- latex_words[1]
for (i in 2:length(latex_words)){
  sw <- paste0(sw, ", ", latex_words[i])
}
cat("-", sw)
```

I'm a bit worried that excluding _document_ meant I lost some signal about historical articles in the LaTeX noise. But this was unavoidable. 

Also note that _anid_ is not a LaTeX term, but it was worthwhile to exclude it here. Something about how the text recognition software JSTOR uses interacted with nineteenth- and early twentieth-century articles meant that several words, especially 'and', got coded as 'anid'. But this was the OCR verison of a typo, and best deleted. (There were a few more of these that were not in the five thousand most common words that on reflection I wish I'd cut too. But I don't think they make a huge difference to the analysis given how rare they are.)

Somewhat reluctantly, I deleted a bunch of spellings out of Greek letters for the same reason; they were mostly from LaTeX code. This meant deleting the following words:

```{r greek_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- greek_words[1]
for (i in 2:length(greek_words)){
  sw <- paste0(sw, ", ", greek_words[i])
}
cat("-", sw)
```

I'm sure this lost some signal. But there was so much LaTeX noise that it was unavoidable.

Next I deleted a few honorifics; in particular:

```{r honorifics, echo=FALSE, cache=TRUE, results='asis'}
sw <- gendered_words[5]
for (i in 6:length(gendered_words)){
  sw <- paste0(sw, ", ", gendered_words[i])
}
cat("-", sw)
```

These just seemed to mark the article as being old, not anything about the content of the article. I didn't need to exclude _mr_ or _dr_ since they were already excluded as too short.

Although I was trying to exclude foreign-language articles, I also excluded a bunch of foreign words. One reason was that it was a check on whether I missed any foreign-language articles. Another was that if I didn't do this, then articles that had extensive quotation from foreign languages would be seen by the model as being in their own distinctive topic merely in virtue of having non-English quotations. And that seemed wrong. So to fix it, I excluded these words:

```{r foreign_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- foreign_words[1]
for (i in 2:length(foreign_words)){
  sw <- paste0(sw, ", ", foreign_words[i])
}
cat("-", sw)
```

Finally, I excluded a bunch of words that seemed to turn up primarily in bibliographies or in text citations. Including them seemed to just make the model be more sensitive to the referencing style of the journal rather than the content. But here the deletions really did cost some content, because some of the words were philosophically relevant. But I deleted them because they seemed to be turning up more often in bibliographies than in text:

```{r ref_words, echo=FALSE, cache=TRUE, results='asis'}
sw <- ref_words[1]
for (i in 2:length(ref_words)){
  sw <- paste0(sw, ", ", ref_words[i])
}
cat("-", sw)
```

The surprising one there is _compilation_. But it most often appears because some journals have a footer saying "Journal compilation ©".

Then to speed up processing, I deleted any word that appeared in any article three times or less This lost some content, but it sped up the processing a lot. Some of the steps I'll describe below took several days computing time. Without this restriction they would have taken several weeks. And I thought words that appear one to three times in an article shouldn't be that significant for determining its content. Though as I'll note below, this might have been too aggressive in retrospect.

## Building a Model

So at this stage we have a list of `r nrow(articles)` articles to include, and a list of several hundred words to exclude. JSTOR provides text files for each article that can easily be converted into a two-column spreadsheet. The first column is a word; the second column is the number of times the word appears. I added a third column for the code number of the article and then merged all the spreadsheets for each article into one giant spreadsheet. (Not for the last time, I used code that was very closely based on code that [John Bernau](https://www.johnabernau.com/about/) built for a similar purpose [@Bernau2018].) Now I had a file that was 137MB large, and had the word counts of all the words in all the articles.

I filtered out the words in all the lists above, and all the words that appeared in an article one to three times. And I filtered out all the articles that weren't on the list of `r nrow(articles)` research articles. This was the master word list I'd work with.

I turned that word list, which at this stage looked like a regular spreadsheet, into something called a document-term-matrix using the ```cast_dtm``` command from Julia Slige and David Robinson's package [tidytext](https://www.rdocumentation.org/packages/tidytext/versions/0.1.3). The DTM format is important only because that's what the [**topicmodels**](https://cran.r-project.org/web/packages/topicmodels/index.html) package (written by Bettina Grün and Kurt Hornik) takes as input before producing an LDA model as output.

I'm not going to go over the full details of how a Latent Dirichlet Allocation (LDA) model is built, because the description that [Grün and Hornik provide](https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf) is better than what I could do. I'll just note that I'm using the default VEM algorithm.

The basic idea is to use word frequency to estimate which words go in which topics. This makes some amount of sense. Every time the word _Rawls_ appears in an article, that increases the probability that the article is about political philosophy. And every time the word _Bayesian_ appears, that increases the probability that the article is about formal epistemology. These aren't surefire signs, but they are probabilistic signs, and by adding up all these signsthe probability that the article is in one topic rather than another can be worked out.

But what's striking about the LDA method is that the topics are not specified in advance. The model is not told, "Hey, there's this thing called political philosophy, and here are some keywords for it." Rather, the algorithm itself comes up with the topics. This works a little bit by trial and error. The model starts off guessing at a distribution of articles into topics, then works out what words would be keywords for each of those topics, then sees if, given those keywords, it agrees with its own (probabilistic) assignment of articles into topics. It almost certainly doesn't, since the assignment was random, so it reassigns the articles and repeats the process. And this process repeats until it is reasonably satisfied with the (probabilistic) sorting. At that point, it tells us the assignment of articles, and keywords, to topics. (Really though, go see the link above for more details if you want to understand the math.)

The output provides topics, and keywords, but not any further description of the topics. They are just numbered. It might be that topic 52 has a bunch of articles about liberalism and democracy, broadly construed, and has words like _Rawls_, _liberal_, _democracy_, and _democratic_ as keywords, and then we can recognize it as political philosophy. But to the model it's just topic 52.

At this stage there are three big choices the modeler has:

1. How many topics should the articles be divided into?
2. How satisfied should the model be with itself before it reports the data?
3. What random assignment should be used to initialize the algorithm?

Although the algorithm can sort the articles into any number of topics one asks it to, it cannot say what makes for a natural number of topics to use. (There is a caveat to this that I'll get to.) That has to be coded by hand into the request for a model. And it's really the biggest decision to make. The next section discusses how I eventually made it.

## Choosing the Number of Topics {#choose-topic-number}

The model-building algorithm automates most of the work; it even chooses what the topics are. But the one thing it doesn't do is choose how many topics there are. That has to be specified in advance. And it's a big choice.

In principle, it can be given as few as two topics to work with. If the model is asked to divide all the articles into two groups, it will usually divide them into something like ethics articles and something like metaphysics and epistemology articles. I say "usually" because it's a fairly random process. And about a quarter of the time, it will find some other way of dividing the articles in two, such as earlier or later, or perhaps things that look maximally like philosophy of science and maximally unlike philosophy of science. But none of these are helpful models; they say more about the nature of the modeling function than they say about the history of philosophy.

The topicmodels package itself comes with a measure that's intended to be used for this purpose. The "perplexity"'" function asks the model, in effect, how confused it is by the data once it has built the model.^[One can ask the model how confused it is about the data that was used to build the model or hold back some of the data from the model-building stage and use it on the held-back data. The second probably makes more sense theoretically, but it didn’t make a huge difference here.] The thought is that once there are too many topics, the perplexity score won’t change as more topics are added. That’s a sign that a natural limit has been reached. But it didn’t help here. As far as I could tell, I could have had something like four hundred topics and the perplexity score would still have fallen every time I added more topics. Philosophers are just too idiosyncratic, and topics need to be fine-grained before the computer is comfortable thinking it has the classifications of articles into topics right.

But a model with four hundred topics wouldn’t help anyone. I did build one such model, and the rest of this paragraph is about why I’m not using it.) On its own, it’s too fine-grained to be useful. I don’t think anyone would actually read it closely. To make the model human readable, I’d have to bundle the four hundred topics into familiar categories (e.g., ethics, metaphysics, philosophy of science, etc.). But when I tried to do that, I found just as many edge cases as clear cases. The only data that would come out of this approach that would be legible to humans would be a product of my choices—not the underlying model. And the aim was to get my prejudices out of the system as much as possible.

I needed something more coarse-grained than the model with lowest perplexity but obviously more fine-grained than simply two topics. I ended up doing a lot of trial and error and looking at how the models came up with different numbers of topics. (This feels
like the thing that most people using topic-modeling tools end up doing.)

When I looked at the models that were produced with different numbers of topics, I was generally looking at four factors, which I will describe in detail. The first two factors push toward more and more topics. The next two were designed to put downward pressure on
the number of topics.

First, how often did the model come up with topics that simply looked disjunctive? The point of the model is to group the articles into _n_ topics, and hopefully each of these topics has a sensible theme. But sometimes the theme is a disjunction (i.e., the topic consists of papers from philosophical debate X and papers from mostly unrelated debate Y). There are always some of these. Some debates are distinctive enough that the papers within that topic always cluster together—the model can tell that it shouldn’t be separating them—but small enough in these twelve journals) that the model doesn’t want to use up a valuable topic on just that debate. There were three of these that almost always came up: feminism, Freud, and vagueness. If a model is built out of these journals with, say, forty topics, then it is almost certain that three of the topics are simply disjunctive, with one of the disjuncts being one of these three topics. My favorite was an otherwise sensible model that decided one of the topics in philosophy consisted of papers on material constitution and papers on feminist philosophy. Now there are links there—some important feminist theories carefully distinguish causation from constitution—but it’s really a disjunctive topic. And the fewer topics there are, the more disjunctive topics you get. It’s good to get rid of disjunctions, and that’s a reason to increase the number of topics.

Second, how often did the model make divisions that cross-cut familiar disciplinary boundaries? Some such divisions are unavoidable, and the model I use ends up with a lot of them. But in the first instance I’d prefer, for example, a model that separates papers on the metaphysics of causation from papers on the semantics of counterfactuals to a model that puts them together. The debates are obviously closely related, but there was a big advantage to me if they were separated. If they were, then measuring how prominent metaphysics is in the journals becomes one step easier, as is measuring how prominent philosophy of language is. So I’d rather models that split them up.

Third, how often did the model divide up debates, and not in terms of what question they were asking but in terms of what answers they were giving (or at least taking seriously)? For instance, sometimes the model would decide to split up work on causation into, roughly, those papers that did and those that did not take counterfactuals as central to understanding causation. This tracked pretty closely (but not perfectly) the division into papers before and after David Lewis's paper ["Causation""](https://philpapers.org/rec/LEWC) [@Lewis1973b]. (Though, amusingly, models that made this division usually put Lewis's own paper into the pre-Lewisian category; which makes sense since most of that paper is about theories of causation that had come before.) This seemed bad—division should be into _topics_, and different answers to the same question shouldn't count.

Fourth, how often did the model make divisions that only specialists would understand? A bunch of models I looked at divided up, for instance, the philosophy of biology articles along dimensions that I, a non-pecialist, couldn't see reason behind. The point of this is not that there are no real divisions there, or that the model was in any sense wrong. It's rather that I want the model to be useful to people across philosophy, and if nonexperts can't see what the difference is between two topics just by looking at the headline data about the topic, then it isn't serving its function.

Still, after a lot of trial and error, it seemed like the best balance between these four criteria was hit at around sixty topics. This isn't to say it was perfect. For one thing, even with a fixed number of topics, different model runs produce very different models, and as I'll discuss in [the next section](#model-seed-choice), I have to choose between them. For another, the optimal balance between these criteria would come at different points in different fields. So perhaps at forty-eight topics a pretty good balance between these criteria within ethics (broadly construed) would be seen, but it might be double that before seeing the right balance in philosophy of mind. There are a lot of trade-offs, as might be expected given that I'm trying to detect trends in the absence of anything like clear boundary lines.

But something odd might be noticed at this stage. I said that I got the best balance at around sixty topics. Yet the model I've based the book on has ninety topics. How I got to that model involves yet more choices. I think each of the choices I made was defensible, but the reason this chapter is so long is that there really were quite a lot of choices, and I think it's worthwhile to lay them all out.

## Choosing between whe Models {#model-seed-choice}

Despite the number of topics being set, there are still a lot of ways that the model can change. Building a model starts with a somewhat random assignment of words and articles to topics, followed by a series of steps (themselves each involving a degree of randomization) toward a local equilibrium. But there is a lot of path dependency in this process, as there always is in finding a local equilibrium.

Rather than walk through the mathematics of why this is so, I find it more helpful to think about what the model is trying to achieve and why it is such a hard thing to achieve. Let’s just focus on one subject matter in philosophy, friendship, and think about how it could be classified it if were trying to divide all of philosophy up into sixty to ninety topics.

It’s too small a subject to be its own topic. It’s best if the topics are roughly equal size, and discussions that are primarily about friendship are, I’d guess, about 0.1 to 0.2 percent of the articles in these twelve journals. It’s an order of magnitude short of being its own topic. It has to be grouped in with neighboring subjects. But which ones? For some subjects, the problem is that there aren’t enough natural neighbors. This is why the models never quite know what to do with vagueness, or feminism, or Freud. But here the problem is that there are too many.

One natural thing to do is to group papers on friendship with papers on love and both of them with papers on other emotions or perhaps with papers on other reactive attitudes. That groups a nice set of papers about aspects of the mental lives of humans that are central to actually being human but not obviously well captured by simple belief-desire models.

Another natural thing to do is to group papers on friendship with papers on families, and perhaps include both of them in broader discussions of ways in which special connections to particular others should be accounted for in a good ethical theory. Again, this produces a reasonably nice set of papers here, with the general theme of special connections to others.

Or yet another natural thing to do is to group papers on friendship with papers on cooperation. And while thinking about cooperation, the natural paper to center the topic around is Michael Bratman’s very highly cited paper ["Shared Cooperative Activity""](https://philpapers.org/rec/BRASCA). From there, there are a few different ways one could go. Expanding the topic to Bratman’s work on intention more broadly and the literature it has spawned could be done. Or one could expand it to include other work on group action, and even perhaps on group agency. (I teach that Bratman paper in a course on groups and choices, which is centered around game theory. Though I think getting from friendship to game theory in a single one of our sixty to ninety topics would be a step too far.)

Which of these is right? Well, I saw all of them when I ran the algorithm enough times. And they all seem like sensible choices to me. How should I choose which model to use when different models draw such different boundaries within the space of articles? A tempting thought is to see which one looks most like what one thinks philosophy really looks like and choose it. But now prejudices are being imposed on the model rather than letting the model teach something about the discipline.

A better thing to do is to run the algorithm a bunch of times and find the output that most commonly appears. Intuitively, we’re looking for an equilibrium, and there’s something to be said for picking the equilibrium with the largest basin of attraction. This is more or less what I did, though there are two problems.

The first problem is that running the algorithm a bunch of times is easier said than done. On the computers I was using pretty good personal computers), it took about eight hours to come up with a model with sixty topics. Running a bunch of them to find an average was a
bit of work. The University of Michigan has a good unit for doing intensive computing jobs like this, but I kept feeling as though I was close enough to being done that running things on my own devices was less work than setting up an account there. (This ended up being a bad mistake.) But I could just leave them run overnight every night for a couple of weeks, and eventually I had sixteen sixty-topic models to average out.

The models are distinguished by their **seed**. This is a number that can be specified to seed the random-number generator. The intended use of it is to make it possible to replicate work like this that relies on randomization. But it also means that a bunch of models can be run, then slight changes can be made to the one that seems most representative. And that’s what I ended up doing. The seeds I used at this stage were famous dates from the revolutions of 1848. And to get ahead of the story, the model the book is based around has seed value 22031848, the date of both the end of the Five Days of Milan and of the start of the Venetian Revolution.^[Why 1848 and not some other historical event? Well, I had originally been using dates from the French Revolution. But I made so many mistakes that I had to start again. In particular, I didn’t learn how many words I needed to filter out, and how many articles I needed to filter out, until I saw how much they were distorting the models. And by that stage I had so many files with names starting with 14071789 and the like that I needed a clean break. So 1848, with all its wins and all its losses, it was.] 

The second is that it isn’t obvious how to average the models. At one level, what the model produces is a giant probability function. And there is a lot of literature on how to merge probability functions into a single function or more or less equivalently) how to find the most representative of a set of probability functions. But this literature assumes that the probability functions are defined over more or less) the same possibility spaces. And that’s precisely what isn’t true here. When building one of these models, what is left is a giant probability function all right. But no two model runs give a function over the same space. Indeed, the most interesting thing about any model is what space it decides is most relevant. So the standard tools for merging probability functions don’t apply.

What I did instead was look for two things. 

First, the model doesn’t just say, “This article goes in this topic.” It says that this article goes in this topic with probability p. Indeed, it gives nonzero probabilities to each article being in each topic. So the thing to look for in a model is what articles does it think have the highest probability of being in any given topic? That is, roughly speaking, Which articles does it think are the paradigms of the different topics it discovers? Then ask, across a range of models, How much does this model agree with the other models about which are the paradigm articles? So, for instance, find the ten articles with the highest probability of being in each of the sixty topics. And then ask, Out of the six hundred articles that this model thinks are the clearest instance of a particular topic, how many of them are similarly in the six hundred articles that other models think are the paradigms of a particular topic? So that was one what I looked for: Which models had canonical articles that were also canonical articles in a lot of other models?

Second, the models don’t just give probabilistic judgments of an article being in a particular topic; they give probabilistic judgments of a word being in an article in that topic. So, the model might say that the probability of the word _Kant_ turning up in an article in topic 25 is 0.1, while the probability of it turning up in most other topics is more like 0.001. That tells us that topic 25 is about Kant, but it also tells us that the model thinks that Kant is a keyword for a topic. Since some words will turn up frequently in a lot of topics no matter what, focus here not just on the raw probabilities like the 0.1 above) but on the ratio between the probability of a word being in one topic and it being in others. That determines how characteristic the word is of the topic. And again this trick can be used to find the six hundred characteristic words of a particular model and ask how often those six hundred words are characteristic words of any model at all. There is a lot of overlap here—the vast majority of models have a topic where Aristotle is a characteristic word in this
sense, for example. But there are also idiosyncrasies, and the models with fewest idiosyncrasies seem like better bets for being more representative. So that was another thing I looked for: Which models had keywords that were also keywords in a lot of other models?

The problem was that these two approaches (and a couple of variations of them that I tried) didn’t really pick out a unique model. It told me that three of them were better than the others but not really which of those three was best. I chose one in particular. Partially this was because I could convince myself it was a bit better on the two representativeness tests from the last two paragraphs, though honestly the other two would have done just as well, and partially it was because it did better on the four criteria from the previous section. But largely it was because the flaws it had all seemed to go one way: they were all flaws in which the model failed to make distinctions I felt it should be making. The other models had a mix; some were missing distinctions but also it had some needless distinctions. And I felt at the time that having all the errors go one way was a good thing. All I had to do now was run the same model with slightly more topics and I’d have a really good model. And that sort of worked, though it was more complicated than I’d hoped.

## Two Refinements {#refinements-section}

So now I had a model, with sixty topics, that looked good but not quite right. And, by design, there was a natural way to fix the problems: just add topics. It turns out that if the seed number is kept the same and the model is given more topics to play with, it makes very few changes. Or, to be a bit more precise, it makes very few changes apart from permuting the numbers. So, if two models are built with the same seed, and the second has one more topic than the first, for the vast majority of topics in the first model, there will typically be a “matching” topic in the second model. And by “matching” here I mean that the correlation between the probabilities the models give to articles being in those topics is very, very high—above 0.99 or so. The matching models won’t always have the same number, so it isn’t always easy to find them. But by simply looking at the correlations between any pairs of topics one from each model) they usually jumped out.

That meant it was possible every time a few topics were added to simply look at the new topics and ask if they were improvements or not. In an earlier attempt at this project—one that was fatally undermined by not filtering out enough latex and bibliographic words—this had led to a clear optimum arising around seventy topics. And that’s what I expected this time. But it didn’t happen.

Instead, what happened was that as I kept adding topics, it (a) kept finding relative, sensible new topics to add and (b) was not splitting up the topics I really hoped it would split. This was something of a disappointment—the project would have been more manageable for me if the model had found an optimum number of topics in the low seventies or lower. But it simply didn’t; by the standards I’d set before looking at the models, they just kept getting better as the number of topics got higher.

Eventually I settled on ninety topics. There were a bit more than I wanted, and I could have gone even higher. But it was starting to get a little more fine-grained than I wanted—I already had three distinct topics in philosophy of biology, for example. Still, the model runs in which I asked for ninety-six topics and then for one hundred topics weren’t clearly worse than the one run with ninety topics by the standards I’d set myself). So stopping here was somewhat arbitrary.

Once I had the ninety-topic model, it still wasn’t perfect. There were a few places where it looked like the model had put some things in very odd spots. Some of this remains in the finished product—the model bundles together some work on probability and coherence with historical work on Hume, and it puts one-half of the Freud papers with medical ethics and the other half of them with intention. But at this stage there were more of these overlaps than I liked.

I relied on one last feature of the topicmodels package. The algorithm doesn’t stop when it reaches an equilibrium; it stops when it sees insufficient progress toward equilibrium. One thing to do would be to refine what counts as “insufficient,” but I found this hard to control. A similar approach is to start not with a random distribution but with a finished model and then ask the algorithm to approach equilibrium from that starting point. It won't go very far; the model was finished to start with. But it will end up with a model that the algorithm likes slightly better. (The model will, for example, have a lower perplexity score.) I'll call the resulting model a _refinement_.

The refinement process takes a model as input and returns a model as output, so it can be iterated.^[If you're interested in doing this yourself, the magic code looks like ```refinedlda <- LDA(all_dtm, k = 90, model = refinedlda, control = list(seed = 22031848, verbose = 1, initialize = "model"))```. That is ```refinedlda``` is an LDA that takes the DTM I started with, and has nineity topics, and is based on a model, where that model is ```refinedlda``` itself. If loops don't scare you, you can simply loop this process to get as many iterations of refinement as you like. They took about forty-five minutes each to run when I did them.] And at this stage I had a clever thought. Since the refinement process improves the model, and it can be iterated, I should just iterate it as often as I can to get a better and better model. At the back of my mind, I had two worries at this point: one was that this was a bit like tightening a string, and if done too much the string will just snap. The other was that I had lost my mind and was fretting about mathematical models of large text libraries using half-baked metaphors concerning the physics of everyday objects.

Reader, it snapped.

After one hundred iterations, the model ended up making an interesting, and amusing, mistake. 

One signature problem with the kind of text mining I'm doing is that it can't tell the difference between a change of vocabulary that is the result of a change in subject matter, and a change of vocabulary that is the result of a change in verbal fashions. If these kinds of models are built with almost any parameter settings, a distinctive topic (or two) for [ordinary language philosophy](#topic24) will turn up. Why? Because the language of the ordinary language philosophers was so distinctive. That's not great, but it's unavoidable. Ideally, that would be the only such topic. And one of the reasons I filtered out so many words was to avoid having more such topics.

But it turns out that there is another period with a somewhat distinctive vocabulary: the twenty-first century. It's not as distinctive as midcentury British philosophy. And usually it isn't distinctive enough to really confuse most of these models. But it is just distinctive enough that if refinements are run iteratively for, let's say, four days while I'm away at a conference, the model will find this distinctive language. So after one hundred iterations, I ended up with a model whose last topic that wasn't a philosophical topic at all, but was characterized by [the buzzwords of recent philosophy](buzzwords-section).

Still, it turns out the refinements weren’t all a bad idea. After fifteen refinements, the model had separated out some of the disjunctive categories I’d hoped it would and was only starting to get thrown by the weird language of very recent philosophy. That’s the model I ended up using—the one with seed 22031848, ninety topics, and fifteen iterations of the refinement process.

## The Output

The result of all this is a model with two giant probability functions. In this section I'll talk through what those functions look like with  a worked example, and then some graphs about how well the models perform at their intended task.

The worked example involves David Makinson's article ["The Paradox of the Preface"](https://philpapers.org/rec/MAKTPO-9) [@Makinson1965]. The input to the model looks like this.

```{r preface-words, cache=TRUE}
preface_words <- word_list %>%
  filter(document == "10.2307_3326519") %>%
  arrange(-wordcount) %>%
  select(word, wordcount)

kable(preface_words, 
      col.names = c("Word", "Word Count"), 
      caption = "Words in \"The Paradox of the Preface.\"",
      digits = c(0, 0)) %>% 
  kable_styling(full_width = F)
```

That is, the word _rational_ appears fourteen times, _beliefs_ appears eleven times, and so on. This is a list of all of the words in the article, excluding the various stop words described above and the words that appear one to three times.

The model gives a probability to the article being in each of ninety topics. For this article, as for most articles, it just gives a residual probability to the vast majority of topics. For eighty-three topics, the probability it gives to the article being in that topic is about 0.0003. The seven topics it gives a serious probability to are:

```{r preface-topics, cache=TRUE}
preface_topics <- relabeled_gamma %>%
  select(document, topic, gamma) %>%
  filter(document == "10.2307_3326519") %>%
  arrange(-gamma) %>%
  select(topic, gamma) %>%
  filter(gamma > 0.02)

kable(preface_topics, 
      col.names = c("Topic", "Probability"), 
      caption = "Topic Probabilities for \"The Paradox of the Preface.\"",
      digits = c(0, 4)) %>% 
  kable_styling(full_width = F)
```

I'm going to spend a lot of time in [the next chapter](#all-90-topics) on what these topics are. For now, I'll just refer to them by number.

The model also gives a probability to each word turning up in a paradigm article for each of the topics. For those nineteen words that the model saw as input, we can look at how frequently the model thinks a word should turn up in each of these seven topics.

```{r preface-large-table, cache=TRUE}
preface_large_table <- relabeled_topics %>%
  select(-date) %>%
  filter(term %in% preface_words$word) %>%
  filter(topic %in% preface_topics$topic) %>%
  arrange(topic) %>%
  mutate(beta = as.character(signif(beta, 2)))

preface_wide_table <- preface_large_table %>%
  pivot_wider(id_cols = term, names_from = "topic", names_prefix = "t", values_from = beta)

kable(preface_wide_table, 
      col.names = c("Word", "Topic 4", "Topic 15", "Topic 37", "Topic 39", "Topic 59", "Topic 76", "Topic 81"), 
      caption = "Word frequencies for topics in \"The Paradox of the Preface.\"")
```

But the model doesn't think that "The Paradox of the Preface" is a paradigm case of any one of these topics; it thinks it is a mix of seven. Therefore, what it thinks the word frequencies in that article should be can be worked out by taking weighted means of these columns, with the weights given by the topic probabilities. And that gives the following results:

```{r preface-cross-check, cache=TRUE}
overall_sum <- sum(preface_words$wordcount)

preface_check_table <- preface_large_table %>%
  inner_join(preface_topics, by = c("topic")) %>%
  group_by(term) %>%
  mutate(beta = as.numeric(beta)) %>%
  summarise(proj = weighted.mean(beta, gamma)) %>%
  inner_join(preface_words, by = c("term" = "word")) %>%
  mutate(f = wordcount/overall_sum) %>%
  select(term, wordcount, f, proj) %>%
  arrange(-wordcount)

kable(preface_check_table,
      col.names = c("Word", "Wordcount", "Measured Frequency", "Modeled Frequency"),
      digits = c(0, 0, 4, 4),
      caption = "Measured and modeled frequencies for \"The Paradox of the Preface.\"")
```

The modeled frequency of _rational_ is given by multiplying, across seven topics, the probability of the article being in that topic, by the expected frequency of the word given it is in that topic. And the same goes for the other words. What I'm giving here as the measured frequency of a word is not its frequency in the original article; it is its frequency among the words that survive the various filters I described above. In general that will be two to three times as large as its original frequency.

The aim is that the two columns here would line up. And, of course they don't. In fact, the model doesn't end up doing very well with this article; it is still a long way from equilibrium.

```{r preface-graph, fig.cap = "Modeled and measured frequency for Makinson (1965).", fig.alt = alt_text, cache=TRUE}
cross_check_graph <- function(x){
  temp_gamma <- relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document"))
  
  temp_words <- word_list %>%
    filter(document == x)
  
  total_words = sum(temp_words$wordcount)
  
  temp_words <- temp_words %>%
    mutate(f = wordcount/total_words)
  
  temp_topics <- relabeled_topics %>%
    select(-date) %>%
    filter(term %in% temp_words$word) %>%
    inner_join(temp_gamma, by = "topic") %>%
    group_by(term) %>%
    summarise(proj = weighted.mean(beta, gamma)) %>%
    arrange(-proj) %>%
    inner_join(temp_words, by = c("term" = "word"))
  
  salient_words <- temp_topics %>%
    mutate(outlier = f + proj) %>%
    arrange(-outlier) %>%
    slice(1:5)
  
  high_number <- max(salient_words$f, salient_words$proj)
  
print(  ggplot(temp_topics, aes(x = f, y=  proj)) + 
    spaghettistyle +
    geom_point(size = 0.5, alpha = 0.5) +
    coord_fixed(xlim = c(0, high_number * 1.02), ylim = c(0, high_number * 1.02)) +
    labs(title = paste0(temp_gamma$authall[1], ", “", temp_gamma$title[1],"”"),
        x = "Measured Word Frequency",
        y = "Modeled Word Frequency")  +
    ggrepel::geom_text_repel(data = salient_words, aes(label = term)) 
)
}

salient_words <- function(x){
    temp_gamma <- relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document"))
  
  temp_words <- word_list %>%
    filter(document == x)
  
  total_words = sum(temp_words$wordcount)
  
  temp_words <- temp_words %>%
    mutate(f = wordcount/total_words)
  
  temp_topics <- relabeled_topics %>%
    select(-date) %>%
    filter(term %in% temp_words$word) %>%
    inner_join(temp_gamma, by = "topic") %>%
    group_by(term) %>%
    summarise(proj = weighted.mean(beta, gamma)) %>%
    arrange(-proj) %>%
    inner_join(temp_words, by = c("term" = "word"))
  
  the_words <- temp_topics %>%
    mutate(outlier = f + proj) %>%
    arrange(-outlier) %>%
    slice(1:5)
  
  paste(the_words$term, collapse = ", ")
}

# Burper

x <- "10.2307_3326519"

cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are rarely met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is well below the forty-five degree line. ",
  "That means they appear in the article more often than the model expects. ",
  "The word belief only appears a bit more often than expected, then others appear much more often."
)
```

On that graph, every dot is a word type. The x axis represents the frequency of that word type in the article (after excluding the stop words and so on), and the y axis represents how frequently the model thinks the word 'should' appear, given its classification of the article into ninety topics, and the frequency of words in those topics. Ideally, all the dots would be on the forty-five degree line coming northeast out of the origin. Obviously, that doesn't happen. It can't really, because, to a very rough approximation, I've only given the model ninety degrees of freedom, and I've asked it to approximate over 32,000 data points.

Actually, this is one of the least impressive jobs the model does. I measured the correlations between measured and modeled word frequency, i.e., what this graph represents, for six hundred highly cited articles. Among those six hundred, this was the twenty-third lowest correlation between measured and modeled frequency. But in many cases, that correlation was very strong. For example, here are the graphs for three more articles where the model manages to understand what's happening.

```{r good-graph-1, fig.cap = "Modeled and measured frequency for Davidson (1990).", fig.alt = alt_text, cache=TRUE}
x <- 0
x <- "10.2307_2026863"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are usually met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is close to the forty-five degree line. ",
  "That means they appear in the article about as often than the model expects. ",
  "The word truth appears a lot; it is 6% of the words in the article. The model expects a little less; around 5%. The others are very close to the forty-five degree line."
)
```

```{r good-graph-2, fig.cap = "Modeled and measured frequency for Edgington (1995).", fig.alt = alt_text, cache=TRUE}
x <- 0
x <- "10.2307_2254793"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are usually met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is close to the forty-five degree line. ",
  "That means they appear in the article about as often than the model expects."
)

```

```{r good-graph-3, fig.cap = "Modeled and measured frequency for Dworkin (1996).", fig.alt = alt_text, cache=TRUE}
x <- 0
x <- "10.2307_2961920"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are usually met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is close to the forty-five degree line. ",
  "That means they appear in the article about as often than the model expects.",
  " The word moral appears a lot, about 4% of all words in the article. And the model predicts this correctly."
)
```

There are some articles that it doesn't manage as well—typically articles with unusual words. (It also does poorly with short articles, like "The Paradox of the Preface".)

```{r bad-graph-1, fig.cap = "Modeled and measured frequency for Thomson (1998).", fig.alt = alt_text, cache=TRUE}
x <- 0
x <- "10.2307_2671962"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are rarely met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is far from the forty-five degree line. ",
  "The model expects the words property and properties will appear a lot, 3-4% of the time, but they make up only about 1% of the words. It does not expect the words time, part and, especially, clay, to appear as often as they do."
)
```

```{r bad-graph-2, fig.cap = "Modeled and measured frequency for Elster (1990).", fig.alt = alt_text, cache=TRUE}
x <- 0
x <- "10.2307_2381783"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are rarely met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Each of them is far from the forty-five degree line. ",
  "The model expects the words society and social to appear more often than they do. But it is very surprised at how often the words honor, norms, and revenge, appear."
)
```

```{r bad-graph-3, fig.cap = "Modeled and measured frequency for Fara (2005).", fig.alt = alt_text, cache=TRUE}
x <- 0
x <- "10.2307_3506173"

cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are rarely met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Four of them are far from the forty-five degree line. ",
  "The model expects the words possible, worlds and, especially, world, to appear much more often than they do.",
  " But it expects the word disposition to appear much less. ",
  "The model does correctly predict that the word true will appear about 2% of the time."
)
```

A few different things are going on here. In Elster's article, the model doesn't expect any philosophy article to use the word _revenge_ as much as the author does. In Fara's article, the model lumps articles about modality (especially possible worlds) in with articles on dispositions. (This ends up being [topic 80](#topic80).) And so it expected that Fara will talk about worlds, given he is also talking about dispositions, but he doesn't. Thomson's article has both of these features. The model is surprised that anyone is talking about clay so much. And it expects that a metaphysics article like Thomson's will talk about properties more than Thomson does.

It isn’t perfect, but as seen above, it does pretty well with some cases. The papers I’ve shown so far are pretty much outliers though; here are some more typical examples:

```{r medium-graph-1, fig.cap = "Modeled and measured frequency for Lewis (1979).", fig.alt = alt_text, cache=TRUE}
x <- 0
x <- "10.2307_2215339"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are mostly met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Four of them are close to the forty-five degree line. ",
  "But the model expects the word laws to appear much more often than it does."
)
```

```{r medium-graph-2, fig.cap = "Modeled and measured frequency for Lakoff and Johnson (1980).", fig.alt = alt_text, cache=TRUE}
x <- 0
x <- "10.2307_2025464"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are mostly met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Four of them are close to the forty-five degree line. ",
  "That means they appear in the article about as often than the model expects.",
  " But the model does not expect the word metaphor to appear so often."
)
```

```{r medium-graph-3, fig.cap = "Modeled and measured frequency for Kelly (2003).", fig.alt = alt_text, cache=TRUE}
x <- 0
x <- "10.2307_20140564"
cross_check_graph(x)

alt_text <- paste0(
  "A scatterplot where the x axis shows how often each word appears in ",
  (relabeled_gamma %>%
    select(document, gamma, topic) %>%
    filter(document == x) %>%
    inner_join(articles, by = c("document")))$citation[1],
  ", and the y axis shows how often the model anticipated that word to appear. ",
  "Here the expectations are rarely met. ",
  "The words ",
  salient_words(x),
  " are highlighted. Belief and reason are far above the forty-five degree line, epistemic and rationality far below it. ",
  "The model expects the word reasons to make up 2% of the words in the article, and in fact it makes up 3%."
)
```

It's not perfect, but the general picture is that the model does a pretty good job of modeling 32000 articles given the tools it has. And, more importantly from the perspective of this book, the way it models them ends up grouping like articles together. And that's what I'll use for describing trends in the journals over their first 138 years.

## Strengths and Weaknesses {#strengths-and-weaknesses-section}

The benefit of using this kind of modeling is that it allows every article to be taken into account. This is the history of philosophy (in these journals) without any gaps whatsoever.

And this is no small feat. Remember that there are 32,261 articles that are being looked at. Let’s say that eight hours a day, five days a week, could be dedicated just to reading these articles, and that one could on average read an article per hour. Some, to be sure, would take less than an hour, even to read closely. But just one hour is an optimistic reading time for the longer articles. Still, let’s make the optimistic assumption. That would mean 807 weeks of just reading through them all. If one takes two weeks a year off, it would take sixteen years just to do the reading. And at the end of that time, one would, at best, have some sketchy notes on the articles and not anything that could be used for an analysis.

To analyze all the articles, to really have no gaps, then the only way is by machine.

But there are a number of downsides to this algorithmic approach, all of which come from the fact that the machine is just doing string recognition. The algorithm doesn’t know any semantics—just syntax. And this causes some complications. I’ll mention five here, along with a brief discussion of how badly they impacted the model I ended up using.

One problem that turned out not to be too big a deal was that the algorithm has a hard time distinguishing between different uses of the same word. But while this is hard, it isn’t impossible. The model seems, for example, to understand the difference between how _function_ is used in philosophy of biology versus how it is used in logic and mathematics. It didn’t run together the different uses of _realism_ or _internalism_ and _externalism_ in a way that I would have expected. There is a hint of running together _scepticism_ in the sense most relevant to epistemology with other kinds of philosophical scepticism. (Someone who is a free-will sceptic doesn’t say that people don’t know whether free will exists but that people know it doesn’t.) But maybe this isn’t too much of a problem, since the views aren’t that separate.

The one time that this particular model seems to have gotten confused over the two related meanings of a word concerned _free_. [Topic 35](#topic35) is a mishmash of work on free will, with work on political freedom. It’s possible to think this isn’t too bad, since the subjects are somewhat connected. But it’s not optimal, and eventually it’s necessary to separate out free will and political freedom. But the big picture is that something that seemed likely to be a problem turned out, pleasingly, to not be that bad.

A second, opposing problem  is that sometimes the differences in topics come from a change in terminology. This can be seen most clearly, I think, in the logic topics in the model. Papers about sequents are put in a different topic than papers about syllogisms. Papers about implications are put in a different topic than papers about validities. Now there is a sense in which that’s a good thing, and the model is picking up a philosophically significant change. But it’s a relatively minor change compared to what the model thinks. Still, this isn’t a particularly serious problem. The worst-case scenario is that one has to come back in later and manually note that the papers on validities and papers on implications need to be put back together when we’re doing analysis. That’s a bit of work but it isn’t too bad—just remember that it happens.

A third, and related, problem comes from the model making fine-grained distinctions within a subject. I mentioned earlier that I saw several models that ended up separating out work on causation that didn’t discuss counterfactuals (such as that of Mackie) from post-Lewisian work, where counterfactuals are front and center. That’s not great—these really are on the same topic—but it isn’t too bad. Again, the worst-case scenario is that these topics need to be combined by hand when doing analysis. But in practice I don’t think I really saw this problem arise in this particular run of the model.

A potentially bigger problem is the converse, which I already discussed when talking about choosing the number of topics. Sometimes the topics are just disjunctive. For example, [topic 37](#topic37) ends up being half about sets and half about the grue paradox. There is a connection of sorts here—Nelson Goodman is kind of important to both literatures. But really this shouldn’t be a single topic. As I already noted, this is a hard problem to fix. If the number of topics are increased, the model becomes harder to read, and it’s just as likely to split a coherent topic like causation) as it is to split a disjunctive topic.

I did three things here to address these disjunctive topics. One, that I’ve already mentioned, was to keep running refinements until the worst of the disjunctiveness was polished away. Before the refinements, some papers on probabilistic epistemology got classified in with papers on Hume, and I don’t know what the computer was thinking. A handful ended up there after the refinements, but not nearly as many.) A second is to use very clear labels for the topics, like “Sets and Grue,” to indicate that it is a disjunctive topic. And a third is to run a further analysis on articles in that topic to divide up the sets of articles from the grue articles. Eventually there ended up being ten topics where I felt this kind of split was worthwhile.

The fifth and final problem is that the algorithm can’t tell changes of topic apart from changes in style. If it becomes a requirement on all right-thinking philosophers to express oneself more or less exclusively in monosyllables, as seems to have been the case in midcentury Britain, then the algorithm will think that there is a new topic that is being discussed right then. I’m exaggerating of course about midcentury Britain, but there is a trend that matters, and that I’ll talk much more about later.

Or imagine what would happen if every philosopher all at once decided that objections shouldn’t be responded to with a new theory that has distinctive consequences but instead one should respond to _worries_ with a new _account_ that has distinctive _commitments_. Well, the model will think that there is this cool new subject about “worries,” “accounts,” and “commitments,” and that they’re being talked about. And if this stylistic change happens all at once across philosophy, the model will think that the generalist journals, the philosophy of science journals, and the moral and political journals are suddenly obsessed with the worry/account/commitment subject. Of course, philosophy couldn’t be so caught up chasing trends that something like this would happen all at once, could it? Could it? Let's return to this issue [at the very end](#buzzwords-section) and see how bad things got.

## Regrets {#regrets-section}

Now that I’ve written the whole methodology that I used, there are a few things I wish I’d done differently. This wish clearly isn’t strong enough to make me scrap the project and start again.^[I did in fact scrap several versions of this when writing up the model revealed mistakes in the model building. This is the model that resulted from acting on the lessons of those mistakes.] But I hope that others will learn from what I’ve done, and to that end, I want to be up front about things I could have done differently.

First, I should have filtered even more words. There are four kinds of words I wish I’d been more aggressive about filtering out:

- There are some systematic OCR errors in the early articles. I caught _anid_, which appears over three thousand times. (It’s almost always meant to be _and_, I think.) But I missed _aind_, which appears about 1,500 times. And there are other less common words that are also OCR errors and should be filtered out.
- I caught a lot of latex words but somehow missed _rightarrow_, as well as a few rarer words.
- If a word is hyphenated in the original journal, each half appears as a word in this data set. (At least if the data was generated by OCR.) I caught a few of the prefixes and suffixes that turn up for that reason, but missed _ity_, which ends up being a reasonably common word.
- And I caught a lot of words that almost always appear in bibliographies, headers, or footers but missed _basil_ (which turns up on a table later) and _noûs_ (though I caught _nous_).

In general, I could have been way more aggressive filtering words like these out.

But second, I think it was a mistake to filter out words that appear one to three times in articles. This actually makes perfect sense for long articles, and for some really long articles words that appear four or five times could be eliminated as well. But it’s too aggressive for short articles. I needed some kind of rule such as filtering out words that appear less than one time in two thousand in the article. It is important, I think, to filter out the words that appear just once or else it’s easy to miss OCR errors and weird latex code. But after that there needs to be some kind of sliding scale.

The next three things are much more systematic, though also less clearly errors.

The third problem was that my model selection was too stepwise and not holistic enough. I found the best sixty topic model I could find. Then I increased the topics on it (eventually to ninety) until the topics looked as good as they could get while holding a fixed seed number from the search through sixty topics. Then I ran refinements on it until the refinements looked like they were damaging the model. Then I split some of the topics up for categorization. What I didn’t do at any time was look back and ask, for example, how the other sixty topic models would look if I applied these adjustments to them.

There was a reason for that. Each of those adjustments cost quite a lot of my time, and even more computer time. Doing the best at each step and then locking in the result makes the process at least a bit manageable. But I should have been (a) a bit more willing to revisit earlier decisions and (b) more forward looking when making each of those intermediate decisions. I was a bit forward looking at one point; one of my criteria for choosing between sixty topic models was a preference for unwanted conflations over unwanted splits. And that was because I knew I could fix conflations in various ways. But I should have been both more forward looking and more willing to take a step or two backward. And maybe I could have stuck much closer to sixty topics if I had.

The fourth problem was that I didn't realize how bad a topic [arguments](#topic55) would turn out to be. For the purposes of the kind of study I'm doing, it's really important that the topics really be _topics_ in the ordinary sense, and not tools or methods. Now this is hard in philosophy, because philosophy is so methodologically self-conscious that there are articles that really are about all the tools and methods one might care about. But I wish I'd avoided having a topic about a tool. (I'll come back in section \@ref(raw-weight-count) to a formal method one can use for detecting these kinds of topics early in the process.)

The fifth problem, if it is a problem, is that I wasn't more aggressive about expanding the list of stop words. This model has a topic on [ordinary language philosophy](#topic24). Actually, _all_ the models I built had a topic like this (at least once they had at least fifteen or so topics). But the keywords characteristic of this topic are words that really could have been included on a stop words list. They are words like _ask_ and _try_. And one side-effect of this is that the model keeps thinking a huge proportion of the articles in the data set are, perhaps,  ordinary language philosophy articles.

Another way to put this is that the boundary between a stop word and a substantive word (in this context) is pretty vague. And given that ordinary language philosophy was a thing that happened and that affected how everyone (at least in the United Kingdom) was writing for a while, there is a good case for taking a very expansive understanding of what the stop words were.

The choice I made was to not lean on the scales at all, and just use the most common off-the-shelf list of stop words. And there was a good reason for that: I wanted the model to not simply replicate my prejudices. But I half-think I made the wrong call here, and that the model would be more useful if I had filtered out more "ordinary language".

<!--chapter:end:01-methodology.Rmd-->

# The Ninety Topics {#all-90-topics}

```{r setup-alts}
# Eval after doesn't seem to work if there is no value for these terms.
# So I'm giving them a starting value here.
alt_text <- ""
alt_text_journals <- ""
```
In this chapter I go through all ninety topics that the model generates. I’ll present a bunch of automatically generated facts about each topic, then say something about either the history of the topic or how it fits into the larger model. In this introduction I’ll explain how each of the automatically generated facts is in fact generated, using the example of [topic 21](#topic21).

As can be seen in the sidebar, each topic has a number and a name. The numbers are taken from the age of the articles in the topic. Lower numbers pick out older topics. While working through the ninety topics, the closer and closer one gets to the present of philosophy. The names are things that I supplied. The statistics that follow are mostly the things I was looking at when I gave each topic its name.

I’ve put each topic into one (or sometimes two) **categories**. These are the familiar disciplines of contemporary philosophy: metaphysics, ethics, history, and the like. For early modern, the categorization was easy.

```{r example-category}
jjj <- 21
if(!the_categories$cat_num[jjj] == 13){
  cat_nam <- the_categories$cat_name[jjj]
}
if(the_categories$cat_num[jjj] == 13){
  temp_category_tibble <- tibble(
    topic = (jjj*100+1):(jjj*100+2))
  temp_category_tibble <- temp_category_tibble %>%
    inner_join(the_categories, by = "topic")
  cat_nam <- paste(temp_category_tibble$cat_name, sep = "/", collapse = "/")
}

cat("**Category**: ", cat_nam, "\n\n")
```

For each topic, the model generates something such as a probability of a word turning up given that an article is certainly) in that topic. That can be used to generate keywords for each topic. But there are a couple of hitches.

The first thing I tried was to identify the keywords for a topic with the words that have the highest probability of turning up in articles in that topic. But that gives the same keywords for most of the topics. Indeed, for every topic it gives keywords that were borderline cases of being  [stop words](#stop-words).

The second thing I tried was to identify keywords using those words where the ratio between the probability the word turns up in this topic with the probability it turns up in an arbitrary article. This is better but still not right. Doing this makes all the keywords incredibly rare words that essentially never turn up in any other topic. (Occasionally this would make _weatherson_ a keyword, for example, though not usually where I expected.)

What I settled on was to use that ratio but only quantify the words that are at least reasonably common across the data. Roughly, they are words that turn up at least once in every twenty thousand words (excluding stop words).^[More carefully, for any word $w$ and model $t_k$ the model provides something like $\Pr(w | t_k)$, the probability of a word turning up in an article in that topic. I'm looking for the words that maximize $\frac{\Pr(w | t_k)}{\sum \Pr(w | t_i)}$, where the sum is over the ninety topics, and the constraint is that the average value of $\Pr(w | t_i)$ is at least $\frac{1}{20000}$.] Applied to early modern, that gives us the following keywords:

```{r example-keywords}
distinctive <- distinctive_topics[(jjj-1)*15 + 1, 1]
for (jj in 2:15){
  distinctive <- paste(distinctive, distinctive_topics[(jjj-1)*15 + jj, 1], sep = ", ")
}

cat("**Keywords**: ", distinctive, "\n\n")
```

Hopefully it isn't too surprising now why I called this early modern. I don't know what _vii_ is doing there; arguably it should have been filtered out. 

Next I'll look at the size of the topic. There are two ways of looking at this, and since I'm going to be using them a lot, it's worthwhile going over them at some length.

The model assigns each article a probability of being in each topic. So for each article there is a topic with maximal probability. (In principle there could be ties, but in practice that doesn't seem to happen.) I'll follow a fairly standard practice and say that an article is **in a topic** if that topic has maximal probability. And then the number of articles in a topic is the number of articles such that this topic gets higher probability for that article than any other topic. These can be counted up to get a sense of the size of the topic.

```{r example-raw-count}
require(toOrdinal)

cat("**Number of Articles**: ", overall_stats$r_count[jjj], "  \n", sep="")
cat("**Percentage of Total**: ", 100*overall_stats$r_percent[jjj], "%  \n", sep="")
cat("**Rank**: ", toOrdinal(overall_stats$r_rank[jjj]), "\n\n", sep="")
```

There are 398 articles that are in early modern in this sense). That’s slightly more than average, since there are about 360 articles in the average topic. It’s 1.2 percent of the total; as can be calculated, the average topic would have 1.1 percent. And if the topics are ordered from largest to smallest, it makes Early Modern the thirty-second largest of the topics.

But this isn’t the only way to measure the size of a topic. The model gives a probability of being in early Mmodern to every article in the data set. Those probabilities can be added up to get another way to measure topic size. Formally, this is the way to calculate the _expected_ number of articles in that topic given the probability distribution, though I don’t believe thinking of these numbers as expected values is particularly helpful. If that is done, summing the probabilities of being in early modern across all articles, the following statistics are retrieved:

```{r example-weighted-count}
cat("**Weighted Number of Articles**: ", overall_stats$w_count[jjj], "  \n", sep="")
cat("**Percentage of Total**: ", 100*overall_stats$w_percent[jjj], "%  \n", sep="")
cat("**Rank**: ", toOrdinal(overall_stats$w_rank[jjj]), "\n\n", sep="")
```

I'll call this calculation the **weighted number** of articles in the topic. As I said, mathematically it's just the formula for expected value calculation, but I'm going to use it more like a weighted sum, hence the name. By this measure, early modern looks a little smaller. It's now only the forty-ninth largest topic, and is under 1 percent. This is one of the larger gaps between the two ways of measuring the size of a topic; mostly they go together. (Though there is going to be one [special case](#topic55) where they come dramatically apart.)

Next I'll look at some facts about the dates of articles in that topic.

```{r example-dates}
cat("**Mean Publication Year**: ", overall_stats$mean_y[jjj], "  \n", sep="")
cat("**Weighted Mean Publication Year**: ", overall_stats$wy[jjj], "  \n", sep="")
cat("**Median Publication Year**: ", overall_stats$median_y[jjj], "  \n", sep="")
cat("**Modal Publication Year**: ", overall_stats$modal_y[jjj], "\n\n", sep="")
```

The first, third and fourth statistics there are easy to understand. I simply took the 398 articles in early modern, and found the mean, median and modal publication dates for them. The second is only a little trickier. I calculated the weighted average of the publication year of all articles in the data set, where the weights are given by the probability of being in early modern. As happens here, the first three numbers usually end up being very similar. The fourth can be quite random and usually leans toward the present since there are more articles published now than in the past.

The last statistics I'll look at, before going on to some graphs, concern how close early modern is to various neighbours. I'll present these then explain them.

```{r example-neighbours}
cat("**Topic with Most Overlap**: ",
    the_categories$subject[closest_neighbour$othertopic[jjj]], 
    " (", 
    round(closest_neighbour$g[jjj],4),
    ")  \n",
    sep="")
cat("**Topic this Overlaps Most with**: ",
    the_categories$subject[closest_neighbour_inverse$topic[jjj]], 
    " (", 
    round(closest_neighbour_inverse$g[jjj],4),
    ")  \n",
    sep="")
cat("**Topic with Least Overlap**: ",
    the_categories$subject[furthest_neighbour$othertopic[jjj]], 
    " (", 
    round(furthest_neighbour$g[jjj],5),
    ")  \n",
    sep="")
cat("**Topic this Overlaps Least with**: ",
    the_categories$subject[furthest_neighbour_inverse$topic[jjj]], 
    " (", 
    round(furthest_neighbour_inverse$g[jjj],5),
    ")\n",
    sep="")

opts_knit$set(eval.after = "fig.cap") # Need this for next chunk
```

Recall that each of the 398 articles in early modern is also assigned a probability of being in each of the other eighty-nine topics. I then calculated the average probability of being in each of the eighty-nine topics among these 398 articles. And the first line here reports that the highest average probability was for [idealism](#topic02). It isn’t huge—just 4.6 percent, but given I’m only looking at nonmaximal probabilities, and it’s a ninety-way partition, this isn’t that small. The third line reports that the lowest of these average probabilities was for [formal epistemology](#topic84). Usually there is a group of ten to twenty topics that have vanishingly small mean probabilities here and it’s a bit random which of them the model picks out.

Each of the other topics could also be looked through and ask, Of the articles in those topics, what is the average probability the model gives to them being early modern articles? And which topic is such that this average is highest? If this is done to all eighty-nine calculations, it turns out the answer is the topic about the [ontological argument](#topic29). This is one of the smaller topics, but the articles in it have on average a probability of just over 2 percent of being early modern articles. On the other hand, the articles in [game theory](#topic75) have a mean probability of being early modern articles of only 0.01 percent. I suspect this is because the model separates out early modern from [social contract theory](#topic31), and any paper on the intersection of seventeenth-/eighteenth-century philosophy with game theory ends up classified as a social contract paper.

That’s enough statistics to get started. I will move on to some nice graphs. First, I’ll show the proportion of articles that are in that topic in each year. I’m using weighted sums for this. So really what each point here shows is the average probability that an article in this year is in this topic.




```{r exampleoverall, fig.cap = str_to_sentence(the_categories$subject[jjj]), fig.height = 5.2, fig.alt = alt_text}
source('topic_comments/topic_summary_overall_graph.R')
alt_text <- paste0(
  "A scatterplot showing which proportion of articles each year are in the ", 
  the_categories$sub_lower[jjj],
  " topic. The x axis shows the year, the y axis measures the proportion of articles each year in this topic. There is one dot per year. The highest value is in ",
  max_year$year[1],
  " when ",
  scales::percent(max_year$y[1], accuracy = 0.1),
  " of articles were in this topic. The lowest value is in ",
  min_year$year[1],
  " when ",
  scales::percent(min_year$y[1], accuracy = 0.1),
  " of articles were in this topic. The full table that provides the data for this graph is available in Table A.",
  jjj,
  " in Appendix A."
)
```

Next, I'll do the same thing but broken down by journals.

```{r examplefacet, fig.cap = paste(str_to_sentence(the_categories$subject[jjj]), "articles in each journal"), fig.alt = alt_text_journals}
jjj <- 21
source('topic_comments/topic_summary_facet_graph.R')
alt_text_journals <- paste0(
  "A set of twelve scatterplots showing the proportion of articles in each journal in each year that are in the ",
  the_categories$sub_lower[jjj],
  " topic. There is one scatterplot for each of the twelve journals that are the focus of this book.",
  " In each scatterplot, the x axis is the year, and the y axis is the proportion of articles in that year in that journal in this topic.",
  " Here are the average values for each of the twelve scatterplots - these tell you on average how much of the journal is dedicated to this topic. ",
  temp_by_journal_summary,
  "The topic reaches its zenith in year ",
  slice_max(temp_by_year, m, n = 1)$year,
  " when it makes up, on average across the journals, ",
  percent(slice_max(temp_by_year, m, n = 1)$m, accuracy = 0.1),
  " of the articles. And it hits a minimum in year ",
  slice_min(temp_by_year, m, n = 1)$year,
  " when it makes up, on average across the journals, ",
  percent(slice_min(temp_by_year, m, n = 1)$m, accuracy = 0.1),
  " of the articles."
)
```

As you can see, this has years on the x axis, and ratios on the y axis, and twelve "facets". What each dot represents is an average probability for an article in a particular year-journal pair being located in this topic. It helps to understand what this means by working through an example.

Look in the facet for _Philosophical Review_. There are two dots that are much higher than the rest, both of them around 0.15. The left-hand one, which is just under 0.15, is from the early 1930s. The right-hand one, just over 0.15, is from 1999. And I'm going to talk through this one for a bit. Here are the eleven articles in _Philosophical Review_ that year, along with their probability of being in topic 21.^[The automatically generated citations include messy things like "Sleighjr", and I just haven't corrected them - I'm just going with what JSTOR feeds me.]

```{r prexample1}
phil_review_1999 <- relabeled_gamma %>%
  filter(topic == 21, year == 1999, journal == "Philosophical Review") %>%
  inner_join(articles, by = "document") %>%
  arrange(fpage) %>%
  select(citation, gamma)

kable(phil_review_1999, 
	col.names = c("Article", "Probability of Being Early Modern"),
	caption = "Articles in Philosophical Review, 1999")
```

If these eleven probabilities are summed, the answer  `r sum(phil_review_1999$gamma)`. And dividing by eleven to get the average, the answer is  `r mean(phil_review_1999$gamma)`. And that's where the dot I was pointing out comes from.

Note that this might feel a little light. Topic 21 is, more or less, early modern metaphysics and epistemology. And that looks like it should be three or four out of the eleven topics, which is much more than 0.15. What’s going on?

One thing that's happened is somewhat inevitable when dealing with history. The article by Michael Griffin [-@Griffin1999] is definitely about Leibniz, which is why a probability well above 0 is seen, but it’s also about modals and counterfactuals. And there is another topic that’s all about [modality](#topic80), so a bunch of the probability went there. (Indeed, it is officially in that topic because that probability was maximal.) And it’s about God, and there are two topics about God in the model, and some of the probability went there. This is the general case. History of philosophy articles involve a lot of philosophy, and whatever kind of philosophy they involve, the model will want to put them with other philosophy articles discussing those points.

But how does that explain the article by Dugald Murdoch [-@Murdoch1999]? Surely that’s an early modern article. And it surely is. The simplest thing to say there is that when there are ninety exclusive hypotheses, ending up with a probability of 0.6 for one of them is actually a lot. And some of the remaining probability also makes sense. There is a topic for the [ontological argument](#topic29), and some of the probability for Murdoch's article goes there. And there is a topic on [arguments](#topic55), and the articles in that topic are mostly about how to understand circularity, so some of the probability goes there. And there is a topic for [ordinary language philosophy](#topic24), and most articles use some ordinary language, and hence are given a non-trivial probability of being in it. That topic causes some complications, because it's as much a style as a topic, and I'll come back to it a lot in what follows.

In essence, that’s how the facet graphs are constructed. Apart from the Mind facet, the graphs do not start all the way to the left edge, because those journals didn’t start publishing until after Mind did. The color of the dots is taken from the color the topic has in the big graphs done later when all ninety topics are presented on one graph.
I need to add a few words about the scale of these graphs. There were two competing considerations when setting the scale. On the one hand, it would be good to have the scale of the Y-axis be the same for all topics. That way, when flipping through the pages, there is an immediate visual sense of how big the topic is. On the other hand, that approach forces us to set the scale to accommodate outliers. And if that is done, the vast majority of the graphs are just dots that bounce on or just above the x axis.

To deal with both these concerns, I’ve split the difference somewhat. On the one hand, the vast majority of the overall graphs—the ones that show the prevalence of the topic across all journals—are set to a common scale. I’ve adjusted the scale a little for the really big topics, such as [idealism](#topic02) and [ordinary language philosophy](#topic24), but mostly the scale is the same. On the other hand, the scales for the graphs that break things up by journal vary a lot. So, when trying to get a quick visual impression from the graphs, the first graph says something about the size of the topic, and the second graph says something about the distribution of the topic over the journals. But the second graph doesn’t say, unless one looks closely at the labeling on the y axis, how big the topic is in each journal.

I have made one other visual note to help read the graphs. The gridlines in each graph are at the same places. Therefore, if the scale is increased for a large topic, there will be lots of gridlines in the background, and that’s a sign that it’s a bigger topic as well. I’m not sure this is an ideal solution, but it seemed less bad than the others I tried.

After these graphs, there are two tables of the articles that are in the topic. Here is the first:

```{r examplecharart}
cat("<br/>")
jjj <- 21
topic_crossref <- 21
source('topic_comments/topic_summary_char_art.R')
temp_dt
```
This table actually lists all 398 articles in early modern. By default it displays ten at a time. It's possible to move through the list by the numbers at the bottom, or extend it using the dropdown menu in the top left.

The search box in the upper right will search for any text in the citation. This is helpful either for finding title words (e.g., _Spinoza_), or author names (e.g., _Curley_).

The year and citation columns are self-explanatory. The probability column gives the probability that the article is actually in early modern. This is helpful to check when finding an article that looks misclassified. If the number is under about 0.25, that means the model is fairly undecided about what to do with the article, and it ended up here for want of somewhere better to put it.

The table is sortable by any of those three columns. But its default sort order is by what I'll call _typicality_. This is the product of its probability of being in the topic, with the log of its length in pages. I'm using this complicated formula because for most topics, the high-probability articles are short discussion notes where the model doesn't see anything to offset its initial judgment about where the article should go. Using this (totally made up) typicality measure as the initial sort meant that the articles that turned up here were more familiar and gave me a better sense of what was, well, typical for the topic.

The second table is a list of highly cited articles in the topic.

```{r example-t21e}
cat("<br/>")
source('topic_comments/topic_summary_high_cites.R')
high_table
```
I used ["Publish or Perish"](https://harzing.com/resources/publish-or-perish) [@Harzing2007] to download the fifty most cited articles from each of the twelve journals according to Google Scholar. I do _not_ stand by these lists as being particularly accurate. I tried a couple of obvious ways to download the data, and they had obvious shortcomings, which I corrected. And then I thought the lists I had were good enough for illustrative purposes, so I stopped. But there were so many obvious things to correct that I'm sure there were also nonobvious things to correct. But the point is not to do a citation study; it's to list some familiar articles that are in the topic.

The order of this list is most to least Google Scholar citations. Note that it is not the six hundred most cited articles in the twelve journals; the fiftieth most cited _Journal of Philosophy_ article is cited more than practically any _Philosophical Quarterly_ article. ^[With one [notable exception](https://philpapers.org/rec/JACEQ).] But again, the point is not to measure how well cited the topics are; it's to list some familiar articles in the topics. And I felt that spreading around the journals was best for that purpose.

Google Scholar isn't particularly reliable at distinguishing between articles with the same title, year, and publication venue. So there is the occasional doubling up as with the two Spitzer articles that are the highly cited articles here. As I said, I don't stand behind the accuracy of these lists; they are there to illustrate the topic.

Some very low probabilities turning up in these tables will be noticeable. Note that the first of the two Spitzer articles only has a probability of being in early modern of about 0.21. But that's higher than its probability of being in any other topic. Sometimes the most influential articles in philosophy are ones that don't neatly fit into one topic or another. Indeed, one of the themes of this book is that by highlighting where the most work has been done, it's possible to see more clearly what areas are left open. They'll probably be the areas that produce highly cited articles of the next 138 years.


<!--chapter:end:02-topics.Rmd-->

```{r t01a}
# Burp
jjj <- 1
source('topic_comments/topic_summary_data.R') # Get data
opts_knit$set(eval.after = c("fig.cap", "fig.alt"))
alt_text <- "Placeholder"
```

```{r t01b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t01c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t01d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic # Burp
temp_dt
```

```{r t01e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
# Have to do this at same time as previous script or a weird caching error occurs, not sure why.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n\n**Comments**\n")
```

Several philosophy journals started their lives as combined journals of philosophy and psychology. Most notably for our purposes, _Mind_ is as much a psychology journal as a philosophy journal for several years. And this topic collects those articles.

It isn't entirely what is now called _psychology_. [George Dawes Hicks](https://philpapers.org/rec/KEEGDH) was an important philosopher, and longtime president of the Aristotelian Society [@Keeling1941]. He read a lot of papers to the society, and it wasn't uncommon when I was building these models to have a run produce a topic that was largely centered on his work. But he ends up being relatively peripheral to the story this model tells, for better or worse.

Perhaps relatedly, we shouldn't think of the boundary between philosophy and psychology in the prewar years as being as strict as it was for much of the twentieth century. As Omar W. Nasim notes in his introduction to the Aristotelian Society's [virtual issue on the emergence of analytic philosophy](http://www.aristoteliansociety.org.uk/the-virtual-issue/the-virtual-issue-no-2/), even an issue like the existence of the external world was often viewed by philosophers at the time as a psychological issue [@Nasim2014].

One thing that surprised me a little was that the model didn't take the recent empirical turn in philosophy of mind as a reason to put more articles into this topic. I would not have been surprised if some recent work on attention, for example, had turned up. But the model seems to have figured out that this topic is pretty much dead as far as the philosophy journals are concerned.

<!--chapter:end:topic_comments/topic01.Rmd-->

```{r t02a}
jjj <- 2
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t02b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t02c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t02d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t02e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The biggest topic of the ninety, at least by number of articles, is one of the earliest. It was a huge surprise to me just how big this topic is, especially in Britain. I knew that idealism was important; I had no idea how important.

Note that the graphs shown above do not follow the conventions that I use for most of the ninety topics. The scale on the second graph is several times larger than the scale of its counterparts in other topics. I've left all those light gridlines in to make clear just how big this was in the early years. At times it is thirty times larger than the average of the other eight-nine topics.

As is sen from the journal names just on the very top articles, this is very much a British topic. There is idealism, or at least idealist-adjacent work, in the United States. But it tends to end up either in [pragmatism](#topic05), or [life and value](#topic03). The latter is sort of idealist-friendly moral and political work. This topic is where hardcore metaphysics lives.

While the topic doesn't contain much political philosophy, it contains a number of politically significant figures. One of these, who will turn up several times in this book, is Bertrand Russell. The articles of Russell's that turn up here aren't particularly idealist, but they do engage with idealism, which is what the model is picking up. In general, the model doesn't do a very good job at figuring out which side of a debate anyone is on—it does a better job of figuring out which debate they are talking about. That will be relevant as we move along, but it isn't particularly relevant here; most of these articles are examples of idealism, not merely discussions of it. The other politically significant figure is Arthur Balfour, later prime minister of the United Kingdom and (more significantly) its foreign secretary from 1916 to 1919. I don't think many of the folks currently writing in _Mind_ will go on to have as much influence on the world as Balfour did, but I guess one never knows.

There are a handful of articles by Indian philosophers, or about Indian philosophy, that turn up in this topic. I don't know that this list is complete, but the Indian articles in this topic include:

- Homo Leone, 1912, "[The Vedantic Absolute,](https://philpapers.org/rec/LEOTVA)" _Mind_ 21:62–78.
- A. R. Wadia, 1919, "[Mr. Joachim's Coherence-Notion Of Truth](https://philpapers.org/rec/WADMJC)" _Mind_ 28:427–35.
- S. N. Dasgupta, 1922, "[The Logic Of The Vedānta](https://philpapers.org/rec/DASVLO)" Proceedings of the Aristotelian Society 22:139-56.
- E. J. Thomas, 1923, "[Dasgupta's "History Of Indian Philosophy"](https://philpapers.org/rec/THODHO)" _Mind_ 32:391–92.
- A. K. Majumdar, 1926, "[The Personalistic Conception Of Nature As Expounded In The Sānkhya Philosophy](https://philpapers.org/rec/MAJTPC)" _Philosophical Review_  35:53–63.
- A. R. Wadia, 1927, "[Is Change Ultimate?](https://philpapers.org/rec/WADICU)" _Philosophical Review_  36:338–45.
- Nikunja Vihari Banerjee, 1930, "[Some Suggestions Towards The Construction Of A Theory Of Sense-Perception](https://philpapers.org/rec/BANSST)" _Philosophical Review_ 39:587–96.
- Mahendranath Sircar, 1933, "[Reality In Indian Thought](https://philpapers.org/rec/SIRRII)" _Philosophical Review_  42:249-71.
- K. R. Srinivasa Iyengar, 1939, "[The Notion Of Dependence](https://philpapers.org/rec/IYETNO)" _Philosophical Review_  48:506–524.
- Pravas Jivan Chaudhury, 1959, "[Vedanta As Transcendental Phenomenology](https://philpapers.org/rec/CHAVAT)" _Philosophy and Phenomenological Research_ 20:252–263.

As well there are five articles by [Hiralal Haldar](https://philpapers.org/s/Hiralal%20Haldar), which are more squarely on European idealism.

```{r haldar}
author_dt(c("Hiralal Haldar"), "Hiralal Haldar", "haldar")
```

One might think that fifteen articles out of 1600 is not a lot. And that might be right. But it's a bit more than is seen in other topics, so it seemed worth noting.

Much of the story of Anglophone philosophy in the first half of the twentieth century involves disputes between two or more of the four schools mentioned in the first paragraph of Roy Wood Sellars's "[A Correspondence Theory of Truth](https://philpapers.org/rec/SELACT)": idealism, pragmatism, positivism, and realism [@Sellars1941, 645]. I've listed them in order of their influence on philosophy in the first half of the century. Or, one could just as well say, I've listed them in inverse order of their influence on philosophy in the second half of the century. 

Idealism, once the dominant force in Anglophone philosophy, simply stopped being part of the debate. It wasn't, I think, that any particular arguments eventually did it in. Rather, it was as [Wilfred Sellars said in 1948](https://philpapers.org/rec/SELRAT):

> It has been said that a system of philosophy is not refuted, but becomes ignored. This is true. It is equally true (and for the same reason) that a clash of systems in the philosophical drama ends not in victory and defeat, but in a changing of the scene. Put from a somewhat different point of view, the historical development of philosophy is more truly conceived as the periodic formulation of new questions, than as a series of attempted answers to an enduring body of problems. To be sure, the new questions which appear in this process can be regarded, for the most part, as revisions or reformulations of earlier issues; however, the fact of revision and reformulation is of the essence of the matter, making new questions out of old. Put in these terms, a system dies when the questions it seeks to answer are no longer asked; and only where the questions are the same can there be a genuine clash of answers. [@Sellars1948, 601]

Idealism became ignored.

To get a sense of how dramatically it was ignored, I look at some of the most prominent authors in this topic, and look at their later influence. I'd normally measure influence by citations, but citation practices have changed enough over time that this is a poor measure. Instead, for authors with distinctive enough names, I'll look at how frequently their names are used. 

These are the twenty authors with the most articles in this topic. (I'm individuating authors by name here, which is a bit sloppy since some people write under multiple names. But this gives a rough idea of who is represented in this topic.)

```{r idealism_authors}
idealism_authors <- relabeled_articles %>%
  filter(topic == 2) %>%
  select(citation, auth1, auth2, auth3) %>%
  pivot_longer(cols = starts_with("auth"), values_to = "auth", values_drop_na = TRUE) %>%
  group_by(auth) %>%
  summarise(n = n_distinct(citation)) %>%
  arrange(-n) %>%
  slice(1:20)

kable(idealism_authors, 
      col.names = c("Author", "Number of Articles"),
      caption = "Number of articles in these twelve journals by famous idealist authors."
      ) %>%
  kable_styling(full_width = F)
```

Now I look at how often some of these men (and they are all men) are mentioned in the twelve journals over the 138 years under discussion. (I'm leaving out the ones whose names are so common that there's not much to learn from seeing the frequency of that name appear; in later years it would usually mean someone else.) Note that I'm counting tokens here, so sometimes a name appears tens (or even hundreds) of times in an article. Also note that since there are more journals, and longer journals, in recent years, graphs like these tend to trend upwards. (Later I'll measure word usage as a proportion of all words in a year to account for this, but for now the raw numbers are more informative.)

```{r idealism_author_graphs}
word_year <- all_journals_tibble %>%
  inner_join(article_year_tibble, by = "document") %>%
  group_by(word, year) %>%
  summarise(count = sum(wordcount))
```

```{r idealism-author-facet, fig.height = 10.2, fig.cap = "Mentions of idealist philosophers in the journals", fig.alt = alt_text}
idealists <- c("bosanquet",
"schiller",
"shadworth",
"hodgson",
"hartshorne",
"bradley",
"dawes",
"hicks",
"creighton",
"muirhead",
"stout",
"balfour",
"wildon",
"urban",
"loewenberg")

idealist_data <- word_year %>%
  ungroup() %>% 
  filter(word %in% idealists) %>%
  complete(year = 1876:2013, word, fill = list(count = 0))

ttttt <- idealist_data %>%
  group_by(word) %>%
  summarise(count = max(count)) %>%
  mutate(year = 2013) %>%
  mutate(name = word)

ggplot(idealist_data, aes(x = year, y = count)) + 
  geom_point(size = 0.15, color = hcl(h = (jjj-1)*(360/cats)+15, l = 65, c = 100)) +
  facetstyle +
  facet_wrap(~word, ncol = 3, scales = "free") +
  labs(x = element_blank(), y = "Word count", title = "Idealists in the Journals") +
  geom_text(data = ttttt,
            mapping = aes(label = name),
            vjust = "inward", 
            hjust = "inward",
            fontface = "bold", 
            size = 3,
            colour = hcl(h = (jjj-1)*(360/cats)+15, l = 65, c = 100)) +
  theme(plot.title = element_text(size = rel(1), 
                                  face = "bold",
                                  colour = hcl(h = (jjj-1)*(360/cats)+15, l = 65, c = 100),
                                  margin = margin(0, 0, 5, 0)))

alt_text <- paste0(
  "Fifteen scatterplots showing the frequency of names of prominent idealists in the journals over time. The names are ",
  paste(idealists, collapse = ", "),
  ". All of them peak fairly early in the data set and then fall away rapidly. The only exceptions are ones where the name has some other use than referring to the famous idealist, as happens with Urban and Bradley."
)
```

It's just staggering how low some of these numbers are. And remember some of the numbers are inflated because there are other uses of these names. More of the uses of _Bradley_ in recent philosophy are about Ben Bradley than F. H. Bradley. The uses of _Hodgson_ are mostly not about Shadworth Hodgson, as you can see from how few uses of _Shadworth_ there are. It's surprising how few uses of _urban_ there are either as a name or in its regular usage, but that's another story.

The point is not that most people don't believe in idealism any more. Views go in and out of fashion. But two things about the distribution really surprise me. 

One is that some of these figures aren't even remembered as objects of scorn and derision an more. Maybe some people talk about Bosanquet as an example of how things were done in the Bad Old Days. But figures like Muirhead and Wildon Carr aren't even criticized.

The other is that the interest in idealism hasn't just dropped, as the interest in positivism and pragmatism has, but that it's gone so close to zero. Most other philosophies that had this many adherents in the past would still have some people keeping the spirit alive, and who would be complaining about the wickedness of the modern world. But I don't really see that in contemporary philosophy. Where is the Shadworth Hodgson Appreciation Consortium? 

It's easy to see how far things have fallen by looking at how few articles on idealism there have been in recent years. (This could be done by sorting the table of articles by year, but I'll list them here for completion.)

```{r recent-idealism}
recent_idealism <- char_art %>%
  arrange(-year) %>%
  slice(1:20)

datatable(select(recent_idealism, year, citation, gamma),           
          colnames = c("Year", "Article", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:2)),
                         pageLength = 10
                         ),
          caption = htmltools::tags$caption(paste0("Recent articles in idealism")#, style = "font-weight: bold"
          )
    )%>%
      formatSignif('gamma',4) %>%
      formatStyle(1:3,`text-align` = 'left')
```

The @Reinhardt2013 article is not really idealism - it's a paradigm of [ordinary language philosophy](#topic24). The next three articles are ones the model is not very confident in, and reading them over does not convince me the model was picking up a resurgence of idealism. The latest article that to me seems really about idealism is the Kekes contribution to a symposium on Rescher. After that, there are nearly twenty years of virtually nothing about a topic that was once the center of all philosophical work.

Just maybe there has been a little reversion to the mean recently. Later in the book I'll look at the [2019 articles in Philosophers' Imprint](#imprint-section). And there is an idealism article there. One article doesn't make a resurgence, but it's one more than most other years have seen.


<!--chapter:end:topic_comments/topic02.Rmd-->

```{r t03a}
jjj <- 3
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t03b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t03c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t03d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t03e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I was very tempted to call this idealist ethics. It does, at first glance, seem to just be the ethics papers of the idealism section. But this would be misleading for a few reasons. For one thing, the idealism topic includes a nontrivial amount of ethics. For another, it's sort of more like social and political philosophy than ethics. For another, this includes some philosophers who are very much not idealists, such as Margaret MacDonald.

MacDonald is an important figure in the story of these twelve journals. Her notes made up a large part of _Wittgenstein's Lectures, Cambridge, 1932-1935_ [@Wittgenstein2001]. She was a founder of, and the second editor of, _Analysis_. And she read a very important paper on natural rights to the Aristotelian Society in 1947 [@MacDonald1947]. MacDonald is usually read as a critic of the notion of natural rights, though she might not have agreed with that formulation.

But note that while MacDonald is no idealist, and her paper is in this topic, it isn't particularly firmly _in_ the topic. The model only gives it a probability of being in this topic of about 19.6 percent. So let's see where else it thinks MacDonald's paper might go. I'm going to present tables like the following for a number of papers in what follows. Although the model gives a nonzero chance to every paper being in each of the topics, the tables are going to be cut off at 2 percent. Sometimes that will make for a short table, especially when the model is quite confident in its assessment. Other times, there will be a much longer list of topics.

```{r macdonald-natural-rights}
individual_article("10.2307_4544427")
```

That's a sign that the model doesn't really know what to do with the paper. It's obviously a paper on normative philosophy, broadly construed. And MacDonald is sympathetic to both ordinary language philosophy and verificationism, so both of those topics turn up. But the model is really rather unsure which normative topic to place the paper in. That's fine; ninety topics is a lot, but it's still too few to capture all the nuances of philosophy.

The common thread in this topic is that although it's rather large, most of the articles that the model confidently places in it are by people who are very much not household names nowadays. 

<!--chapter:end:topic_comments/topic03.Rmd-->

```{r t04a}
jjj <- 4
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t04b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t04c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t04d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t04e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The faceted graph here is thrown off by a single data point from _Philosophical Quarterly_; it's the dot just above the _s_ in _Philosophical_. This is a downside of trying to graph all the data!

This is the only topic of the ninety that I think best gets a negative characterization; it's history articles on figures who don't fit into the other history categories. So it's what's left after you take out [pragmatists](#topic05), [ancient Greeks](#topic13), [early moderns](#topic21), [Heidegger and Husserl](#topic27), [social contract theorists](#topic31), [Kant](#topic32) and [Hume](#topic45). That leaves a lot, and it's a bit of a grab bag of figures who are left. 

But yet, as can be seen from the graphs above, they collectively make almost no impact on the journals from about 1970–2013. History of philosophy, at least as represented in these journals, got really narrow for several decades. I'll return [much later in the book](#imprint-section) to some evidence that things have changed since 2013, but it's a little shocking how small this category gets for the last forty-odd years I'm studying.

This is especially striking when considering that as well as some of the more surprising figures to turn up here, like Arnold Geulincx and James Marsh, we also see Nietzsche, James, Mill, and at least some historical articles on Wittgenstein. I'd have thought those three alone would have made more impact than the under 1 percent numbers we see for many recent decades. (And they are very much part of the resurgence I alluded to just above.)

So who _does_ fit into this mixture of a category? The following table gives us some clues. I've generated it by looking at the words that appeared more than fifty times in a single article somewhere in the topic. I then split out the names from that list, as well as the word _revolution_ which seemed interesting. And in each row I listed four things.

- First, how often within the topic the word appears fifty or more times in an article.
- Second, how often within the topic the word appears ten or more times in an article.
- Third, how often within the topic the word appears at all in an article.
- And fourth, how often within the whole study the word appears fifty or more times in an article.

The first three give us some sense of the spread of the term around a topic. A word that appears fifty or more times in an article is going to be more or less the subject of the article. A word that appears ten or more times is a focus of some sustained discussion in the article. And a word that appears at all is, at least, mentioned. And the last column is to give you a sense of how often this figure (or at least their name) shows up in the broader data set, so there's a better sense of what's distinctive about this topic.

```{r history-setup}
history_articles <- articles %>%
  filter(document %in% filter(relabeled_articles, topic == 4)$document)

history_word_list <- all_journals_tibble %>%
  filter(document %in% filter(relabeled_articles, topic == 4)$document)

history_mentions <- history_word_list %>%
  group_by(word) %>%
  summarise(n1 = n_distinct(document))

history_mentions_10 <- history_word_list %>%
  filter(wordcount > 9) %>%
  group_by(word) %>%
  summarise(n10 = n_distinct(document))

history_mentions_50 <- history_word_list %>%
    filter(wordcount > 49) %>%
    group_by(word) %>%
    summarise(n50 = n_distinct(document))

history_names <- history_mentions_50 %>%
  slice(5, 7, 8, 13, 18:21, 27:31, 35, 36, 38,
        43, 36, 49, 50, 51, 57, 58, 68, 70, 71, 75, 76, 80, 81,
        82, 83, 84, 87, 91:93, 96, 100:105, 113, 116, 119:121,
        123, 130, 139, 143, 148, 152, 154, 161, 165, 175, 176, 181, 
        187, 188, 189, 191, 192, 193, 194, 195, 204, 205, 208, 212, 214, 
        231, 234, 235, 236) %>%
  inner_join(history_mentions_10, by = "word") %>%
  inner_join(history_mentions, by = "word") 

all_mention_50 <- all_journals_tibble %>%
  filter(word %in% history_names$word) %>%
  filter(wordcount > 49) %>%
  group_by(word) %>%
  summarise(a50 = n_distinct(document))

history_names <- history_names %>%
  inner_join(all_mention_50, by = "word")
```

```{r history-table}
kable(history_names, 
      col.names = c("Name", "50 Usages (in Topic)", "10 Usages", "1 Usage", "50 Usages (Anywhere)"), 
      caption = "Prominent names in the topic other history.",
      align=c("l", "c", "c", "c", "c"))
```

Nietzsche is the big standout here, being a focus of seventeen of the articles. (And that's not because he is widely discussed elsewhere.) I really would have expected there to have been more Nietzsche in the journals, but he still makes up a big part of this topic.

As can be seen from this table, and from the characteristic articles above, Mill gets a lot of attention here, even though he also gets discussed a lot elsewhere. But note how little attention Harriet Taylor gets. The uses of _Taylor_ cover a fairly wide range of people, so _Harriet_ is the more useful clue. And the only time she is mentioned more than in passing, at least in this topic, is in L. W. Sumner's "[More Light on the Later Mill](https://philpapers.org/rec/SUMMLO)" [-@Sumner1974]. It does say that the word 'Harriet' turns up in five other articles, but it turns up a total of six times across those five. Until Sumner, she is bascially erased from the historical discussion.

Otherwise there is a pleasingly eclectic grab-bag of philosophers here.

- [James Marsh](https://philpapers.org/rec/NICJMA) was an early nineteenth-century transcendentalist who tried to popularize Coleridge among the New England transcendentalists. [@nicolson1925a]
- Marsh's attempts didn't really succeed, but there remained some philosophical interest in Coleridge. He is the focus of two _Philosophical Review_ papers, one in [1919](https://philpapers.org/rec/WILTDO-5) and the other in [1936](https://philpapers.org/rec/MOSCAB). [@mossner1936a; @wilde1919a]
- [Henry Thomas Buckle](https://philpapers.org/rec/BENBAT) was an early exponent of the view that history has laws, and that differences in either the character of great men, or of nations, make less difference to historical development than facts about geography. This has all sorts of echoes in work since, from Marx to [Walter Schiedel](https://press.princeton.edu/books/hardcover/9780691172187/escape-from-rome) [-@Scheidel2019], but Buckle's own work has fallen well out of fashion. [@benn1881a]
- [Antoine Augustin Cournot](https://philpapers.org/rec/MOOTPO-5) anticipated many of the key ideas in twentieth-century economics, including the notion of equilibrium, and the idea of supply and demand graphs, but it's tricky to say just how causally significant he ended up being. His work on duopoly and oligopoly is still the foundation for how we think about those matters today. And, perhaps more relevant to philosophy, he made a number of contributions to the study of probability. [@moore1934a]
- [Denis Diderot](https://philpapers.org/rec/BECTDO-2) could hardly be summarzsed in the little capsules I'm doing here. It is a great mystery to me how he hasn't had more direct influence on the philosophy journals. [@becker1915a]
- [Jonathan Edwards](https://philpapers.org/rec/EWOJE) is one of the most important US theologians in history. (And grandfather of Aaron Burr.) But most of the philosophical attention to him comes during the idealist period, and is interested in how his work relates to idealism. [@e1904a]
- [Antonio Rosmini](https://philpapers.org/rec/BARPII) and [Vincenzo Gioberti](https://philpapers.org/rec/BARPII) are two of the most important Italian philosophers and theologians of the nineteenth-century. They both played significant roles in the revolutions of 1848. [@barzellotti1878a]
- [José Vasconcelos](https://philpapers.org/rec/ROMBIM) was the most important philosophical figure in the Mexican Revolution and an important figure in the Revolution itself. Given everything that was happening around him, it is mildly surprising that he survived until 1920. But he did, and became Rector of UNAM (Universidad Nacional Autónoma de México) and Secretary of Education in Obregón's government. [@romanell1960a]

There is one last article in this topic that needs some comment. It is the one and only article of the 566 to get onto the highly cited list. And it is cited not so much because of its content, but because of its author: Charles Darwin.

The article itself is like nothing else I've seen in these twelve journals [@Darwin1877]. Darwin was, as is well known, an incredibly astute biological observer. So when his first son was born, he had a new subject to observe. And he dispassionately observed his development, and took meticulous notes. Then, some years later, he published the notebooks—in _Mind_. There is very little attempt at developing much of a theory here. Indeed, the text doesn't have anything like the form of what we'd think of as an article. It really is just a lightly edited version of the notes Darwin took while observing his child's development. I've seen it claimed that this is a significant document in the history of developmental psychology. And while I'm no expert on the history of developmental psychology, I find this very hard to believe. Again, it is really just a single case study, and it's not clear what could be learned in principle about developmental psychology from one case. But it's a fascinating insight into what kind of parent Darwin was.


<!--chapter:end:topic_comments/topic04.Rmd-->

```{r t05a}
jjj <- 05
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t05b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t05c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t05d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t05e}
#Burp
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

One of the disappointing things about this model was how it handled pragmatism. Most model runs ended up with a very nice pragmatism topic, that gave a very clear sense of the rise and fall of pragmatism in the different journals. This model did not. James primarily ended up in [the previous topic](#Topic04), Pierce was all over the place, but primarily in [universals and particulars](#Topic14), and Dewey is here. There isn't a single clear look at where pragmatism goes.

This topic also involves as much looking back at Dewey as it does original work. Of course in philosophy that's sometimes a distinction without a difference; plenty of Kantians do exegetical work that is continuous with their work defending ethical conclusions. But still, it's not great that the closest we get to a pragmatism topic is one that feels as much like a history of pragmatism topic as it does one that reflects pragmatism being done the "first time around".

Relatedly, this model doesn't give a clear test of the claims that Joel Katzav and Krist Vaesen make in their paper "[On the Emergence of American Analytic Philosophy](https://www.tandfonline.com/doi/full/10.1080/09608788.2016.1261794)". Here's the abstract of their paper, which summarises it as well as I could do.

> This paper is concerned with the reasons for the emergence and dominance of analytic philosophy in America. It closely examines the contents of, and changing editors at, The Philosophical Review, and provides a perspective on the contents of other leading philosophy journals. It suggests that analytic philosophy emerged prior to the 1950s in an environment characterized by a rich diversity of approaches to philosophy and that it came to dominate American philosophy at least in part due to its effective promotion by The Philosophical Review’s editors. Our picture of mid-twentieth-century American philosophy is different from existing ones, including those according to which the prominence of analytic philosophy in America was basically a matter of the natural affinity between American philosophy and analytic philosophy and those according to which the political climate at the time was hostile towards non-analytic approaches. Furthermore, our reconstruction suggests a new perspective on the nature of 1950s analytic philosophy. [@Katsav2017, 772]

The short version is that there was a change in management at the Philosophial Review in the late 1940s, and after that, what had been a flourishing diversity of approaches was narrowed down, and the Review only published papers that met with the approval of the analytically inclined editors. But the short version is a bit of a bad simplification of their view. For one thing, they note that one kind of pragmatism, the kind of scientific pragmatism associated with Dewey, continued well after the analytic takeover. Relatedly, a fall away in this topic isn’t seen at _Philosophical Review_  until around 1948. For another thing, they note that through the early 1950s, some papers that were more representative of the old style of Review articles were getting through. This wasn’t the hard crackdown on some subjects that Gilbert Ryle executed at _Mind_. And for another thing, “analytic” is possibly not quite the right term for what takes over in the 1950s. Max Black is unambiguously an analytic philosopher, though a lot of the other folks who play signature roles, especially Norman Malcolm, have somewhat more difficult relationships to what would now be called analytic philosophy.

But the most relevant point is that if they’re right, there should be a topic that is a big part of _Philosophical Review_  up until the late 1940s, then falls away very rapidly. And there should be, but in this model there isn’t. That’s not because Katsav and Vaesen are wrong though; it’s an idiosyncrasy of this particular model. It was very common when I was building different models to see topics that looked exactly like what would be expected if Katsav and Vaesen were correct. Rather than walking through a whole new model though, I’m going to look at some of the underlying data that supports their view.

As a methodological note, I found the words that I’m about to focus on by building a small LDA model for just _Mind_, _Philosophical Review_, and the _Journal of Philosophy_ for the midcentury years and looking for topics where _Philosophical Review_  stopped publishing around the time Katsav and Vaesen focus on. And then I looked at the keywords for those topics. That’s to say, the words I’m about to talk about were not selected at random. But the data about them does, I think, show that something distinctive happened at the Review around the middle of the century.

What I’m going to do is show how often a bunch of words appeared in those three journals (i.e., _Mind_, _Philosophical Review_, and _Journal of Philosophy_), between 1930 and 1970. The words were chosen to give a sense of what kinds of things the _Review_ published articles about before 1950 but didn’t publish about after 1950. This whole book has been based around using very fancy models to compute that kind of thing from the word frequencies. But sometimes it helps to just look at the word _frequencies_ (or, in this case, the word counts) themselves.

I’ve put the words I’m looking at into three categories:

```{r mid-century-setup-a}
mid_articles <- articles %>%
  filter(journal == "Mind" | journal == "Philosophical Review" | journal == "Journal of Philosophy") %>%
  filter(year > 1929, year < 1970)

mid_word_list <- all_journals_tibble %>%
  filter(document %in% mid_articles$document)

topic_1 <- c("mead",
"behavioral", 
"organism", 
"brain", 
"environment", 
"psychologists", 
"interaction", 
"selves", 
"stimulus", 
"conscious", 
"physiological", 
"immediacy", 
"consciousness",
"awareness", 
"unconscious"
)

topic_2 <- c("natura", 
             "res", 
             "spinoza", 
             "berkeley", 
             "hume", 
             "agency", 
             "cogito", 
             "esse", 
             "descartes", 
             "treatise", 
             "intellect", 
             "impressions", 
             "ideas", 
             "immanent", 
             "idea")

topic_3 <- c("philosophic", 
             "speculative", 
             "categories", 
             "idealism", 
             "dialectic", 
             "metaphysics", 
             "naturalism", 
             "synthesis", 
             "dewey", 
             "traits", 
             "ontology", 
             "philosopher", 
             "philosophies", 
             "contemporary", 
             "dialectical")

topic_6 <- c("comic", 
             "sovereign", 
             "democracy", 
             "comedy", 
             "cultures", 
             "democratic", 
             "political", 
             "government", 
             "international", 
             "liberty", 
             "culture",
             "national", 
             "society", 
             "peace", 
             "economic")

topic_12 <- c("kierkegaard", 
              "heidegger", 
              "existentialism", 
              "sophist", 
              "sartre", 
              "logos", 
              "lotze", 
              "christianity", 
              "plato", 
              "republic", 
              "worship", 
              "santayana", 
              "christian", 
              "anselm", 
              "parmenides")

mid_article_year_journal <- mid_articles %>%
  select(document, journal, year)

mid_topics <- c(topic_1, topic_2, topic_3, topic_6, topic_12)

mid_words <- all_journals_tibble %>%
  filter(word %in% mid_topics) %>%
  inner_join(mid_article_year_journal, by = "document") %>%
  group_by(journal, year, word) %>%
  summarise(n = sum(wordcount))

pr_mind_words <- c(
  "consciousness",
  "physiological",
  "conscious",
  "stimulus",
  "selves",
  "interaction",
  "environment",
  "immediacy",
  "organism"
)

pr_idealism_words <- c(
  "dialectical",
  "contemporary",
  "philosopher",
  "synthesis",
  "naturalism",
  "metaphysics",
  "idealism",
  "categories",
  "speculative"
)

pr_pol_words <- c(
  "economic",
  "society",
  "national",
  "culture",
  "cultures",
  "democracy"
)
```

I've put the words I'm looking at into three categories.

Philosophy of Mind Words
:    `r pr_mind_words`

Speculative Philosophy Words
:    `r pr_idealism_words`

Political Philosophy Words
:    `r pr_pol_words`

None of the three titles I've given here are exactly apt. (This is sort of the point; the work from before 1950 doesn't even naturally fall into the categories that are now used.) Philosophy of mind includes a mix of empirical psychology and idealist-inflected reflections on the nature of consciousness. What I've called "speculative philosophy" is a bit of a grab-bag of things that the analytic philosophers didn't like. And what I've called "political philosophy" is as much about social theory as politics as we'd now understand it. But as long as we keep track of what's being measured, the terms provide a helpful enough shorthand.

I'll start by looking at the philosophy of mind words. I'll first graph out how frequently each of these nine words appears, and then look at what happens when they're summed up.

```{r phil-review-mind-words, fig.cap = "Philosophy of mind words in the three big journals, 1930–1970.", fig.alt = alt_text}
t <- mid_words %>%
  filter(word %in% pr_mind_words)

facet_labels <- t %>%
  ungroup() %>%
  mutate(year = 1968, n = 151, journal = "Mind")

ggplot(t, aes(x = year, y = n, color = journal, group = journal)) +
  facetstyle +
  geom_point(size = 0.4) +
  geom_smooth(size = 0.2, method = "loess", formula = "y ~ x", se = F) +
  facet_wrap(~word, ncol = 3) +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        panel.grid.major.x = element_line(color = "grey85", size = 0.07)) +
  scale_x_continuous(breaks = c(1950)) +
  labs(x = element_blank(), y = "Occurrences of word") +
  geom_text(data = facet_labels,
            mapping = aes(label = word),
            vjust = "outward", 
            hjust = "inward",
            size = 3,
            colour = "grey40")

alt_text <- paste0(
  "Nine scatterplots showing the number of occurrences of the words ",
  paste(pr_mind_words, collapse = ", "),
  " in the three big journals. The x-axis is the year, from 1930–1970. The y-axis is the frequency, from 0 to 150.",
  " The general trend is flat is downwards for Journal of Philosophy. For Philosophical Review, the trend is flat through 1950, then down. For Mind the trend is flat at a low level; the level the other two journals end at."
)
```

```{r phil-review-mind-words-summary, fig.height = 5, fig.cap = "Sum of philosophy of mind words in the three big journals, 1930–1970.", fig.alt = alt_text}
tt <- t %>%
  group_by(journal, year) %>%
  summarise(nn = sum(n))

ggplot(tt, aes(x = year, y = nn, color = journal)) + 
  spaghettistyle +
  geom_point(size = 0.4) +
  geom_smooth(size = 0.2, se = F, method = "loess", formula = "y ~ x") +
  labs(x = element_blank(), y = "Word Count", title = "Philosophy of Mind Words") +
  theme(legend.position = "right",
        legend.title = element_blank(),
        panel.grid.major.x = element_line(color = "grey85", size = 0.07)) +
  scale_x_continuous(breaks = c(1950)) +
  labs(x = element_blank(), y = "Occurrences of words")

alt_text <- "A single scatterplot showing the sums of the values in the nine scatterplots in the previous graph. The trends mentioned there are more pronounced when the sum is shown."
```

There are three things happening here, which combine to form the distinctive graph at the end.

One is that _Philosophical Review_, like a lot of other philosophy journals, started out as a cross between what we'd now think of as a philosophy journal, and what we'd now think of as a psychology journal. Most such journals had split into one of the other by the 1930s, but the _Review_ kept publishing psychology papers for quite a while. But they fade away over the period looked at here (i.e., 1930–1970).

A second is that as idealism drops out of the conversation, a particular kind of writing about consciousness goes away with it.

And a third is that behaviorism happens, and this really puts a damper on discussions of minds.

The third trend gets reversed eventually, but the second trend doesn't. Post-behaviorist writing about consciousness on the whole does not feel a lot like prebehaviorist writing. (Though note that the model does think of "[What Is It Like to Be a Bat](https://www.jstor.org/stable/2183914?seq=1)" as a fairly old-fashioned paper.)

But the result of these three is a dramatic falling away in the late 1940s and early 1950s for these nine words.

Now for what I've called speculative philosophy. I'm going to leave the _Journal of Philosophy_ off the first set of graphs because some of the words were used so often in some of the years that it threw off the scale. I'll come back to the _Journal_ when I sum these graphs together. I'm also leaving off one data point for the Review: 'philosopher' which gets used 176 times in 1947. The loess curve that is shown does, however, take that hidden point into account.

```{r phil-review-idealism-words, fig.cap = "Speculative philosophy words in (two of) the three big journals, 1930–1970.", fig.alt = alt_text}
t <- mid_words %>%
  filter(word %in% pr_idealism_words) 

facet_labels <- t %>%
  ungroup() %>%
  mutate(year = 1968, n = 129, journal = "Philosophical Review")

ggplot(t %>%
         filter(journal != "Journal of Philosophy"), 
       aes(x = year, y = n, color = journal, group = journal)) +
  facetstyle +
  geom_point(size = 0.4) +
  geom_smooth(size = 0.2, method = "loess", formula = "y ~ x", se = F) +
  facet_wrap(~word, ncol = 3) +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        panel.grid.major.x = element_line(color = "grey85", size = 0.07)) +
  scale_x_continuous(breaks = c(1950)) +
  scale_y_continuous(breaks = c(100, 200)) +
  coord_cartesian(ylim = c(0, 130)) +
  labs(x = element_blank(), y = "Occurrences of word") +
  geom_text(data = facet_labels,
            mapping = aes(label = word),
            vjust = "inward", 
            hjust = "inward",
            size = 3,
            colour = "grey40")

alt_text <- paste0(
  "Nine scatterplots showing the number of occurrences of the words ",
  paste(pr_idealism_words, collapse = ", "),
  " in Mind and Philosophical Review. The x-axis is the year, from 1930–1970. The y-axis is the frequency, from 0 to 150.",
  " These words do not appear particularly often in Mind, but they occur a lot in Philosophical Review until a  falling away around 1950."
)
```

Now I'll show what that looks like after summing the nine words, and adding the _Journal of Philosophy_ back into the picture.

```{r phil-review-idealism-words-summary, fig.height = 5, fig.cap = "Sum of speculative philosophy words in the three big journals, 1930–1970.", fig.alt = alt_text}
tt <- t %>%
  group_by(journal, year) %>%
  summarise(nn = sum(n))

ggplot(tt, aes(x = year, y = nn, color = journal)) + 
  spaghettistyle +
  geom_point(size = 0.4) +
  geom_smooth(size = 0.2, se = F, method = "loess", formula = "y ~ x") +
  labs(x = element_blank(), y = "Word Count", title = "Speculative Philosophy Words") +
  theme(legend.position = "right",
        legend.title = element_blank(),
        panel.grid.major.x = element_line(color = "grey85", size = 0.07)) +
  scale_x_continuous(breaks = c(1950)) +
  labs(x = element_blank(), y = "Occurrences of words")

alt_text <- "A single scatterplot showing the sums of the values in the nine scatterplots in the previous graph. These words do not get used very often in _Mind_. In Philosophical Review, they are used roughly three times as often until 1950, when they slowly decline back to Mind's level. The same is true through 1950 for Journal of Philosophy, but theyn they actually increase in frequency through the 1950s, before declining rapidly in the 1960s."
```

Again, there is a falling off after 1949, but it's not completely sudden. Finally, I'll look at the political philosophy words.

```{r phil-review-polphil-words, fig.height = 6.2, fig.cap = "Political philosophy words in the three big journals, 1930–1970.", fig.alt = alt_text}
t <- mid_words %>%
  filter(word %in% pr_pol_words) 

facet_labels <- t %>%
  ungroup() %>%
  mutate(year = 1968, n = 129, journal = "Philosophical Review")

ggplot(t, 
       aes(x = year, y = n, color = journal, group = journal)) +
  facetstyle +
  geom_point(size = 0.4) +
  geom_smooth(size = 0.2, method = "loess", formula = "y ~ x", se = F) +
  facet_wrap(~word, ncol = 3) +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        panel.grid.major.x = element_line(color = "grey85", size = 0.07)) +
  scale_x_continuous(breaks = c(1950)) +
  scale_y_continuous(breaks = c(100, 200)) +
  coord_cartesian(ylim = c(0, 130)) +
  labs(x = element_blank(), y = "Occurrences of word") +
  geom_text(data = facet_labels,
            mapping = aes(label = word),
            vjust = "inward", 
            hjust = "inward",
            size = 3,
            colour = "grey40")

alt_text <- paste0(
  "Six scatterplots showing the number of occurrences of the words ",
  paste(pr_pol_words, collapse = ", "),
  " in the three big journals. The x-axis is the year, from 1930–1970. The y-axis is the frequency, from 0 to 100.",
  " The words culture and society fall away dramatically in frequency in Philosophical Review after about 1952, and after the late 1950s in Journal of Philosophy."
)
```

```{r phil-review-polphil-words-summary, fig.height = 5, fig.cap = "Sum of political philosophy words in the three big journals, 1930–1970.", fig.alt = alt_text}
tt <- t %>%
  group_by(journal, year) %>%
  summarise(nn = sum(n))

ggplot(tt, aes(x = year, y = nn, color = journal)) + 
  spaghettistyle +
  geom_point(size = 0.4) +
  geom_smooth(size = 0.2, se = F, method = "loess", formula = "y ~ x") +
  labs(x = element_blank(), y = "Word Count", title = "Political Philosophy Words") +
  theme(legend.position = "right",
        legend.title = element_blank(),
        panel.grid.major.x = element_line(color = "grey85", size = 0.07)) +
  scale_x_continuous(breaks = c(1950)) +
  labs(x = element_blank(), y = "Occurrences of words")

alt_text <- "A single scatterplot showing the sums of the values in the six scatterplots in the previous graph. A loess curve through the graph shows that they start declining in frequency in Philosophical Review in the late 1940s, and in Journal of Philosophy in the mid 1950s."

```

Again, there is a fairly big drop, though here it seems like the break comes in 1952. And note from the word-by-word graph that it's the parts that seem most like political philosophy in the current sense that hold up for the longest. _Philosophical Review_ in the 1950s talked about democracy a bit, but it didn't talk about culture.

I'll sum all these together to get an overall picture. The next graph shows how often these twenty-four words appear in each of these journals in each year.

```{r make-phil-rev-look-bad, fig.height = 5, fig.cap = "Sum of word usage for twenty-four distinctive words in the three big journals, 1930–1970.", fig.alt = alt_text}
tt <- mid_words %>%
  filter(word %in% c(pr_pol_words, pr_idealism_words, pr_mind_words)) %>%
  group_by(journal, year) %>%
  summarise(nn = sum(n))

ggplot(tt, aes(x = year, y = nn, color = journal)) + 
  spaghettistyle +
  geom_point(size = 0.4) +
  geom_smooth(size = 0.2, se = F, method = "loess", formula = "y ~ x") +
  labs(x = element_blank(), y = "Word Count", title = "How Philosophical Review Changed") +
  theme(legend.position = "right",
        legend.title = element_blank(),
        panel.grid.major.x = element_line(color = "grey85", size = 0.07)) +
  scale_x_continuous(breaks = c(1950)) +
  labs(x = element_blank(), y = "Occurrences of words")

alt_text <- "A single scatterplot summing the frequency of the 24 words discussed so far. Their annual usage in Mind is reasonably stable, around 200 per year. In Philosophical Review they are used around 600 times per year through 1950, then are used even less often than in _Mind_. In Journal of Philosophy they are used around 700 to 800 times a year until around 1960, then drop to 300 to 400 usages per year."
```

By that measure, the 1950s at Philosophical Review do look quite different to the 1940s. This difference is why, I think, most models I looked at had at least one topic that disappeared from _Philosophical Review_ right around 1950. For whatever reason this model didn't have such a topic; this topic on Dewey is the closest, but I wanted to show  why this was a common occurrence.

That said, the big change in Philosophical Review could be summarized by saying that it changed from being like the _Journal of Philosophy_ to being like _Mind_. It wasn't like they did something completely unprecedented, or were well out in front of the international trends.

And the changes that _Philosophical Review_ made were eventually replicated at the _Journal of Philosophy_. Not just that, the changes preceded the glory years of the Journal in the 1970s. I don't really think that the changes the Review made around 1950 were particularly bad things. But the evidence from word counts does point towards there really being a change.

It's not connected to Dewey, or the _Philosophical Review_, but I wanted to show  how this kind of evidence from word counts can reveal a change in the journal. So I ran exactly the same kind of query as before, but with words connected to [early modern philosophy](#topic21). 

```{r setup-phil-review-history-words}
history_words <- c(
  "idea",
  "ideas",
  "immanent",
  "intellect",
  "treatise",
  "esse",
  "hume",
  "berkeley",
  "spinoza",
  "res",
  "natura",
  "descartes"
)
```

Early Modern Words
:    `r history_words`

Note the dramatic fall in the late 1940s, at the same time Ryle becomes editor of _Mind_. I will have more to say about this in what follows.

```{r phil-review-history-words, fig.height = 10.2, fig.cap = "Early modern words in the three big journals, 1930–1970.", fig.alt = alt_text}
t <- mid_words %>%
  filter(word %in% history_words) 

facet_labels <- t %>%
  ungroup() %>%
  mutate(year = 1951, n = 325, journal = "Philosophical Review")

ggplot(t, 
       aes(x = year, y = n, color = journal, group = journal)) +
  facetstyle +
  geom_point(size = 0.4) +
  geom_smooth(size = 0.2, method = "loess", formula = "y ~ x", se = F) +
  facet_wrap(~word, ncol = 3) +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        panel.grid.major.x = element_line(color = "grey85", size = 0.07)) +
  scale_x_continuous(breaks = c(1950)) +
  scale_y_continuous(breaks = c(100, 200)) +
  labs(x = element_blank(), y = "Occurrences of word") +
  geom_text(data = facet_labels,
            mapping = aes(label = word),
            vjust = "inward", 
            hjust = "outward",
            size = 3,
            colour = "grey40")

alt_text <- paste0(
  "Twelve scatterplots showing the number of occurrences of the words ",
  paste(history_words, collapse = ", "),
  " in the three big journals. The x-axis is the year, from 1930–1970. The y-axis is the frequency, from 0 to 250.",
  " The main significance is seeing how all these values in Mind fall to very low levels after Ryle becomes editor after World War II.."
)
```

```{r phil-review-history-words-summary, fig.height = 5, fig.cap = "Sum of early modern words in the three big journals, 1930–1970.", fig.alt = alt_text}
tt <- t %>%
  group_by(journal, year) %>%
  summarise(nn = sum(n))

ggplot(tt, aes(x = year, y = nn, color = journal)) + 
  spaghettistyle +
  geom_point(size = 0.4) +
  geom_smooth(size = 0.2, se = F, method = "loess", formula = "y ~ x") +
  labs(x = element_blank(), y = "Word Count", title = "Early Modern Words") +
  theme(legend.position = "right",
        legend.title = element_blank(),
        panel.grid.major.x = element_line(color = "grey85", size = 0.07)) +
  scale_x_continuous(breaks = c(1950)) +
  labs(x = element_blank(), y = "Occurrences of words")

alt_text <- "A single scatterplot showing the sum of the previous 12. The fall in how often these words appear in Mind is very dramatic, from averaging about 400 appearances per year to about 150."
```

Is it possible this kind of crude counting could detect a change in journal policy? I think that's possible I think you have the answer right there. It's a bit interesting that there is a fall at the _Journal_ and the _Review_ as well, but that's a story for [another topic](#Topic21).

<!--chapter:end:topic_comments/topic05.Rmd-->

```{r t06a}
jjj <- 6
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t06b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t06c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t06d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t06e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

At first when I saw this topic come up, I assumed that it would be part of the influence of positivism. If positivism is going to work, a lot of terms need to be defined along with a theory of definitions. And the timeline, especially on the single graph, checks out for that explanation. But I’m not sure that’s all of what is going on. When looking through the notable articles that are clearly about definitions, such as the Dubs or Reid articles mentioned above, they don’t seem super positivist. They don’t seem openly hostile to positivism, but they don’t motivate the search for a theory of definitions with an account of their role in positivist philosophy.

That said, they also don’t come up with things that would pass muster by contemporary standards as theories. I’m going to quote Dubs’s conclusion at length because I can’t really summarize it otherwise:

> The foregoing summarizes the important features of what I find necessary for definition. To recapitulate: definitions are of various types, depending upon what function they are to fulfil. Dictionary definitions attempt to explain a meaning to a person who is ignorant of it. The three requirements for such a definition are commensurateness, understandability, and usually) conceptuality. Scientific definitions attempt to enable the exact identification of cases of a term in experience or thought. There are two requirements: commensurateness and definition only in terms previously defined. Such definitions are conceptual or non-conceptual. Conceptual definitions, in any developed science, form an elaborate hierarchy, the creation of which is sometimes an extremely difficult task. Non-conceptual definitions may be causal definitions, denotative definitions, or semi-denotative definitions. In a few sciences, essential definitions have been achieved; most definitions are merely nominal. I hope that the foregoing account of definition fits the varied facts of scientific and logical usage and constitutes an intelligible and consistent account of definition. (577)

I mean, sure definitions are conceptual or nonconceptual; that doesn’t seem like a stunning conclusion. And more generally, what we get here feels more like a shopping list than a sharp account. But it’s still something, and Dubs does make some good points along the way about the mischief that philosophers can do with badly drawn definitions.

One other thing to note about this topic is that it is very American. The articles here are primarily by US authors in US journals. And for all my complaints about Dubs, there is a kind of professionalism to the American writers that isn’t always shared by their British contemporaries. I suspect this professionalism is not always admired by those who look to philosophy for deep insight. But there is a sense here, in a way that there isn’t in the earlier topics, that we’re getting early versions of philosophy as it is now done.

This topic is helpful for seeing the difference between the raw counts and the weighted count. The weighted count is much higher, as seen above. This is because every time someone introduces a definition in their paper (as such), the model gets a bit excited and thinks that it is a paper on definitions. It quickly figures out that this probably isn’t the case, but it can leave a residual probability. Those residuals add up, and eventually the topic is the ninth highest by weight out of the ninety. And that’s why even though there are very few papers in these journals that are in any sense on definitions since the 1980s, the graph doesn’t go to zero. All those times the model thinks it is 2 or 3 percent likely that a paper is about definitions add up, and the result is a graph like this one.

<!--chapter:end:topic_comments/topic06.Rmd-->

```{r t07a}
jjj <- 7
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t07b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t07c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t07d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t07e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Note that the scale in the second graph is thrown off by the very high numbers from the early years of _Analysis_. This is, as seen at the top of the page, one of the larger topics in the study, despite the graphs looking quite low.

This overlaps a lot with [deduction](#topic17). Indeed, it is possibly best to simply regard them as part of the same topic. Fortunately, to do that one doesn’t have to do much more than change the scales on the graphs, because their distribution over time, and over journals, is pretty similar.

This topic gets _implication_ rather than _validity_, which is moved to [deduction](#topic17). Therefore, the logical works in it are a touch earlier. But it also gets _proposition_, so it picks up a bit more weight from more contemporary work on propositions. Here, for instance, is the table for Jeff King’s paper “Designating Propositions.”


```{r king-designating}
individual_article("10.2307_3182547")
```

It mostly goes with [sense and reference](#topic64). That topic, despite its Fregean name, appears much later in our story. But note that the model can't quite shake the idea that it should be placed in this topic. It's this kind of assessment that keeps the graphs for this topic from really collapsing.

<!--chapter:end:topic_comments/topic07.Rmd-->

```{r t08a}
jjj <- 8
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t08b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t08c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t08d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t08e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The striking thing about this topic is how confident the model is that articles from both ends of the timeline belong together. 

```{r bosanquet-aesthetics}
individual_article("10.2307_4543553")
```

```{r dom-aesthetics}
individual_article("10.2307_23012980")
```

It knows that Bosanquet is an idealist and McIver Lopes is not. But it also knows that despite that, they have a subject matter in common. This isn't something you see for many other topics. The same thing happens with the [temporal paradoxes](#topic19) and, to a lesser extent, with [colour](#topic40). But it is particularly marked here.

<!--chapter:end:topic_comments/topic08.Rmd-->

```{r t09a}
jjj <- 9
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t09b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t09c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R') # Burp
```

```{r t09d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic # Burp
temp_dt
```

```{r t09e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I really didn’t expect this. I sort of thought that physicalism, in the way it was discussed by contemporary writers, was quite distinct from what early twentieth-century writers were talking about. And I thought that even if that weren’t true, the terminological changes—using language like “supervenience” and “physicalism” that I thought was not common in the early twentieth century—would have been enough to get the model to split up Stoljar and Montero from Stebbing and Trumbull Ladd. But the model thinks that they fit together, and I can sort of see its point of view.
The mind-body problem is one of the oldest and most venerable problems around. Actually, since I’m discussing Stoljar, I should at this point acknowledge [his good point](http://dailynous.com/2017/11/07/philosophy-makes-progress-guest-post-daniel-stoljar/) that it is a class of problems not a problem, and missing this point leads one to seriously misstate the history of work in this field [@Stoljar2017]. It was a big deal in nineteenth-century philosophy, it is a big deal in twenty-first-century philosophy, and, apparently, it was an even bigger deal in philosophy in the 1920s and 1930s.

The only thing that disappoints me about the model’s result here is that it put almost none of Samuel Alexander’s work in this topic. I’d have thought the preeminent emergentist of the early twentieth century would play an important role in debates about physicalism, but the model disagreed, and I’m not entirely sure why it did so.

<!--chapter:end:topic_comments/topic09.Rmd-->

```{r t10a}
jjj <- 10
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t10b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t10c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t10d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t10e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is a topic that I find quite interesting but which has largely vanished from contemporary philosophy. Arthur Hatto's paper on revolutions [-@Hatto1949] was one of the papers I was most pleased about finding from doing this project, but it's something it's hard to imagine getting published in the last fifty years.

The topic is largely philosophy of history, with quite a bit of attention paid to Collingwood and Toynbee. But there is also work at the intersection between philosophy and sociology and, more generally, work discussing culture. There is a smattering of work here that would be called history of philosophy, such as the paper seen above on Vico, but there is much more work about history as a discipline than history of philosophy.

There are also a handful of papers on the philosophical significance of various political figures, such as Jefferson, Lenin, or Roosevelt. Analytical political philosophy has something of an aversion to dealing with figures who were politically influential in their own time. Even when studying such figures, there is tendency to steer away from the works that actually made them influential. There are ongoing attempts to make Anglophone political philosophers take Ghandi and Martin Luther King Jr. more seriously. If those attempts succeed, they will not just increase the racial diversity of who gets studied but also introduce figures who were seriously influential in their own lifetime.

<!--chapter:end:topic_comments/topic10.Rmd-->

```{r t11a}
jjj <- 11
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t11b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t11c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t11d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t11e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is largely traditional philosophy of religion, with the exception of work on the ontological argument, which is [topic 28](#topic28). It isn't a huge part of the journals over the years. There is a bit of interest in idealism and Christianity in the later parts of the Idealism boom. Several of the papers that make up those dots around 1920 are about Royce or Bosanquet, for example. Then there is more sustained attention to something like analytic philosophy of religion in the middle of the century, but that slowly peters out. Partially what's happening here is that the journals are losing interest in philosophy of religion, and what little interest they have is taken up with the ontological argument.

I don't have a way of sorting articles by an author's gender that I'm happy with, so this is very impressionistic, but this feels like a very male set of authors. Just eyeballing the data, I'm not sure there are any articles in this topic by women in any of the US journals. There are a few recent articles by women in _Proceedings of the Aristotelian Society_, and some in _Philosophical Quarterly_, but after that it gets very thin.

<!--chapter:end:topic_comments/topic11.Rmd-->

```{r t12a}
jjj <- 12
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t12b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t12c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t12d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t12e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is a strange topic. It peaks in the early part of the timeline, and a lot of the articles in it feel very continuous with the other early topics. But unlike [idealism](#topic02) and the topics connected to pragmatism, it keeps picking up articles to the present day. I think this is something of a coincidence; the fact that there is more lexical overlap between philosophy of mind circa 1912 and philosophy of mind circa 2012 than there is in, say, ethics or metaphysics across the one hundred years doesn't show that there is much overlap in content.

The top characteristic article here, G. W. Cunningham's "[Self-Consciousness and Consciousness of Self](https://philpapers.org/rec/CUNSAC)" [-@Cunningham1911] is useful for thinking about what goes into this topic. On the one hand, the example Cunningham starts with, of a young boy doing public speaking for the first time, is the kind of thing found in more contemporary work. And the distinction Cunningham draws, between the way the boy feels about himself in contrast to the wider world and the sum total of the boy's stream of consciousness, also feels contemporary enough. On the other hand, Cunningham explicitly rejects the equation of these contrastive feelings (such as embarrassment at being the center of attention) with self-consciousness. That's consciousness of self for him. And within a few pages we're in a discussion of whether the Absolute (capitals very much in original) could have a consciousness of self if there is no Other to contrast with. And this feels like it could just be an idealism paper.

Still, it's a bit helpful that the model teased out these philosophy of mind papers from the general run of idealism papers. It end up revealing a bit more about what's going on in these early days.

<!--chapter:end:topic_comments/topic12.Rmd-->

```{r t13a}
jjj <- 13
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t13b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t13c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t13d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t13e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

One of the reasons I ended up with ninety topics was because when I ran this model with sixty to seventy-five topics, it kept putting a weird subset of the Plato papers in with other topics. This was odd because  sorting the ancient philosophy papers neatly together was something that just about every other model run did perfectly. And I thought it should be easy for this one too, if I just bumped up the number of topics a bit. And eventually it happened. I kind of expected that I'd have more models that separated Aristotle from Plato, but I don't think I ever saw that.

The publication numbers are much as I expected. The raw numbers outrun the weighted numbers, which is possibly surprising. If there was more name checking of ancient philosophy in contemporary works—meaning there were a bunch of things that are not really ancient philosophy papers but which had just enough ancient philosophy to confuse the model—it would be the other way around. But the model knows that the word _platonism_ in a metaphysics or philosophy of math paper doesn't mean we're maybe doing ancient philosophy.

Still, it does end up being twenty-fifth of the ninety topics. Given the relative importance of books to papers in ancient philosophy, and the complete lack of interest in ancient philosophy from some of these journals, I feared it could be lower than this.

<!--chapter:end:topic_comments/topic13.Rmd-->

```{r t14a}
jjj <- 14
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t14b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t14c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t14d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t14e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is a large and long-lasting topic. And it is somewhat disjunctive.

It includes some metaphysics and, indeed some of the things I'd think of as paradigmatic to early analytic metaphysics, such as Russell's "[On the Relations of Universals and Particulars](https://philpapers.org/rec/RUSOTR-3)" and Ramsey's "[Universals](https://philpapers.org/rec/RAMU)". And it includes their contemporary successors, such as Fraser MacBride's "[Could Armstrong Have Been a Universal?](https://philpapers.org/rec/MACCAH)".

It includes some philosophy of language, though surprisingly less than I would have guessed from the keywords. It includes, for example, Wilfrid Sellars's "[Naming and Saying](https://philpapers.org/rec/SELNAS)", and David Liebesman's "[Simple Generics](https://philpapers.org/rec/LIESG)".

It includes a large amount of Peircean pragmatics. Here is a list just of the articles in the topic with "Peirce" in the title:

```{r peirce-universals}
peirce_universals <- relabeled_articles[grep("Peirce", relabeled_articles$title),] %>%
  as_tibble() %>%
  filter(topic == 14) %>%
  arrange(year) %>%
  select(mcitation)
  
for (jj in 1:nrow(peirce_universals)){
  cat(jj,". ", peirce_universals$mcitation[jj], " \n", sep="")
}
```

And there are other articles too that are about Peirce, or Peircean thought, without including the string _Peirce_ in the title.

But ultimately it is largely a topic about logic. It starts off with articles about Aristotelian logic, then moves into discussions of the role of subject and predicate in contemporary (broadly Fregean) logic. And that's where the model places it.

And it's not surprising, given the importance of debates about universals to philosophy from Plato, through Abelard and Ockham, through the present day, that it is one of the largest (and hardest to place) topics. 

<!--chapter:end:topic_comments/topic14.Rmd-->

```{r t15}
jjj <- 15
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t15b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t15c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t15d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t15e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is one of the main places in the model where logical positivism is seen: a sustained discussion from about 1936 (the publication of _Language, Truth and Logic_) to 1980 of the verification principle. It feels to me like this model really gets going when the problems for the verification principle are most pressing—many of these articles are about problems for, and the rapidly diminishing probability of salvaging, the verification principle. For instance, the second highest probability the model gives to an article being in this topic is Robert Brown and John Watling's "[Amending the Verification Principle](https://philpapers.org/rec/BROATV)" [@BrownWatling1951], which as early as 1951 is already deep into the "one patch per puncture" approach to saving the principle.

The model didn't quite know what to do with papers about conditionals in general. I'll talk about them more in what follows, because they are a tricky problem. But it turns out a bunch of pre-Lewisian papers on subjunctive conditionals ended up here as well. That's not surprising; they are a particularly tricky problem for verificationists to handle.

<!--chapter:end:topic_comments/topic15.Rmd-->

```{r t16a}
jjj <- 16
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t16b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t16c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t16d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t16e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The model divides up the ethics topics more finely than I would like, but this one is reasonably clear. It's about axiology i.e., the theory of value, fairly broadly construed. Happiness is often taken to be central to what's valuable, so there are articles about happiness in here. I worried that the model might confuse moral value with values of variables, but it seemed to understand that difference. It did end up thinking this topic had more to do with aesthetics than with the rest of ethics, but that's one spot where I feel free to override the model.

One of the most important articles in this topic is "[The Naturalistic Fallacy](https://philpapers.org/rec/FRATNF)" by [William Frankena](https://lsa.umich.edu/philosophy/undergraduates/prizes/william-k--frankena.html) [-@Frankena1939]. Frankena studied at Oxford, and this paper is from his doctoral thesis. He moved to the University of Michigan and stayed for all his career, serving as chair for fourteen years, and as president of the (then) Western Division of the APA. Much of Frankena's influence comes from his books, but he has twelve articles in the journals I'm investigating, and five of them are in this topic. 

The model isn't entirely sure what to say about "The Naturalistic Fallacy", but I think it ultimately makes the right call on it.

```{r frankena-naturalistic-fallacy}
individual_article("10.2307_2250706")
```

All of the top four topics there make sense. The paper is about Moorean conceptions of goodness, so naturally it's about value. It's about the definability of "good", and which principles in that definition might be analytic, so naturally those two topics turn up as well. And a crucial conclusion of the paper is that what Mooreans should say is that their opponents lack a certain kind of moral insight, something continuous with the discussions of moral conscience. So I think the model did a reasonable job of classifying Frankena's important early paper.

I'll come back later in the book to Frankena's esteemed colleagues at Michigan, especially Charles Stevenson and Richard Brandt.

<!--chapter:end:topic_comments/topic16.Rmd-->

```{r t17a}
jjj <- 17
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t17b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
jjj <- 17
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t17c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
jjj <- 17
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t17d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t17e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is a hard to classify topic. On the one hand, it's clearly about logic, so that's easy enough. But everything else about it is a little odd. The graphs don't show much life after 1920. But it ends up being either the twenty-seventh or fifteenth largest topic, depending on which measure is used. The measure that makes it fifteenth is the one the graphs are showing. Its top keyword is _syllogism_, but most of the characteristic articles are very modern pieces that barely mention the word. In fact, the word mostly vanishes from the journals after the 1960s.

```{r syllogism-graph, fig.height = 5, fig.cap = "How frequently does _syllogism_ appear in the journals?", fig.alt = alt_text}
word_frequency_graphs(c("syllogism"))
alt_text <- word_frequency_graph_alt_text(c("syllogism"))
```

Part of what's happened here is that while the model has decided to treat talk of _implication_ back in [topic 7](#topic07) differently to talk here of validity, it's thrown some very old notions in with this topic. That's not absurd, but it does lead to some odd results.
If squinting, a brief uptick can be seen at the end of the graph. But a big part of the story here is that this kind of work, to the extent that it's supported by the discipline at all, has largely moved to more specialist journals. So not as much of it is seen in these journals as in the past. And it would take a broader study to see what happens when specialist logic journals are added to the mix.

<!--chapter:end:topic_comments/topic17.Rmd-->

```{r t18a}
jjj <- 18
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t18b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t18c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t18d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t18e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This one really surprised me. 

It isn't a surprise that there is a topic on mechanisms. "[Thinking About Mechanisms](https://philpapers.org/rec/MACTAM)" is one of the most cited philosophy papers of the last few decades, and it makes sense that it would give rise to a topic.

No, what really surprised me was that there are so many articles in here from so long ago. I thought this would be a very modern topic, really taking off after 2000.

Part of what happens is that there is a series of articles on, broadly speaking, behaviorist psychology that end up here. These start out looking like straightforward philosophy of mind articles, but they move into being something more like psychology or philosophy of psychology articles, and from there it isn't too hard to get to philosophy of biology. And that's basically where the topic ends up, and it's how I've classified it. So it includes papers like "[A Tentative Analysis of the Primary Data of Psychology](https://philpapers.org/rec/KANATA)" [@Kantor1921], which is an attempt to separate out what is meant by stimulus, response and behavior. This is taken to be the crucial foundational question, because these constitute the core psychological data. It's a fairly interesting bit of history, and one that I wasn't at all aware played out in philosophy journals.

<!--chapter:end:topic_comments/topic18.Rmd-->

```{r t19a}
jjj <- 19
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t19b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t19c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t19d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t19e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I've called this _temporal paradoxes_, though that should be read  a bit disjunctively. The core of the topic is articles about Zeno's paradoxes. But there are some other articles about paradoxes. Carroll's paper (both the original and the repeat) is there I suspect because of the words _Achilles_ and _tortoise_. And there are several other articles about the passage of time that aren't connected to the paradoxes. Some of the recent work on temporal parts is in here, though more of it appears much later in [composition and constitution](#topic89).

The steady pace of articles here is quite striking, and maybe a touch ironic given the subject matter.

<!--chapter:end:topic_comments/topic19.Rmd-->

```{r t20a}
jjj <- 20
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t20b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t20c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t20d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t20e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This topic contains a bunch of stuff on space and time in classical (i.e., prerelativistic) physics, some stuff on philosophical issues arising in geometry, and some historical work connected to those two things. It has a lot of overlap with [space and time](#topic50). The big differences are that this topic is more classical, and space and time is more relativistic. And space and time is considerably more technical and mathematical. I've used the first difference to name the topics, but the second difference is what made this go in metaphysics, while space and time goes in philosophy of science.

What I've called the characteristic articles for this topic turn out not to be particularly characteristic. They are the ones with the highest probability of being in the topic. But they don't look much like the typical paper in the topic. The bulk of the articles in the topic are simply about space and time. But those articles are the ones that the model was most hesitant about clearly putting in this topic rather than space and time, and so they don't have the highest probability of being in the topic. The articles about geometry, and about Reid, didn't confuse the model as much, so they get listed here.

Other model runs didn't distinguish classical from relativistic work on space time. A more standard model run ended up distinguishing work on quantum mechanics from classical and relativistic mechanics, while grouping together classical and relativistic mechanics. And with the extra topic the model had to work with, it would instead include a topic centered on the philosophy of geometry and measurement. That's still a small topic but quite a lively one. Brent Mundy [-@Mundy1986; -@Mundy1987] did a lot of important work in this field, which has been getting a bit of attention in recent years. I regret that this model didn't highlight work along these lines as much as some other models did.

<!--chapter:end:topic_comments/topic20.Rmd-->

```{r t21a}
jjj <- 21
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t21b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t21c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t21d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t21e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is the main topic the model finds on early modern metaphysics and epistemology. It is a little bit centered on the rationalists; the characteristic articles are all on broadly rationalist figures. But that's largely because the model wants to hedge its bets on articles about Locke. Topic 31 will be on social contract theory, and Locke will play a major role in that obviously. So while this topic has twenty-eight articles with "Locke" in the title, all on his metaphysics and epistemology, the model is a little nervous about whether it should be putting those in with the other Locke articles in social contract theory. So that's why the characteristic articles are mostly about rationalist philosophers.

The big story here is the collapse in this category when Ryle takes over _Mind_ in 1947. According to [D. M. Hamlyn](https://www.jstor.org/stable/23955672?seq=5#metadata_info_tab_contents), who edited _Mind_ after Ryle, Ryle thought the only history worth publishing was Greek philosophy. As Michael Kremer pointed out to me, that's hard to square with the fact that Ryle published on several figures in history, including in early modern. Indeed, the first volume of his collected papers [-@Ryle1971] consists entirely of history papers, and only a few of these were in ancient philosophy. The volume includes two papers on Locke and one on Hume, so it's not as if he thought those figures should be ignored.

But in fact once he took over at _Mind_, a relatively thriving tradition of that journal publishing articles on early modern came to a complete halt. Apart from _Philosophical Review_, and to a smaller extent PPR, that put an end to articles in early modern turning up in the twelve journals I'm looking at. I don't have a good theory about why Ryle did this, and it's possible it was a policy handed down to him, but it is a striking fact about _Mind_'s publication record.

One quick point about the list of highly cited articles. The method I used for generating the list of highly cited articles only individuated articles by title, journal and publication year. I thought that would be enough, but it turns out journals used to publish two part articles without individuating the parts. So we get results like this one; I don't really know which of them is really cited the most. It's possible that neither part on its own is cited enough to deserve a spot here, but merging the citations got it into the list. In future work I hope to look more closely at citation data, and then I'll have to be more careful about cases like this one.

<!--chapter:end:topic_comments/topic21.Rmd-->

```{r t22a}
jjj <- 22
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t22b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t22c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t22d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t22e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This topic is mid-century philosophy of language, especially influenced by Wittgenstein's later work. It's tempting to say that it's influenced by _Philosophical Investigations_, though as you can see some of this work takes place well before the Investigations are published. Still, a lot of this work turns out to have had a Wittgensteinian influence. 

For example, those high points in the graph for pre-war _Analysis_ are largely driven by a few exchanges. One of those is between Margaret MacDonald, A. M. MacIver on how to understand the type/token distinction. The exchange includes at least the following. (I think this is complete, though I may have missed some.)

- [Language and Reference, by Margaret MacDonald](https://academic-oup-com./analysis/article/4/2-3/33/175389), Analysis, 4 (2/3): 33–41 [-@macdonald1936a].
- [Token, Type and Meaning, by A. M. MacIver](https://philpapers.org/rec/MACTTA-2), Analysis 4 (4):58 - 64 [-@maciver1936a].
- [Reply to Mr. MacIver, by Margaret MacDonald](https://philpapers.org/rec/MACRTM-4) Analysis 4 (5):77 - 80 [-@macdonald1937a].
- [Rejoinder to Miss MacDonald, by A. M. MacIver](https://philpapers.org/rec/MACRTM-3), Analysis 4 (6):81 - 88 [-@maciver1937a].
- [Further Reply to Mr. MacIver, by Margaret MacDonald](https://philpapers.org/rec/MACFRT-2), Analysis 5 (1):12 - 16 [-@macdonald1937b].
- [Last Words to Miss MacDonald, by A. M. MacIver](https://philpapers.org/rec/MACLWT), Analysis 5 (2):28 - 31 [-@maciver1937b].
- [Erratum: Last Words to Miss MacDonald, by A. M. MacIver](https://philpapers.org/rec/MACELW),  Analysis 5 (3/4):64 [-@maciver1938a].

I don't know how I ultimately want to feel about this exchange. On the one hand, I'm on MacIver's side of the philosophical dispute at the heart of the exchange. On the other hand, you could put McIver's articles in a museum as an exemplar of mansplaining. 

MacDonald is notable for several reasons, this exchange not really being one of them. She was one of the founders of _Analysis_, and then edited it from 1948 until her tragically early death in 1956. She wrote a very important paper on natural rights; it is the only highly cited paper back in [Topic 3](#topic03). And it was her notes, along with those of Alice Ambrose, that got turned into Wittgenstein's _Lectures on Ethics_. So there is a Wittgensteinian influence to these pre-war papers too.

I was surprised that this topic didn't feature more papers in _Philosophical Review_. I thought Norman Malcolm's move to Cornell in 1947 would have led to more Wittgensteinian articles here. But most of the Wittgenstein/Malcolm influenced work that turned up in the Review was in Topic 24.

<!--chapter:end:topic_comments/topic22.Rmd-->

```{r t23a}
jjj <- 23
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t23b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t23c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t23d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t23e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is a fairly easy topic to summarise—at its core, it's Marxism. And the trends are fairly easy to describe as well. There isn't much in the generalist journals at any time. Unsurprisingly, the moral and political journals discuss it a lot at their founding. And, more to my surprise, so do the philosophy of science journals at their founding. In all four cases there is a fairly quick drop-off, so this is now a fairly niche topic.

The upturn towards the end, in both _Philosophy of Science_ and _Aristotelian Society_ is because the model puts two other kinds of articles in here as well. First, the model thinks that anything broadly about economics might be here. We see that in the fact that some articles by famous economists, such as Sen and Arrow, are here. (Though the model is really uncertain about the Sen article.) Second, the model thinks that anything that talks about structures might be here as well. So papers on structural realism occasionally get put here - especially if the focus is more on the structures than on the realism. Both these kinds of articles, roughly meaning non-Marxist economics and issues about structures, are getting more important, and possibly would be separated out if our data ran through the present day.

<!--chapter:end:topic_comments/topic23.Rmd-->

```{r t24a}
jjj <- 24
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t24b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t24c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t24d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t24e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The biggest topic in the model is also one of the hardest to classify. I've called it _ordinary language philosophy_, and you can see from the graphs that it peaks in the years we associate with ordinary language philosophy, but beyond that it gets hard to say precisely what it is.

It is such a big deal in the years after World War II, especially in the _Proceedings of the Aristotelian Society_, that I had to change the scale for the graphs by journal. And yet there are very few articles that are unambiguously in this topic.

Part of what's going on here is that what the model is finding is a style as much as a content. Take a look at the keywords at the top of the page: _ask_, _certainly_, _really_, _surely_, _try_, _anything_, etc. These aren't philosophical topics, but they are a way of talking about philosophy. And they were a distinctive enough style that the model, which is always on the lookout for correlated word usages, declares it a big topic. 

To get a sense of just how distinctive the style is, take a look at the frequency of various words that were popular with the ordinary language philosophers. These graphs are taken straight from the JSTOR data. So they cut out all the one and two letter words, and some stop words that JSTOR filters out, but they don't remove all the other words that I took out. So the fractions on the y-axis show how often the word appears as a function of almost all the words in the article, not just of the words included in this study.

The dashed lines are roughly speaking the average rate that the word appears across the whole study. But not exactly. More precisely, it is the average across the 138 years of the rate of the word in each year. I thought that was a little more representative of the role of the word in the study, rather than just having the huge number of articles in recent years swamp the averages.

```{r word-frequency-by-journal}
word_year_journal_frequency <- function(x, y){
  left_join(word_year_journal_count %>% 
              filter(journal == y), 
            all_journals_tibble %>%
              filter(word == x) %>%
              left_join(articles, by = "document") %>%
              filter(journal == y) %>%
              group_by(year) %>%
              dplyr::summarise(c = sum(wordcount)),
            by = "year") %>%
    replace_na(list(c = 0)) %>%
    mutate(f = c / a) %>%
    mutate(term = x)
}

pas_word_journal_frequency_graphs <- function(x){
  temp_fun <- function(v){
    word_year_journal_frequency(v, "Proceedings of the Aristotelian Society")
  }
  t <- lapply(x, temp_fun) %>% bind_rows()
  ggplot(t, aes(x = year, y = f, color = term, group = term)) +
    freqstyle +  
    stat_summary(fun = mean, 
               aes(x = 1950, yintercept = ..y.., group = term), 
               geom = "hline",
               linetype = "dashed",
               size = 0.2) +
    geom_point(size = 0.6, alpha = 0.8) +
    scale_x_continuous(minor_breaks = 10 * 1:201,
                       expand = expansion(mult = c(0.01, 0.01))) +
  scale_y_continuous(expand = expansion(mult = c(0.01, .03)),
                     minor_breaks = scales::breaks_pretty(n = 12),
                     breaks = scales::breaks_pretty(n = 3),
                     labels = function(x) ifelse(x > 0, paste0("1/",round(1/x,0)), 0)) +
#  scale_y_continuous(labels = scale_inverter) +
  labs(x = element_blank(), y = "Word Frequency") +
  theme(legend.title = element_blank())
}

pos_word_journal_frequency_graphs <- function(x){
  temp_fun <- function(v){
    word_year_journal_frequency(v, "Philosophy of Science")
  }
  t <- lapply(x, temp_fun) %>% bind_rows()
  ggplot(t, aes(x = year, y = f, color = term, group = term)) +
    freqstyle +
    geom_point(size = 0.6, alpha = 0.8) +
    scale_x_continuous(minor_breaks = 10 * 1:201,
                       lim = c(1889, 2013),
                       expand = expansion(mult = c(0.01, 0.01))) +
  scale_y_continuous(expand = expansion(mult = c(0.01, .03)),
                     minor_breaks = scales::breaks_pretty(n = 12),
                     breaks = scales::breaks_pretty(n = 3),
                     labels = function(x) ifelse(x > 0, paste0("1/",round(1/x,0)), 0)) +
#  scale_y_continuous(labels = scale_inverter) +
  labs(x = element_blank(), y = "Word frequency") +
  theme(legend.title = element_blank())
}

```

```{r olpwords-graphs-t24, fig.height = 5, fig.cap = "Popular words in ordinary language.", fig.alt = alt_text}
word_frequency_graphs(c("ask", "surely", "try", "put", "tell"))
alt_text <- word_frequency_graph_alt_text(c("ask", "surely", "try", "put", "tell"))
```

A rise is visible in the frequency of each word in the years after World War II. In some cases this is restoring the level that existed back in the 1870s and 1880s, but in all cases there is a steep fall from the mid-1960s onwards. Let's see what happens if we restrict this to the journal that seems central to ordinary language philosophy, _Proceedings of the Aristotelian Society_. (Note that the averages on this graph are averages for just this journal, so they are higher than the overall averages.)

```{r olpwords-graphs-t24-pas, fig.height = 5, fig.cap = "Popular words in ordinary language in _Proceedings of the Aristotelian Society_.", fig.alt = alt_text}
pas_word_journal_frequency_graphs(c("ask", "surely", "try", "put", "tell"))
alt_text <- journal_word_frequency_graph_alt_text(c("ask", "surely", "try", "put", "tell"), "Proceedings of the Aristotelian Society")
```

And again you see much higher frequencies in the 1950s and 1960s.

There is a question about whether I should have manually filtered out something like this whole topic. I could have added _ask_, _quite_, _else_ and so on to the stop words that I filtered out. If I'd been really aggressive about this, I could have gotten rid of this whole topic. And maybe I should have done this. There are a few topics that are like this. I could have added _argument_ as a stop word and gotten rid of the [arguments](#topic55) topic. I could have added _concept_ as a stop word and gotten rid of the [concepts](#topic78) topic. There is an interesting question about where to stop, and how tightly to focus on _topics_ as opposed to tools (like arguments or concepts), or styles (like ordinary language). I do think that if I were starting over I would try to strip some of the keywords here out, but it's not obvious to me what's right.

There are a lot of highly cited articles in this topic. That's largely because there are a lot of articles in this topic. Notably, a lot of the highly cited articles here are well after the peak of ordinary language philosophy. The highly cted papers are by a fairly disparate set of authors. But one striking thing is that, to my eyes at least, there are a lot of very good writers represented here. So we see papers, for instance, by Bernard Williams, Frank Jackson, Stephen Yablo and Philippa Foot. On the other hand, there is also a Wittgenstein paper here, and I never thought of Wittgenstein as a great stylist, so maybe it's just a coincidence. But mostly I think most writers would be better off if they tried to adopt the styles of the midcentury greats.

Finally, note that I've put this topic in two categories. While the papers here to cover a huge range (there are fourteen hundred of them after all), there do seem to be two big clusters in ethics and in philosophy of mind. I'll go over how I split topics in two when I come to another topic, [sets and grue](#topic37), that is even more binary than this one.

<!--chapter:end:topic_comments/topic24.Rmd-->

```{r t25a}
jjj <- 25
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t25b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t25c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t25d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t25e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I've called this _moral conscience_, though there are any number of other terms I could have used. Along with [value](#topic16), [virtues](#topic49) and [duties](#topic53) it forms the core of normative ethics. The division among these four topics is somewhat arbitrary, and it isn't wrong to see them as really forming one large topic.

Not that this topic needs any extra size—it's already either the third or eleventh largest, depending on which measure is used. Despite the rise in attention to ethics in recent years, it actually peaks in the 1950s, and declines a lot after the 1980s. I think this is due to changes in terminological fashion rather than a change in the topics being considered, but that's a very contentious take on the literature.

It's notable that we see some small overlap with [idealism](#topic02) here. Usually the model is clear about which articles are, and are not, idealist. But the discontinuous array of dots to the left of the graph is driven by this generalization breaking down. We can see this in, for instance, this article by (future prime minister) Arthur Balfour.

```{r balfour-ethics}
individual_article("10.2307_2246618")
```

This topic also features some important work by Michigan faculty, such as these papers. (I'm sure I'm missing a few here as well.)

```{r michigan-ethics-articles}  
michigan_ethics <- relabeled_articles[grep("Frankena|Darwall|Brandt|Railton", relabeled_articles$authall),] %>%
  as_tibble() %>%
  filter(topic == 25) %>%
  arrange(year)
for (jj in 1:nrow(michigan_ethics)){
  cat("- ", michigan_ethics$mcitation[jj], " \n", sep="")
}
```

As I mentioned [earlier](#topic16), a lot of Frankena's most important work is not included in this study because it wasn't in journals. So it's nice that it gets included, by proxy at least, via Darwall's remembrance.

<!--chapter:end:topic_comments/topic25.Rmd-->

```{r t26a}
jjj <- 26
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t26b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t26c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t26d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t26e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

A more accurate, if less pithy, name for this category would be midcentury general philosophy of science. It isn’t all midcentury; if one scrolls down the list of articles in the topic there are several from recent years. But there is something midcentury about the spirit of it; it’s philosophy of science that is very much not philosophy of one or other science in particular. And as that has gone out of fashion, this topic has declined.

The statistics for this topic were very surprising. The years make sense; midcentury philosophy peaks in the midcentury. But the numbers show how misleading the graphs can be. Eyeballing the graphs makes it look like this was just bubbling along, but the numbers say it is the fifth or sixth biggest topic of the ninety. The story here, I think, is that there was much less specialization in the 1950s and 1960s, and so individual topics get a chance to grow a lot.

And the topic overlaps were surprising enough to make me recheck the code. It isn’t completely surprising that philosophy of science would intersect somewhat with Marx. “Scientific socialism” was a thing, and plenty of philosophers of science had Marxist sympathies. And there is a lot of time sensitivity to these analyses, and Marx was a big topic at the same time that this topic was bit. But that it would be the highest overlap was a surprise. And it shows how much philosophy of science has changed that formal epistemology is literally the lowest overlap.

<!--chapter:end:topic_comments/topic26.Rmd-->

```{r t27a}
jjj <- 27
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t27b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t27c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t27d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t27e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

In earlier iterations of this project I ran through a few different journals. And one of the effects of doing this was that I’d occasionally see topics that weren’t really that big in philosophy in general but were a big deal in that journal. And some of them were very distinctive to the interests of one or another journal editor. This iteration of the project mostly didn’t have this; very few topics are held up by a single journal.

This topic is an exception. The graph for _Philosophy and Phenomenological Research_ obviously looks very different to the graphs for the rest of the journals. But I don’t think this means that we’re seeing something unrepresentative here. Rather, what we’re seeing is something like a blind spot or deliberate oversight) in the other eleven journals, which is counterbalanced by _Philosophy and Phenomenological Research_. This does mean that when _PPR_ changes its focus in the 1980s, this topic seems to fall away very quickly. That’s misleading though, if one cares about all of philosophy. What’s really true is that the topic moves away from these twelve journals, and this becomes a respect in which the twelve journals are less than fully representative of philosophy.

When there is a difference this striking between journals, or between times, I like to go back and check the underlying data to see whether it is just a weird consequence of the way the model was built. In this case it is easy enough to see that there is a striking difference beteween _PPR_ and the other eleven journals. Here are the number of times the word _Husserl_ appears in each journal each year.

```{r husserl-year-graph, fig.cap = "Number of times Husserl is mentioned in each journal each year.", , fig.alt = alt_text}
x = "husserl"

jjj <- 27
cats <- 90

husserl_tibble <-          all_journals_tibble %>%
            filter(word == x) %>%
            left_join(articles, by = "document") %>%
            group_by(year, journal) %>%
            dplyr::summarise(c = sum(wordcount)) %>% 
            filter(!is.na(year)) %>% 
            ungroup() %>% 
            add_row(year = 1918, journal = "Mind", c= 0) %>% 
            add_row(year = 1928, journal = "Mind", c= 0) %>% 
            arrange(desc(year)) %>% 
            complete(year, journal, fill = list(c = 0))

cate_num <- 3

yupper <- max(husserl_tibble$c, na.rm=TRUE)

facet_labels <- chap_two_facet_labels %>%
  mutate(year = 1954.5, c = yupper)

facet_labels$journal <- factor(facet_labels$journal, levels = journal_order)

ggplot(data = husserl_tibble, aes(x = year, y = c))  +
  facetstyle +
  scale_y_continuous(breaks = c(200, 400)) +
  geom_point(size = 0.15, colour = hcl(h = (jjj-1)*(360/cats)+15, l = 65, c = 100)) +
  theme(legend.position="none",
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        plot.title = element_text(colour = hcl(h = (jjj-1)*(360/cats)+15, l = 65, c = 100)))  +
  labs(x = element_blank(), y = "Number of Mentions", title = "Husserl") +
  facet_wrap(~journal, ncol = 3) +
  geom_text(data = facet_labels,
            mapping = aes(label = short_name),
            vjust = "inward", 
            hjust = "middle",
            fontface = "bold", 
            size = 3,
            colour = "grey40")

alt_text <- "A graph showing how often the word Husserl appears in each journal in each year. It appears a lot in Philosophy and Phenomenological Research, and almost never in the other eleven journals."
```

And here is the same graph for Heidegger. Note that in some recent years, 2002 and 2012, the word _Heidegger_ does not appear in these twelve journals.


```{r heidegger-year-graph, fig.cap = "Number of times Heidegger is mentioned in each journal each year.", , fig.alt = alt_text}

x = "heidegger"

heidegger_tibble <-          all_journals_tibble %>%
  filter(word == x) %>%
  left_join(articles, by = "document") %>%
  group_by(year, journal) %>%
  dplyr::summarise(c = sum(wordcount)) %>% 
  filter(!is.na(year)) %>% 
  ungroup() %>% 
  add_row(year = 1932, journal = "Mind", c= 0) %>% 
  add_row(year = 2002, journal = "Mind", c= 0) %>% 
  add_row(year = 2012, journal = "Mind", c= 0) %>% 
  arrange(desc(year)) %>% 
  complete(year, journal, fill = list(c = 0))

cate_num <- 3

yupper <- max(husserl_tibble$c, na.rm=TRUE)

facet_labels <- chap_two_facet_labels %>%
  mutate(year = 1959.5, c = yupper)

facet_labels$journal <- factor(facet_labels$journal, levels = journal_order)

ggplot(data = heidegger_tibble, aes(x = year, y = c))  +
  facetstyle +
  scale_y_continuous(breaks = c(200, 400)) +
  scale_x_continuous(breaks = c(1950, 1975, 2000)) +
  geom_point(size = 0.15, colour = hcl(h = (jjj-1)*(360/cats)+15, l = 65, c = 100)) +
  theme(legend.position="none",
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        plot.title = element_text(colour = hcl(h = (jjj-1)*(360/cats)+15, l = 65, c = 100)))  +
  labs(x = element_blank(), y = "Number of Mentions", title = "Heidegger") +
  facet_wrap(~journal, ncol = 3) +
  geom_text(data = facet_labels,
            mapping = aes(label = short_name),
            vjust = "inward", 
            hjust = "middle",
            fontface = "bold", 
            size = 3,
            colour = "grey40")

alt_text <- "A graph showing how often the word Heidegger appears in each journal in each year. It appears a lot in Philosophy and Phenomenological Research, and almost never in the other eleven journals."
```

I’m calling this topic a history topic because most of the papers feel historical; they are looking back at work done in the glory days of phenomenology. But those 1940s papers that make up a chunk of the topics weren’t intended as being papers in history of philosophy; they were just doing philosophy. This is a systematic challenge with a study this long. If I extended the study even further—say by including seventeenth- to nineteenth-century books—there would be even more dramatic versions of this effect. Topic modeling can’t typically tell works by an author apart from works about that author, so any historical figure will be grouped in with the history of work on that author.

<!--chapter:end:topic_comments/topic27.Rmd-->

```{r t28a}
jjj <- 28
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t28b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t28c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t28d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t28e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is an important topic but one that’s a little hard to classify. A lot of the papers here are early psychology papers, which seem like philosophy of mind papers. A rather large part of what they cared about was the nature of emotions. In recent years it has been as much a topic in ethics as anything else. One can even see a little bump in _Ethics_ in the 1990s. And that’s where the three highest cited papers come from. In between, it also ends up as a mind topic, but more connected to Rylean behaviorism than to traditional psychology. If we extended the study through the 2010s and into the 2020s, it would start to blend into recent social and political philosophy via work on the role of anger.

The effect is that the chronological ordering that I’ve been using for this breaks down a fair bit. It’s not like this topic should be centered on the time just after ordinary language philosophy. Rather, it draws on a bunch of somewhat connected papers from across the 138 years, and so it ends up centered around the center of those years.

<!--chapter:end:topic_comments/topic28.Rmd-->

```{r t29a}
jjj <- 29
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t29b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t29c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t29d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t29e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

In almost every model I ran, philosophy of religion was a single topic. But here, possibly because I was using so many topics, it split off ontological arguments from [the rest of philosophy of religion](#topic11). There are a few other articles in here on the general topic of arguments for God's existence, but they mostly are ontological argument papers. I didn't really realize that there were so many of them. There are, for example, about twice as many articles in the twelve journals on the ontological argument as on the Gettier problem, which is really not what I had realized.

<!--chapter:end:topic_comments/topic29.Rmd-->

```{r t30a}
jjj <- 30
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t30b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t30c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t30d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t30e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I've called this _philosophy of chemistry_, but it could just as easily be described as nonfundamental physics. Any kinds of discussion of physics beyond the most fundamental is a candidate for ending up here.

But note this could easily be merged with the later topic on [thermodynamics](#topic73), which captures a lot of recent work on philosophical issues arising out of the study of entropy. Looking at these graphs gives a slight underestimate of how much time philosophers of physics (broadly construed) spend looking at anything other than maximally fundamental issues in physics. 

<!--chapter:end:topic_comments/topic30.Rmd-->

```{r t31a}
jjj <- 31
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t31b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t31c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t31d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t31e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

One of the first courses I ever taught was on the history of social contract theory. It was an easy enough class to teach, because there are so many versions of the Hobbes, Locke, Rousseau course around to use as a basis. And this is basically the Hobbes, Locke, Rousseau topic.

As you can see, _Ethics_ used to include a lot more papers on history, but now they do very little. And _Philosophy and Public Affairs_ has never done much history. So this becomes a rather small topic, relative to its historical importance. But since none of these articles ever troubled the citation indices enough to appear here, maybe the journal editors are just reacting correctly to the signals they are getting from the market.

<!--chapter:end:topic_comments/topic31.Rmd-->

```{r t32a}
jjj <- 32
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t32b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t32c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t32d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t32e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Another fairly easy topic to describe—it's Kant. Both Kantian ethics and Kantian metaphysics end up here. Like all of the other early modern topics, it drops away a bit after Ryle takes over _Mind_ and excludes all (non-Greek) history from that journal. But it always hovers a bit above the zero line.

I had expected there would be a bit more in recent years, but I think I'd been taking _Philosophical Review_ to be a more representative than it turns out to be. The data is a little noisy, but it does look like _Philosophical Review_ has been substantially more receptive to work on Kant in the last twenty years than any of the other journals.

<!--chapter:end:topic_comments/topic32.Rmd-->

```{r t33a}
jjj <- 33
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t33b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t33c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t33d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t33e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This, like a few other ethics topics, is a little strange. There is enough unity in ethics that the model had a slightly hard time finding natural joints to carve at.

Looking at the graph over time, this looks like a very temporally specific graph. That peak in the early-to-mid-1960s followed by a dramatic fall looks like a topic that simply burned out. But look more closely, and one see that it doesn't fall nearly to zero. And several of the characteristic articles are from recent years.

What's happened here, I think, is that the model has stumbled on to a somewhat disjunctive topic. This topic makes more sense if thought of as one part deontic logic, and one part promises. The deontic logic part has a really sharp peak; there isn't much work on that either before or after that peak around 1966 and 1967. This is just about the last topic to become fashionable before the boom in the late-1960s where lots of things became fashionable and stayed so more or less ever since. But the promises part is not nearly as uneven, and most of the recent work is on promises. I suspect that in the long run that work will end up being more relevant to the citations than the deontic logic work, but it's hard for new articles to have huge citation counts.

Did it make sense for the model to throw together deontic logic and promises? Not really; it had to make some arbitrary choices and it made this one. We're about to get a run of topics where the model seems to have settled for disjunctive topics. In this one at least it picked two disjuncts from the same subdiscipline. We won't always be so lucky.

<!--chapter:end:topic_comments/topic33.Rmd-->

```{r t34a}
jjj <- 34
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t34b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t34c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t34d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t34e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

A very small topic on the analytic/synthetic distinction. It's small because so many of the papers that one might have thought would end up in here are instead in [radical translation](#topic60), which is all about Quinean philosophy of language. What we get here instead is a combination of four overlapping discussion threads.

- A handful of early, Kant-influenced works.
- A much larger number of works in or about positivism, with Carnap as the primary figure.
- The distinction between two kinds of analyticity introduced by Boghossian.
- Works in or about the Canberra plan, with Jackson and especially Chalmers as the central figures.

The second of those is the largest, which is why this topic is turning up so early in the story. But the latter two are sizable as well, which is why this is turning up much later than the discussions of definitions or of verification.

<!--chapter:end:topic_comments/topic34.Rmd-->

```{r t35a}
jjj <- 35
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t35b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t35c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t35d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t35e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

One of the big fears I had doing this project was that the model would get confused between different uses of the one word. We’ve already seen the model get confused by terminological changes that don’t reflect philosophical changes. But that’s not a deep problem; it’s possible to just manually add the generated topics back together. But common terminology for the same idea is harder to deal with.

And it’s what is presented here. This topic includes work on political freedom and work on free will. It doesn’t help that both fields spend a lot of time talking about libertarian views. And it really doesn’t help that some people, perhaps not that many but enough to confuse the model, think there are deep connections between the fields.
Fortunately there turns out to be a way to handle this problem. I’ll go over this when I get to [sets and grue](#topic37), but the short version is that building a two-topic LDA out of the articles in this topic lets me more or less split apart the articles on political freedom from the articles on free will.

I’ve classified the free will papers as ethics, although I think a lot of the people working on the topic would have thought of themselves as doing metaphysics. Certainly when I was a student we covered free will in intro metaphysics classes, not intro ethics classes. But it seems to me this is clearly about ethics, and the recent trend toward looking much more closely at the nature of responsibility, especially in light of empirical discoveries about the mind and brain, makes this classification much more natural.

Note that this is very much not the last of free will. There will be an entire topic devoted to Frankfurt cases. And some of the work on responsibility comes up in the several ethics topics. So don’t take these graphs as any kind of measure of how much coverage free will gets.

<!--chapter:end:topic_comments/topic35.Rmd-->

```{r t36a}
jjj <- 36
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t36b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t36c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t36d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t36e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The bulk of this topic is issues about punishment. And it's striking just how little coverage this gets in the "generalist" journals. This particular gap wasn't the reason that I included _Ethics_ and _Philosophy and Public Affairs_ in the study, but it would have done just as well as a reason. Punishment is a really important philosophical topic, and most philosophy journals simply don't talk about it.

This topic also picks up some recent topics that are not, or at least not entirely, about laws and institutions. Instead they are about the role of mercy and forgiveness in personal interactions. These were best classed as ethics papers rather than social and political papers. A binary sort was able to find these papers as a separate category, so I was able to sort those papers into ethics. But the vast bulk of the papers are about legal punishment (and legal forgiveness and legal mercy), and those are put into social and political.

<!--chapter:end:topic_comments/topic36.Rmd-->

```{r t37a}
jjj <- 37
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t37b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t37c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t37d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t37e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is just about the paradigm example of the model finding a disjunctive topic. It knew that it wanted to put the articles on set theory in one place. And it knew that it wanted to put the articles on the grue paradox in one place. And it didn't have enough places to put them all, so it put both of them here.

The two topics aren't wholly unconnected; they are both problems for the kind of philosophical world view that Nelson Goodman wanted to endorse. And, of course, Goodman himself had important views on how to solve each of them. But we shouldn't go overboard trying to figure out why the model put them in one place. Most model runs had these topics separated.

As can be seen from the highly cited articles list the model also put some articles on puzzles about verisimilitude in here. That makes more sense. Like the grue paradox, that's a puzzle about how to manage ampliative reasoning using just the tools of formal logic.

Because the topic is so disjunctive, it lends itself reasonably well to a binary sort. So I took the 555 articles the model found, and generated a two-topic LDA from just those articles. 

```{r binary-grue}
load("binary_lda/lda_37.RData")
grue_gamma <- tidy(binary_lda, matrix = "gamma") %>%
  left_join(relabeled_articles, by = "document") %>%
  mutate(gamma.z = gamma.x * gamma.y)

grue_years <- grue_gamma %>%
  group_by(year, topic.x) %>%
  dplyr::summarise(g = sum(gamma.x)) %>%
  filter(year > 1929)
```

Here are the top fifty articles in what it called topic 1.

```{r binary-grue-table-one}
grue_gamma_table_one <- grue_gamma %>%
  filter(topic.x == 1) %>%
  arrange(-gamma.x) %>%
  slice(1:50) %>%
  select(citation, gamma.x)

datatable(grue_gamma_table_one,           
          colnames = c("Article", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:1)),
                         pageLength = 5
                         ),
          caption = htmltools::tags$caption("Articles in subtopic 1."#, style = "font-weight: bold"
          )
    )%>%
      formatSignif('gamma.x',6) %>%
      formatStyle(1:2,`text-align` = 'left')
```

It's very confident about its binary sort, as is clear from the probabilities. (I've normally been doing these to four significant figures, but I needed six here just to get any distinctions.) But some of these are articles that it was not confident were in the category in the first place. Let's instead sort articles by the product of their probability of being in topic 37 (i.e., sets and grue) and their probability of being in subtopic 1 (which is starting to look like grue).

```{r binary-grue-table-two}
grue_gamma_table_two <- grue_gamma %>%
  filter(topic.x == 1) %>%
  arrange(-gamma.z) %>%
  slice(1:50) %>%
  select(citation, gamma.z)

datatable(grue_gamma_table_two,           
          colnames = c("Article", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:1)),
                         pageLength = 5
                         ),
          caption = htmltools::tags$caption("Articles in subtopic 1."#, style = "font-weight: bold"
          )
    )%>%
      formatSignif('gamma.z',4) %>%
      formatStyle(1:2,`text-align` = 'left')
```

The first page here leads to things downstream of the grue paradox that I wasn't particularly familiar with: work on the proper formalisation of scientific theories, and on the foundations of learning algorithms. Click onto page 2 and there are more articles by Ullian and by Zabludowski, so we're in the mainstream of grue articles. Let's do the same thing for articles in subtopic 2.

```{r binary-grue-table-three}
grue_gamma_table_one <- grue_gamma %>%
  filter(topic.x == 2) %>%
  arrange(-gamma.x) %>%
  slice(1:50) %>%
  select(citation, gamma.x)

datatable(grue_gamma_table_one,           
          colnames = c("Article", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:1)),
                         pageLength = 5
                         ),
          caption = htmltools::tags$caption("Articles in subtopic 2."#, style = "font-weight: bold"
          )
    )%>%
      formatSignif('gamma.x',6) %>%
      formatStyle(1:2,`text-align` = 'left')
```

These aren't a million miles from set theory, but in some cases they are a little bit distinct. Let's have a look at what happens when crossing the probability of being in sub-topic 2 with the probability of being in topic 37.

```{r binary-grue-table-four}
grue_gamma_table_two <- grue_gamma %>%
  filter(topic.x == 2) %>%
  arrange(-gamma.z) %>%
  slice(1:50) %>%
  select(citation, gamma.z)

datatable(grue_gamma_table_two,           
          colnames = c("Article", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:1)),
                         pageLength = 5
                         ),
          caption = htmltools::tags$caption("Articles in subtopic 2."#, style = "font-weight: bold"
          )
    )%>%
      formatSignif('gamma.z',4) %>%
      formatStyle(1:2,`text-align` = 'left')
```

And here we are looking at something very much like set theory.

The point of all this was to demonstrate something of the power of this method of analysis. Even when the model does something that seems wrong—like smushing together two topics that don't really belong—we can use the very same tools to undo the mistake. Now that the topics are separated their frequency over time can be seen.

This is a graph of the distribution of those 555 articles into the two buckets (grue and sets) over time.

```{r grue-binary-graph, fig.height=5, fig.cap = "Articles on sets and articles on grue.", fig.alt = alt_text}
grue_years <- grue_years %>%
  mutate(topic.x = as.factor(topic.x))
ggplot(grue_years, aes(x = year, y = g, group = topic.x, color = topic.x)) + 
  spaghettistyle +
  geom_point() + 
  labs(x = element_blank(), y = "Weighted number of articles", title = "Articles in Subtopics Sets and Grue") +
  theme(legend.title = element_blank(),
        legend.position = "right") +
  scale_color_discrete(labels = c("Grue", "Sets"))
alt_text <- "A scatterplot showing how the model divides up articles into sets and grue. It is described briefly in the text below."
```

One can see that both of the topics peak in the late 1960s and early 1970s. And set theory doesn't fall away quite as quickly as grue. But I had expected the difference between the two topics to get rather dramatic after 2000, and it doesn't really. The model, somewhat sensibly, puts other work on formal learning in with grue, and this means the topic doesn't crash away.


<!--chapter:end:topic_comments/topic37.Rmd-->

```{r t38a}
jjj <- 38
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t38b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t38c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t38d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t38e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

A tiny topic, but still one that seemed to need breaking up in order to categorise. The papers are mostly about origins and ends, which make sense as a unified category. But then it bifurcates into debates within metaphysics, primarily about essentialism and identity over time, and in philosophy of science, about the role of teleological reasoning. 

There are only ninety articles here, so there isn't a ton to say about them collectively. It was a little surprising to me that there isn't a Kripke/Parfit-related bump in the 1970s for work on personal identity, especially since so much of the work is on origin essentialism. Part of the story is that there is a bit of work in the British journals on personal identity before the Kripke/Parfit boom, some of it inspired by Wiggins. And the topic is small enough that that's enough to stop a huge rise once Kripkean and Parfittian theories of personal identity become prominent.

<!--chapter:end:topic_comments/topic38.Rmd-->

```{r t39a}
jjj <- 39
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t39b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t39c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t39d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t39e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

One of four different topics the model found on time, which has been a continuing source of interest in the journals. This one is on broadly speaking the existence of times. We've already looked at discussions of the [temporal paradoxes](#topic19), and [space and time in classical physics](#topic20). and we're coming up to the discussion of [space and time in relativistic physics](#topic50). And that's setting aside the [composition and constitution](#topic89), which includes a lot of papers on temporal parts.

Especially compared to its neighbors in debates about time, this one has a very flat distribution across time. Perhaps there is some kind of irony in this. This can be seen when looking at all four topics on a single graph. (I'm starting this in the twentieth century to exclude some outlier years, and I've included trend lines to make things clearer.)

```{r graph-time-categories, fig.height = 5, fig.cap = "Weighted frequency of articles in four topics about time.", fig.alt = alt_text}
  weight_ratio_graph <- ggplot(filter(weight_ratio, topic == 19 | topic == 20 | topic == 39 | topic == 50), aes(x = year, y = y, color=topic, group=topic))
  weight_ratio_graph + 
    spaghettistyle +
    coord_cartesian(xlim = c(1900, 2013), ylim = c(0, 0.04)) +
    scale_colour_discrete(labels = c(the_categories$subject[19], the_categories$subject[20], the_categories$subject[39], the_categories$subject[50])) + 
    theme(legend.title = element_blank(), legend.position = "right") +
    geom_point(size = 1, alpha = .5) +
    geom_smooth(se = F, method = "loess", size = .5, formula = "y ~ x") +
    labs(x = element_blank(), y = "Weighted frequency of articles")
  
alt_text <- "A scatterplot showing the proportion of articles in the four main topics about time. The two early topics, temporal paradoxes and classical space and time, slowly recede over the years. This topic, which I've simply called time, peaks in the 1980s. And the topic I've called space and time, which is largely about relativity, keeps rising through the century."
```

The two early topics gradually fall, though maybe they have both reached a steady state. Relativistic space and time rises through around 1980, though it too might have obtained an equilibrium. But this topic, the metaphysics of time(s) stays at a very steady level around 1 percent year after year.

<!--chapter:end:topic_comments/topic39.Rmd-->

```{r t40a}
jjj <- 40
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t40b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t40c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t40d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t40e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Just like [the previous topic](#topic39), color/colour shows a very constant pattern over time. There are some outlier measures in the early years, when there are papers that are probably just as much psychology papers as philosophy papers by twentieth-century standards). But otherwise it stays at around 0.5–1 percent per year.

When I was building a lot of models to see what patterns turned up repeatedly, a topic on color/colour turned up reliably but not uniformly. This model is about as paradigm an instance of the topic as one might hope to see. It has articles from the nineteenth century through the twenty-first century and multiple articles near the top of the characteristic articles table by Edward Wilson Averill and Mark Eli Kalderon. That feels right to me, but not all of the models agreed. It isn’t too hard to build a model were some of the early papers get rolled in with the other psychology papers, other papers get lumped into the broader [perception](#topic37) topic, and yet others get put in with metaphysics. But that seemed wrong; this is a distinctive and important topic.

It is useful to see how much it has (and has not) changed over time. The model gives the highest probability of being in this topic to [this discussion note](https://www.jstor.org/stable/2246673) by Grant Allen [-@Allen1879]. It’s a speculative piece about what recent experimental results might say about the structure of cones and their arrangement inside the eye. This kind of piece doesn’t feel out of place in _Mind_ circa 1879, and it wouldn’t feel out of place in present-day philosophy, but it would very much feel out of place for much of the twentieth century.

<!--chapter:end:topic_comments/topic40.Rmd-->

```{r t41a}
jjj <- 41
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t41b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t41c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t41d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t41e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I've called this war, and that is mostly what it is, though there are two ways in which this is misleading. 

One is that the topic is quite broadly about the ethics of what states do. For most of the period that I'm\ looking at, the state action that philosophers were most interested in was the act of going to war. But the topic includes articles on the relationship between states and international institutions, as well as a few articles on voting and democracy. I suspect that some recent work on immigration, and whether states are permitted to point guns at people who have the temerity to move around the world, would end up here if I extended the study to the present day.

The other, which is related, is that this topic is really focused on state-level actions. Contemporary philosophers writing about war have spent more time focusing on what is permissible and impermissible for individual soldiers to do. And that work is classified with work on [self-defence](#topic71).

<!--chapter:end:topic_comments/topic41.Rmd-->

```{r t42a}
jjj <- 42
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t42b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t42c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t42d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t42e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is a bit of a grab-bag of papers, loosely connected to the idea of depiction. I've classified it as aesthetics because it includes papers like

- [Analysis of the Poetic Similie](https://philpapers.org/rec/RIEAOT) by Max Reiser.
- [On What A Painting Represents](https://philpapers.org/rec/ZIFOWA) by Paul Ziff.
- [The Aesthetics Of Photographic Transparency](https://philpapers.org/rec/LOPTAO) by Dominic MacIver Lopes.

But it also includes papers that seem less neatly classed as Aesthetics, like

- [You Can Call Me 'Stupid', ... Just Don't Call Me Stupid](https://philpapers.org/rec/FARYCC) by Delia Graff Fara.
- [Self to Self](https://philpapers.org/rec/VELSTS) by David Velleman.
- [On the New Riddle of Induction](https://philpapers.org/rec/BAROTN-2) by S. F. Barker and Peter Achinstein.

Still, the aesthetics papers predominate, so that's the category I've gone with. Very often these models would locate the aesthetics papers quite neatly and put them in a category. But this model has separated out these papers on depiction from the larger category of papers on beauty. And the result is that the topic is fairly small.

<!--chapter:end:topic_comments/topic42.Rmd-->

```{r t43a}
jjj <- 43
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t43b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t43c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t43d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t43e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

It's basically "On Denoting".

_Mind_ reprinted "On Denoting" for its one hundredth anniversary, and the model provides slightly different probabilities for it. Partially this is because this is a nondeterministic model, and partially it's because there was a little bit of editorial introduction that made it into the 2005 text.

Some of the papers in here are best thought of as history papers. This is one of the challenges with these topics from early analytic work. We're covering such a long range that we include papers that were not at all history papers, like say "On Denoting", as well as historical work on those papers. But I think it makes much more sense to call this a history topic.

"On Denoting" is one of the most important papers in the history of analytic philosophy, so it makes sense that it would be a topic of its own, that its importance would be recognized immediately, and that it would lead the citation counts. Except only the first of those three things is true. 

A bit of attention paid to it in 1911 and 1912—there is an important exchange between Russell and [Emily Elizabeth Constance Jones](https://plato.stanford.edu/entries/emily-elizabeth-constance-jones/)—but then it is basically invisible from the start of World War I to the end of World War II. This pattern—Russell writes a super important paper but it only gets proper attention decades later—will repeat itself a bit, though this is the paradigm case.

And I was a little surprised at which papers are cited more often than "On Denoting". According to Google, at least, "What Is It Like to Be a Bat?" has twice its citations. "What Is It Like to Be a Bat?" is an important paper too, but still this was a bit of a shock.

<!--chapter:end:topic_comments/topic43.Rmd-->

```{r t44a}
jjj <- 44
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t44b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t44c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t44d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t44e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I’ve called this _chance_, and it does have a lot to do with chance, but a more perspicuous name would be _philosophers of science talk about probability_ because the big difference between this and [formal epistemology](#topic84) is that the folks here are philosophers of science, and the folks there are epistemologists. There are a lot of papers about chance in here—the second characteristic article is Michael Strevens responding to David Lewis’s revision of the “Principal Principle”—but there are also a lot of papers about probabilistic reasoning in scientific methodology.

Now, I was very surprised that the model could tell the difference between the philosophy of science articles and the epistemology articles. I have a reasonably strong “know-it-when-I-see-it” sense of how philosophy of probability gets divided up into philosophy of science and epistemology. But I wasn’t sure it was anything more than a set of gut reactions/prejudices, and I would not have expected at all that a machine-learning algorithm could replicate it. Yet here we are—the model really did do a very good job of splitting them up.

It’s not like it doesn’t think the two are connected. This topic and formal epistemology are two of the closest connected topics. But it does just enough to tell them apart. We can see this by looking at two David Lewis articles.

```{r lewis-bug}
individual_article("10.2307_2254396")
```

```{r lewis-sleeping}
individual_article("10.2307_3329230")
```

That's pretty good; in each case the model gives the intended topic 10 times the probability it gives the unintended topic. It's not perfect; I really don't know why it thinks the Sleeping Beauty paper is just about as likely to be a [Modality](#topic80) article as the Humean Supervenience article. But this is a quibble; I'm really happy that the model found this distinction. It made looking at the splits between Epistemology and Philosophy of Science much easier.

<!--chapter:end:topic_comments/topic44.Rmd-->

```{r t45a}
jjj <- 45
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t45b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t45c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t45d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t45e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This topic is a bit of a surprise in a couple of respects. 

First, it is a history topic on a single pretwentieth century figure who is not Kant. When I built a bunch of sixty topic models, almost all of them had a Kant topic, and almost none of them had a topic on any other single figure. Occasionally they would get two such topics by splitting Plato and Aristotle, but that was uncommon. But here we get a topic on Hume. That's unusual.

Second, the model decided it really really wanted to put work on coherence measures in here and not in with any of the other topics on logic or probability. I have no idea why it wanted to do this. Indeed, one of my motivations for bumping up the number of topics, and on running extra refinements on the model, was to give the model a chance to see the error of its ways. And it worked a little - there are many fewer formal epistemology articles in here than there originally were. But as you can see from the highly cited list, there are still some. This feels just like a bug, but I ran out of tools to squash it with.

There isn't a drop in this topic after World War II as there is with other topics in early modern philosophy. This is, I suspect, because Hume wasn't that big a topic before the war. It wasn't that there was no work on Hume. Keynes and Sraffa, for example, published their important argument that Hume was the author of the _Abstract_ in 1938 [@HumeAbstract]. But while it was reviewed in various journals, it didn't get much sustained attention in the philosophy journals until much more recently.

```{r graph-time-categories-t45, fig.cap = "Weighted frequency of articles in four topics in early modern philosophy.", fig.height = 5, fig.alt = alt_text}
  weight_ratio_graph <- ggplot(filter(weight_ratio, topic == 21 | topic == 31 | topic == 32 | topic == 45), aes(x = year, y = y, color=topic, group=topic))
  weight_ratio_graph + 
    spaghettistyle +
    coord_cartesian(xlim = c(1877, 2013), ylim = c(0, 0.03)) +
    scale_colour_discrete(labels = c(the_categories$subject[21], the_categories$subject[31], the_categories$subject[32], the_categories$subject[45])) + 
    theme(legend.title = element_blank(), legend.position = "right") +
    geom_point(size = 1, alpha = .5) +
    geom_smooth(se = F, method = "loess", size = .5, formula = "y ~ x") +
    labs(x = element_blank(), y = "Weighted frequency of articles")
  
alt_text <- "A scatterplot, with trendlines, of the topics early modern, social contract theory, Kant, and Hume. In the early years, Hume is the smallest by a long way. But the other three fall a lot, and Hume gently rises, so that in recent years social contract is by far the smallest, and the other three are similar in size."
```

One can see from this graph of the four early modern topics that Hume is the only one that really rises. The generic early modern topic maybe edges up a bit from the start to 1920, but there is a lot of noise. And social contract theory has a Rawls-inspired arrest of its fall. But otherwise the other three are either steady or falling, while Hume sees a mild rise.

Note that I’ve cut off this graph at 3 percent to make it clearer. There are a few points not in Hume) that are not shown. But these points are taken into account in building the trendlines.


<!--chapter:end:topic_comments/topic45.Rmd-->

```{r t46a}
jjj <- 46
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t46b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t46c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t46d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t46e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This looks like a small topic, but that's largely because it gets squeezed between so many other topics. It ends up as a natural philosophy of science topic, though most of the topics around it are in metaphysics. The weighted number of articles is so much higher than the raw number because so many papers in other topics are also in large part about laws.

For instance, papers about explanation in a broadly Hempelian tradition are often classified as being about [explanation](#topic61), even if they are in no small part about laws as well. See, for example, this paper which is very squarely in the Hempelian tradition.

```{r hempel-history}
individual_article("10.2307_2017635")
```

And some papers about the necessity of laws get classified as either papers about [modality](#topic80) or, less commonly, [chemistry](#topic30).

```{r bird-necessity-laws}
individual_article("10.2307_3329211")
```

Some papers that are largely about laws are also largely about [chances](#topic44) and get put there.

```{r lange-chance}
individual_article("10.2307_3873472")
```

Many papers that are about laws are also about causation and other topics.

```{r bvf-laws}
individual_article("10.2307_2107781")
```

And, of course, there are plenty of papers about laws in the topics on the special science.

```{r brandon-biology-laws}
individual_article("10.2307_188424")
```

The same does happen in the other direction. Some papers that are about several of these topics do get classified (barely) in the laws of nature topic.

```{r times-arrow}
individual_article("10.2307_2215339")
```

But mostly it's the other way around. And that makes sense. As interesting as laws of nature are as a topic in their own right, a huge part of their philosophical interest comes from the role they play in clarifying other things we care about. And that's what the model reflects.

<!--chapter:end:topic_comments/topic46.Rmd-->

```{r t47a}
jjj <- 47
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t47b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t47c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t47d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t47e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

One can see from the facet graphs that perception has been a much bigger focus of some journals (e.g., _Proceedings of the Aristotelian Society_ and _Philosophy and Phenomenological Research_) than others (e.g., _Journal of Philosophy_ and _Analysis_). This sounds plausible to me, though it wasn't something I had realized until running the study.

Perception has been a recurrent interest of philosophers throughout this period. It feels like there is something distinctive about this; very few topics have so much interest across such a long time. Here's one way to test whether that is true. If something is discussed a lot over a long time, it should do well on a maximin measure—it should have a high lower bound. The data here is a bit noisy to trust that measure though; some wide-ranging topics might have a single down year. Instead I'll use the following measure.

For each topic-year pair, I'll look at what proportion of the articles in that year are in that topic. (Using weighted sums here, not raw sums.) Then within each topic I'll rank the years from highest to lowest proportion. Then in those lists I'll find the one hundredth highest (i.e., the thirty-ninth lowest) proportion. This should give us a pretty good breadth measure while filtering out some noise. And here is what we get if we rank the topics that way.

```{r breadth-measure}
breadth_measure <- weight_ratio %>%
  group_by(topic) %>%
  mutate(rank = rank(-y)) %>%
  filter(rank > 99) %>%
  top_n(1, y) %>%
  ungroup() %>%
  arrange(-y) %>%
  mutate(topic = as.numeric(topic)) %>%
  left_join(the_categories, by = "topic") %>%
  select(subject, y)

kable(breadth_measure, col.names = c("Subject", "Proportion"), caption = "One hundredth highest proportion for each topic.") 
```

And we get the result that perception is indeed a topic of continuing interest. The only ones ahead of it are [ordinary language philosophy](#topic24), which is a style as much as a topic, and [methodology of science](#topic26), which is obviously also something of continuiting interest to philosophers.

Most of the others are not surprising, except perhaps for [life and value](#topic03). Part of what this shows is that that topic isn't quite as closely tied to [idealism](#topic02) as it looks at first; it persists long after idealism falls away.


<!--chapter:end:topic_comments/topic47.Rmd-->

```{r t48a}
jjj <- 48
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t48b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t48c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t48d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t48e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I've called this _intention_, but I might have been less misleading to call it _Bratman studies_. If you're looking at work in English in the last few decades on intention, there are two people who seem like they can't be left out: Michael Bratman, and Elizabeth Anscombe. And this topic certainly doesn't leave out Bratman—as can be seen from the tables above. But it does leave out Anscombe.

Of course, it wasn't asked to place Anscombe's book _Intention_; we're only doing journals. But there are a couple of articles that one might have hoped would be grouped together with these pieces.

```{r anscombe-intention-pas}
individual_article("10.2307_4544583")
```

```{r anscombe-brute-facts-analysis}
individual_article("10.2307_3326788")
```

In both cases it thinks about putting the article in this topic, but decides against it. That's too bad. I'll leave it for readers to decide whether putting such a sharp gap between Bratman and Anscombe's work is a sign the model doesn't understand philosophy, or that it understands it all too well.

Although there are some articles about Anscombe in this topic—just type 'Anscombe' into the search box above and it will return a few—the model puts none of her own articles in here. In general it doesn't really feel like the model knows what to do with Anscombe. Here are the articles of hers that it analyzes, along with the topic they most probably ended up in and the probability that they are in that topic.

```{r anscombe-author-table}
author_dt(c("G. E. M. Anscombe"), "G. E. M. Anscombe", "anscombe-author-table")
```

There are a couple of articles that it is reasonably confident about; it knows that papers with "Anselm" in the title are going to have to do with the ontological argument, but mostly it isn't very sure. 

This topic also finds some of the articles that confused the algorithm on every single run of the model—articles about Freud. This is a tricky topic for the model because the Freud articles are so distinctive that it wants to put them together, yet they are so few that they never make a topic of their own. Over a bunch of model runs I saw the algorithm try putting the Freud articles together with just about anything one could think of. Here they ended up with intention. Don't think too hard about why it might have done that; it feels completely random. The good news from my perspective is that for purposes of categorizing the topic I didn't have to worry, since both Bratman-inspired work on intention, and Freudian influenced-philosophy both feel like philosophy of mind.

<!--chapter:end:topic_comments/topic48.Rmd-->

```{r t49a}
jjj <- 49
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t49b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t49c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t49d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t49e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Given that this topic covers virtue ethics, discussions of individual virtues, and emotivism, I was surprised to see how low it came in. I was particularly  surprised to see so little life in the _Journal of Philosophy_ graph and not even much in the _Philosophy and Public Affairs_ graph. The split between ethics and political philosophy is very visible here

A slight surprise is that "Stevenson" turns up as a keyword. It isn't surprising that this is a keyword at all; [Charles Leslie Stevenson](https://plato.stanford.edu/entries/stevenson/) is one of the most important figures in midcentury philosophy. It's rather that it isn't entirely clear why the model put the work that engages with Stevenson here rather than somewhere else.

Part of what makes this odd is that the model only puts one of Stevenson's own journal articles in this topic.

```{r stevenson-articles}
author_kable(c("Charles L. Stevenson", "Charles Leslie Stevenson"), "Charles Leslie Stevenson")
```

Stevenson's own most important work, the book _Ethics and Language_, is not part of this study. But we can look at the works that engage with that book. But it's good to look directly at the raw data. Which articles use the word _Stevenson_ most often? And do those articles mean to refer to Charles Leslie Stevenson?

```{r articles-about-stevenson}
articles_with_word("stevenson")
```

The two articles by Brown are about _Adlai_ Stevenson, and the exchange between Leslie Stevenson and Ralph Walker is about Leslie Stevenson, but the rest I believe are about Charles Leslie Stevenson. And they naturally spread across a range of topics.

I think that part of what's happening here is that Stevenson's work is so wide-ranging that the model doesn't feel comfortable putting it into any one topic. Look, for instance, at how low the numbers are in the table of Stevenson's work. It somehow left the word _stevenson_ in this model, but that's balanced out by the fact that the words Stevenson himself used are mostly in other topics.

<!--chapter:end:topic_comments/topic49.Rmd-->

```{r t50a}
jjj <- 50
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t50b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t50c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t50d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t50e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This differs from the earlier topic on [classical space and time](#topic20) in two respects.

1. It is primarily about relativistic, as opposed to classical, physics.
2. It is very much philosophy of science, as opposed to metaphysics. This shows up in the characteristic articles, which are all in philosophy of science journals.

Much to my surprise, this topic featured some of the articles the model was most confident about. There are six articles that the model gives a probability greater than 0.9 of being in this topic. No other topic features more than four such articles; and only four other topics feature more than two. This was surprising given how much overlap this topic has with [classical space and time](#topic20). But it turns out that the language philosophers of physics use to talk about space and time is (or more importantly was) rather different from the language metaphysicians use. And that was enough for the model to tell them apart.

<!--chapter:end:topic_comments/topic50.Rmd-->

```{r t51a}
jjj <- 51
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t51b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t51c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t51d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t51e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

When I did a new model run, one of the first things I checked was whether the model had found the philosophy of mathematics topic. And the quickest way to check for that was whether it had clearly put the two big Benacerraf papers in the same topic. Most models passed this test, but some of them did so less clearly than others. This model run just passes the test. Here are the probability distributions over the ninety topics for the two articles.^[Remember that these tables are cropped at 2%; the model assigns probabilities to all 90 topics but I'm not showing them all here.]

```{r benacerraf-one}
individual_article("10.2307_2183530")
```

That makes sense—it is a philosophy of mathematics article, but it is largely about sets, and it can't escape the fact that it's written during the era of ordinary language philosophy. But the second table is a closer run thing.

```{r benacerraf-two}
individual_article("10.2307_2025075")
```

Again, this sort of makes sense—the article is about truth. But the model is much less sure how to classify this article, as evidenced by the number of topics that get a probability between 4.6 percent and 7.4 percent.

What the model is working with is a conception of philosophy of mathematics that is centered around two debates. One is the nature of infinity, the other is the nature of proof. That's not a terrible take on twentieth-century philosophy of mathematics, but it does mean that papers like "[Mathematical Truth](https://philpapers.org/rec/BENMT)" get treated as less than fully paradigmatic.

I mostly look at how big a topic is by eyeballing the twelve journal graph. But in this case that would be highly misleading. Because this topic is spread across many journals, and many years, those twelve lines look like they barely have a pulse. But the tables show that, depending on size is measured, this is the twenty-third or fourtieth biggest topic. So many other topics are concentrated in a small number of journals or times, and they make a bigger visual impact than topics like this that keep chugging along.

<!--chapter:end:topic_comments/topic51.Rmd-->

```{r t52a}
jjj <- 52
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t52b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t52c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t52d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t52e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Some model runs came up with a topic that was straightforwardly Rawls-studies. This model didn't quite do that. It has this topic on liberalism and democracy, and a later topic on [egalitarianism](#topic65). And narrowly focused Rawls work ends up split across both of those topics.

But both topics also include a lot more besides Rawls. This is clear just from the time distribution of the articles. Those handful of papers around 1920, followed by sustained engagement in the early 1940s, can't be about Rawls. Some of them are articles you might easily count as history of philosophy, such as papers on Aristotle, on Mill, or on the Federalist papers. 

As this suggests, there was traditionally more work in this topic that was at least a little applied; as the century progressed it moved more into the realm of high theory. The applied work here (which isn't applied by anything other than philosophical standards) is very focussed on Anglophone countries. Papers on other countries tend to end up in the [history and culture](#topic10) topic. So a paper like "[The Mystery of 1789](https://philpapers.org/rec/SEATMO-2)" is about the wonders of the US Constitution, not the overthrow of European feudalism [@Seagle1948].

A lot of the papers here get picked up in the political theory literature, and from there they get huge citation counts. Tis ends up being one of the highest cited topics in the whole study.

<!--chapter:end:topic_comments/topic52.Rmd-->

```{r t53a}
jjj <- 53
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t53b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t53c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t53d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t53e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I needed to take the model up to ninety topics to make the needed distinctions in a bunch of subdisciplines. But I fear that the model would be more enlightening with respect to ethics with a smaller number of topics. Some of the ethics topics end up being distinguished by which near synonym the author decided to use. And this feels like one of them.

The formula I used for the "characteristic articles" weighs length as well as probability. The list of articles that have the highest probability of being in this topic make it look like it concerns very in-house debates.

```{r duties-high-probability}
t <- relabeled_articles %>%
  filter(topic == 53) %>%
  arrange(-gamma) %>%
  slice(1:10) %>%
  select(citation)

for (jj in 1:10){
  cat(jj,". ", t$citation[jj], " \n", sep="")
}
```

And this makes sense. In terms of subject matter, this topic overlaps considerably with other topics. What distinguishes it is not subject matter, but word choice. And discussion notes tend to mirror the language of the articles they are discussing.

We get a better sense of what's happening by looking not here but at the larger ethics category that I'll discuss in later chapters.

<!--chapter:end:topic_comments/topic53.Rmd-->

```{r t54a}
jjj <- 54
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t54b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t54c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t54d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t54e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

For a topic that I thought was one of the central, even dominant, topics in twentieth century metaphysics, this ended up having a smaller presence in the data than I expected. It's not tiny, either thirty-sixty or twenty-seventy depending on which measurement is used. But the graphs don't jump up nearly as much as I expected.

Partially this is because it's kind of hard to get the boundaries of discussions of causation right. Here are three things that different models struggled with.

1. A lot of models wanted to separate out pre-Lewisian work on causation, often centered around Mackie, from Lewisian work. This model decided (correctly I think) that they belong together.
2. The boundary between work on causation, and work on [laws](#topic46) is pretty blurry. The boundary between work on causation and work on [explanation](#topic61) might be even blurrier.
3. A lot of recent work on causal models looks distinctive enough that it can get carved off, as it does here into a topic on [models](#topic88).

This model is very confident that Lewis's two _Journal of Philosophy_ articles belong either in causation or in laws, but it's not completely sold on them being causation articles.

```{r lewis-causation-one}
individual_article("10.2307_2025310")
```

```{r lewis-causation-two}
individual_article("10.2307_2678389")
```

And this isn't a shortcoming of the models, I think. These topics really do blend together, and it's hard to say where one starts and the other ends. One advantage of using a probabilistic model like this is that blurred boundaries can be modeled as intermediate probabilities, and it is still possible to roughly measure the size of each topic without making arbitrary distinctions. The three problems above still remain, and one could put the boundary between causation and laws, explanation and models at very different places. But I think this is a decent picture of the size of discussions of causation over time.

Note one other thing about this topic. There is an [early Russell paper](https://philpapers.org/rec/RUSOTN-7) that is (just barely) in the topic.

```{r russell-causation}
individual_article("10.2307_4543833")
```

But it barely shows up in the graphs. This is such a common phenomenon; topics that are huge parts of late twentieth-century philosophy are also the topic of early Russell papers. But the Russell papers made next to no impact in these journals at the time he wrote them.

<!--chapter:end:topic_comments/topic54.Rmd-->

```{r t55a}
jjj <- 55
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t55b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t55c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t55d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t55e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

So this is a slightly unfortunate choice that the model made. It notices that there are papers that are about arguments, focusing on things like question begging and circularity. Then it identifies this as a topic, and everywhere someone talks about arguments, it says that maybe that paper should be in this topic. But a lot of philosophy papers talk about arguments! If all papers talked about arguments to the same extent, then the model would zero it all out. But that's not what happened. It doesn't even happen to equal amounts over years. This can be seen just by looking at word frequency graphs.

```{r t55-frequency-graphs, fig.cap = "Words about arguments.", fig.alt = alt_text}
word_frequency_graphs(c("argument", "arguments","premise", "conclusion"))
alt_text <- word_frequency_graph_alt_text(c("argument", "arguments","premise", "conclusion"))
```

It's interesting that _premise_ and _conclusion_ don't change their frequency over time. But there is a rapid acceleration in how often the word _argument_ is used. And, though I'm not sure the graph makes this clear, _arguments_ also increases too. And note that the word frequency of _argument_ tracks really closely the weighted sum of the topic.

The 122 articles that are primarily in the topic are a somewhat odd group. There are some, as can be seen from the top of the charactistic articles list, that are really about things like circularity. Further down, there are articles that are particularly about arguments for incompatibilism and, especially, arguments for dualism.

This topic has, by a lot, the biggest gap between its raw sum and weighted sum. This is because there are so many articles where the model gives a small, but far from negligible, probability to the article being in the topic. Here's one way to see that. For each topic, we can ask for how many articles is it the _n_-th most probable topic. We've done that already for _n_ = 1; that's what the raw count reports. But for this topic the values for _n_ between 2 and 10 are a little eye popping.

```{r rank-of-arguments-topic}
t <- relabeled_gamma %>%
  group_by(document) %>%
  mutate(r = rank(-gamma)) %>%
  filter(topic == 55) %>%
  ungroup() %>%
  group_by(r) %>%
  tally() %>%
  mutate(r = round(r,0)) %>%
  slice(1:10)

kable(t, 
      col.names = c("Rank", "Number of Articles"), 
      align=c("c", "c"),
      caption = "The number of articles that have arguments as the n-th highest ranked topic."
      ) %>%
  kable_styling(full_width = F)
```

For comparison, here's what that table looks like for [causation](#topic54.)

```{r rank-of-causation-topic}
t <- relabeled_gamma %>%
  group_by(document) %>%
  mutate(r = rank(-gamma)) %>%
  filter(topic == 54) %>%
  ungroup() %>%
  group_by(r) %>%
  tally() %>%
  mutate(r = round(r,0)) %>%
  slice(1:10)

kable(t, 
      col.names = c("Rank", "Number of Articles"), 
      align=c("c", "c"),
      caption = "The number of articles that have causation as the n-th highest ranked topic."
      ) %>%
  kable_styling(full_width = F)
```

Here are some of these articles that the model thinks are second most likely to be in arguments. (I've restricted this list to articles at least twenty pages long.)

```{r rank-of-arguments-topic-second-best}
t <- relabeled_gamma %>%
  group_by(document) %>%
  mutate(r = rank(-gamma)) %>%
  filter(topic == 55, r == 2) %>%
  ungroup() %>%
  arrange(-gamma) %>%
  select(-year) %>%
  inner_join(relabeled_articles, by = "document") %>%
  filter(length.x > 19) %>%
  select(citation, year, gamma.x)

datatable(t,           
          colnames = c("Article", "Year", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:2)),
                         pageLength = 10
                         ),
          caption = htmltools::tags$caption(paste0("Articles that have arguments as second most likely topic."), style = "font-weight: bold")
    )%>%
      formatSignif('gamma.x',4) %>%
      formatStyle(1:3,`text-align` = 'left')
```

Probably if the model hadn't landed on this topic, it would have been even clearer that some articles were philosophy of religion articles or ancient philosophy articles.

And this is why this topic is ultimately one I regret having. If I did this whole project over again, I would steer away from models that select topics based on tools, and towards ones that focus more centrally on particular philosophical subject matters.

<!--chapter:end:topic_comments/topic55.Rmd-->

```{r t56a}
jjj <- 56
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t56b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t56c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t56d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t56e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is largely about theory testing and measures of confirmation. It's striking that Deborah Mayo's articles, mostly written from a non-Bayesian perspective, are the most characteristic articles, although the topic as a whole is extremely Bayesian. I think what's happening is that there are a lot of parts of philosophy where Bayesian tools are used and discussed. So when the model sees Bayesian discussions going on, it gets nervous about exactly where it should be placing the articles. But non-Bayesian mathematical accounts of evidence and confirmation smoothly slot into this topic.

This topic is really concentrated in the philosophy of science journals. From a contemporary perspective, this seems surprising. Questions about evidence, observation, and confirmation seem central to epistemology in general. But the powers that be who ran the generalist journals over the twentieth century did not agree. Even _Analysis_, which is not averse to publishing articles on probability, barely shows up here.

Finally, note the shape of the overall graph. After being very low through 1940, it seems to be on a steady upward trajectory. The modal publication year is the last year of the study, 2013. We are about to see a lot of topics that take off around (or just after) the middle of the twentieth century. But most of them are going to level off by the 1980s, and that's not what is seen here.

<!--chapter:end:topic_comments/topic56.Rmd-->

```{r t57a}
jjj <- 57
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t57b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t57c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t57d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t57e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I'm going to say a bit more in chapter \@ref(sortingchapter) about why this is in ethics, because it turned out to be one of the hardest topics to classify. Note for now that the keywords suggest that the focus of this topic is on the utility end of the probability-and-utility model of choice, and that suggests it naturally fits with value theory.

The graph that runs through each journal separately shows that this topic is more prevalent in _Ethics_ than any other journal, which is consistent with classifying the topic in ethics. _Journal of Philosophy_ started taking an interest in the late 1970s, but it's hard to see a trend after this. Some years there is a lot of decision theory work in _Analysis_, but some years there is not. And there was a flood of work that was mostly centered around the [Pasadena game](https://philpapers.org/rec/NOVVE) in _Mind_ in the 2000s. 

But mostly there is a little less here than I expected. I think part of what's happened is that some work I was mentally classifying as decision theory the model instead treated as [formal epistemology](#topic84). And that's why this is mostly sitting at or even under 1 percent frequency.

<!--chapter:end:topic_comments/topic57.Rmd-->

```{r t58a}
jjj <- 58
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t58b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t58c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t58d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t58e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is another topic that came in a little smaller than I expected, perhaps in part because the model was construing it slightly more narrowly than I was. I certainly would have guessed that articles about minds and machines would be much more than 0.6–0.8 percent of the articles.

The explanation here is that the topic got squeezed from all sides. There are topics about [physicalism](#topic09), [conceivability arguments](#topic55), [wide content](#topic85) and [cognitive science](#topic87). What's left here are articles on a very specific range of arguments about whether minds are best thought of as machines. Put that way, the real surprise is that it hasn't faded more. There is a small downward trajectory towards the right-hand edge, but only a small one. Issues that Turing, Gödel, Searle and others raised continue to fascinate philosophers.

<!--chapter:end:topic_comments/topic58.Rmd-->

```{r t59a}
jjj <- 59
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t59b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t59c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t59d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t59e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The graphs by journal do not make this look like a particularly big topic, but the numbers at the top show, it's the eighth biggest by weighted count. And that's not surprising—I would have guessed that theories of truth would be a big deal. And so they are, except they aren't a particularly big part of any journal in any year except for _Mind_ in the early 2000s.

The spike in the late 1900s and early 1910s is due to an interest in theories of truth in pragmatist and voluntarist philosophical theories. Some of these papers involved early contributions from Susan Stebbing, who would go on to contribute many papers to the British journals we're looking at.

```{r stebbing}
author_dt(c("L. Susan Stebbing", "L. S. Stebbing"), "L. Susan Stebbing", "stebbing")
```

The only one of these 27 articles that is actually in this topic is the earliest of them, a two page note critical of Schiller's defences of pragmatism. But an interest in how different theories think about truth runs through a lot of her work. And the model picks this up; a lot of these articles are as much about truth as anything else. For instance, her Aristotelian Society article on Bergson (which was extracted from her MA thesis!) cuts across a number of the topics in this model.

```{r stebbing-bergson}
individual_article("10.2307_4543842")
```

Unlike almost every other topic with a notable presence in prewar philosophy, truth was undergoing a resurgence towards the end of the period I'm looking at. It's moved from being a metaphysical (or perhaps epistemological) concern to a more distinctively logical one. That is, most of those articles on the right of the graph are about paradoxes, and about how and whether classical logic should be revised to handle them. That's a somewhat different subject matter to what, say, Stebbing and Schiller were debating, but I think the model got it right in linking them together. 

And between them there are a lot of articles on Tarskian and Davidsonian theories of truth. A perhaps surprising result of this is that _snow_ is a keyword for the topic. I'm a little surprised that _white_ doesn't come with it.

<!--chapter:end:topic_comments/topic59.Rmd-->

```{r t60a}
jjj <- 60
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t60b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t60c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t60d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t60e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This feels like a narrow topic to me, but the numbers suggest it is almost exactly the median size of topic. I think this is because the _Journal of Philosophy_ published a lot of articles on indeterminacy of translation from the 1950s all the way through at least the 1990s. Indeed, 111 of the 342 articles in this topic are from the _Journal of Philosophy_.

Within philosophy of language, this topic feels like a bridge between the Wittgensteinian work done before and immediately after the war, and the work post-Montague that is more continuous with linguistics. There is some level of formalism in the papers here, but nothing like the Montagovian works.

The core figure in this topic is clearly Quine, but the topic gets a second burst of life in the mid-1970s. That slowly fades away, but never quite ends.

And it is possible that Robbie Williams's work will lead to a reversal of the downward trend. Indeed, one could squint and almost make out that it had already reversed the trend by the late 2000s. Here are the papers Robbie had in the data-set as of 2013, though he has published much more in these journals since then.

```{r robbie-williams-papers}
author_kable(c("J. R. G. Williams", "J. Robert G. Williams"), "Robbie Williams")
```


<!--chapter:end:topic_comments/topic60.Rmd-->

```{r t61a}
jjj <- 61
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t61b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t61c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t61d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t61e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I've called this _explanation_, but that's potentially misleading. It's really focussed on noncausal approaches to explanation. I suspect that's less because noncausal theories of explanation are distictive, and more because there are so many other topics that discuss causation.

This topic ends up being somewhat squeezed, so the numbers here understate the importance of explanation as a topic. Some papers you might intuitively classify as being about explanation end up in [causation](#topic55). Others end up in [laws](#topic46). Yet others end up in [theories and realism](#topic67). So the numbers here are more of a lower bound for the importance of explanation than anything else.

That said, the upward trend here is a bit interesting to me. I wouldn't have expected that just from the topic. Part of the story is that this topic also includes papers on reduction (at least if they didn't get classified in laws or in theories and realism), and there was a bit of a regrowth of interest in that in the 2000s and early 2010s.

<!--chapter:end:topic_comments/topic61.Rmd-->

```{r t62a}
jjj <- 62
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t62b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t62c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t62d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t62e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Given how big a topic this was when I was a graduate student, I was very surprised that it wasn't bigger than it turned out to be. But a look at the overall graph explains why I got this wrong. The high water mark for this topic was from about 1985–1995. When I was in graduate school, it really was all over the journals that we were reading. We couldn't have predicted that it would fall from nearly 2 percent of the articles to under 1 percent.

One of the upsides of having a computer model analyze the data rather than going with one's own impressions is that it reduces the impact of having an idiosyncratic set of experiences in cases like this one.

<!--chapter:end:topic_comments/topic62.Rmd-->

```{r t63a}
jjj <- 63
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t63b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t63c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t63d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t63e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This graph represents a story that suggests I was in the right place at the right time to see it first-hand. It's a very distinctively shaped graph. I don't think any of the other eighty-nine have a peak, then a steep dip, then a second wave, quite like this.

The story behind it is I think fairly well known by now. Austin developed speech act theory in the 1950s, but the key ideas didn't really get picked up until well into the 1960s. Then the story was developed more by Searle and others, and for a while it looked like a really promising approach to thinking about a number of puzzling features of language. But in part because the early promise didn't seem to be being realized and in part because it was overtaken by Kripkean and Montagovian approaches, it started to feel like a superseded research program. (And maybe there is a connection here to the [linguistics wars](https://en.wikipedia.org/wiki/Linguistics_wars) [@Harris1995], but I don't really know how much they impacted the philosophy journals at all.) So by the 1980s and early 1990s it was looking like yet another midcentury research program that had once been quite prominent in the journals, but now wasn't.

Except at Monash it didn't look like that at all. Rather, it looked like speech act theory provided the key tools for developing and explaining a really interesting, and provocative, theory of communication. And one of our new junior faculty members was playing a key role in building that theory. It was an interesting enough theory that people were talking about illocutionary and perlocutionary effects in between planning parties in the student union. It felt like if this caught on, then speech act theory wasn't a superseded research program at all but something that was essential to understand in order to understand the role of language in human society.

It caught on.

That new junior faculty member is now the [Knightbridge Professor of Philosophy](https://www.newn.cam.ac.uk/person/professor-rae-langton/) at Cambridge, and her work, along with the work of other important feminist philosophers, has transformed our understanding of speech acts. And it is responsible for one of the only second acts in analytic philosophy, as speech act theory itself went from being on the path to obsolescence to a central place in philosophical theorizing.

<!--chapter:end:topic_comments/topic63.Rmd-->

```{r t64a}
jjj <- 64
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t64b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t64c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t64d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t64e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The model is carving things up a little more finely than may be ideal here, so let's put some stuff back together. This topic obviously has a lot to do with [denoting](#topic43) and with [belief ascriptions](#topic72). And it's useful to see all three of these on a single graph.

```{r three-language-topics, fig.height = 5, fig.cap= "Relative frequency of denoting, sense and reference, and belief ascriptions", fig.alt = alt_text}
frb <- weight_ratio %>%
  filter(topic == 43 | topic == 64 | topic == 72) 

ggplot(frb, mapping = aes(x = year, y = y, color = topic)) + 
  spaghettistyle +
  geom_point(alpha = 0.5) + 
  geom_smooth(se = F, size = 0.5, method = "loess", formula = "y ~ x") +
  labs(x = element_blank(), y = "Weighted proportion of articles", title = "Three Classic Topics") +
  scale_color_discrete(name = "Topics", labels = c("Denoting", "Sense and reference", "Belief ascriptions"))

alt_text <- "The graphs for three topics—denoting, sense and reference, and belief ascriptions—on one chart. The shape of the graph is described in the text below."
```

There is a tiny amount of discussion of these three, mostly focussed on descriptions, up until World War II. Then they all start to take off. But descriptions as such starts to fade quite early, with its peak being the late 1970s. Some of that is just substitution—there is a lot of discussion of how descriptions work in articles on sense and reference, and articles on belief ascriptions. But the sum seems to be headed a little downwards as we move into the twenty-first century, and I'd expect that gentle trend continues through 2020.

Even at their peak, these topics don't take up as much space in the journals as they take up in philosophers' self-representation. At most these topics are coming to about 3–4 ercent of the discussion over any sustained period of time. But during that time they were routinely taking up more than 3—4 percent of space in graduate education. (For instance, it was not unusual to have compulsory first-year seminars largely about these topics.) My feeling is that the journals got this right and the departments got this wrong, and 3–4 percent was a sensible proportion of space to devote to these topics. That said, it was surprising to me that this never went much higher than that; it certainly felt at times like more than 3–4 percent of the discussion was devoted to denotation, sense, reference and opaque contexts.

<!--chapter:end:topic_comments/topic64.Rmd-->

```{r t65a}
jjj <- 65
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t65b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t65c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t65d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t65e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

In the early 1970s two things happened almost simultaneously. One was that Rawls published _A Theory of Justice_. The other was that _Philosophy and Public Affairs_ launched. The effect of these two things was to push questions about distributive justice onto the pages of philosophy journals in ways that they had never been before. There are a lot of topics that really take off in the early 1970s, but none have as steep a rise as this one.

Note also how widely cited the articles in this topic are. The topic has only 1.2 percent of the articles, but nearly 4 percent of the six hundred articles I've called "highly cited". There are a few reasons for this. One is that some of these are genuinely great articles, and they fully deserve the attention they've received. Another is that citation practices have been changing over time, and the result is that topics with more recent articles in them are going to get more citations. And a third is that philosophers tend to be relatively stingy with citations, so articles that get picked up outside philosophy tend to do very well on citation counts. We see this in cases where philosophy articles cross over into psychology/cognitive science, or into linguistics, or, as here, into political science.

<!--chapter:end:topic_comments/topic65.Rmd-->

```{r t66a}
jjj <- 66
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t66b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t66c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t66d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t66e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

A couple of things about this topic surprised me, though I think this says more about my ignorance than about the field.

One was that the peak of this topic comes considerably earlier than I expected. I had thought of this as a very modern topic, but we see means for it coming in the 1990s and even 1980s. These topics are sort of chronologically ordered, there are still twenty-four topics to go. Now to be fair, in other models the quantum physics topic did come much later in the sequence. But I don't think I need a fancy model to see that (a) there are topics on quantum physics going right back to the founding of _Philosophy of Science_, and (b) there is a little less quantum physics in the philosophy of science journals now than in the 1980s and 1990s.

The other striking thing about this topic is how Michigan centered it is. Every time I ran one of these models, one of the things I checked for was what percentage of the characteristic articles in the quantum physics topic had a Michigan author. And often the percentage was well over 50 percent. This model run wasn't quite that high, but there are four papers with Michigan authors on the first ten.

<!--chapter:end:topic_comments/topic66.Rmd-->

```{r t67a}
jjj <- 67
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t67b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t67c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t67d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t67e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This study matches up with my experiences surpisingly well. When I was a grad student back in the 1990s, I felt that work on scientific realism was all around. This was in part because I was reading a lot of philosophy of economics, but it also felt like part of the background that everyone should know about. Since then it had seemed less and less relevant to what was going on around me. I had always thought this said more about the changes in my environment than in philosophy at large. I assumed, that is, that there were still just as many discussions about scientific realism going on, but changes in my reading habits meant I wasn't seeing them as much.

Nope! This topic really has fallen away quite a bit from the 1990s. Or, at least, it has really fallen away in these twelve journals. I expect a study that covered a broader range of journals would tell a different story. But scientific realism has become a smaller topic in the twelve journals I'm looking at.

One doesn't need a fancy model to see this. It can be seen in the raw word counts. This is helpful because it lets us cross test the output of the fancy model. It's not helpful to look at how often the word _realism_ is used, since this has so many different meanings. But we can look at some of the names most associated with this debate to get a sense of how often the debate was taking place in the pages of these journals.^[Note that the Y-axis measures the frequency of the words across all words in the JSTOR dataset, not just amongst the words that I'm including. So the denominator here includes the stop words, the LaTeX words, the bibliographic words, etc. This will be true whenever I do one of these graphs throughout the book.]

```{r realism-names, fig.height = 5, fig.cap = "Word frequency for four famous names in philosophy of science.", fig.alt = alt_text}
word_frequency_graphs(c("laudan", "kuhn", "boyd", "lakatos"))
alt_text <- word_frequency_graph_alt_text(c("laudan", "kuhn", "boyd", "lakatos"))
```

There is a lot of year-to-year variation there, but the basic pattern that things rise pretty quickly through the 1980s, and then fall away just as quickly after that, seems fairly clear. In the 1990s when I was reading this stuff, it was a debate running out of steam. 

As one other aside, I'm surprised that the raw counts for _Kuhn_ and _Boyd_ are so low. For one thing, those names belong to more people than their most famous owners. For another I thought _Boyd_ might have more of a boost from being at Cornell, even in the _Philosophical Review_. For another, _Kuhn_ is much lower than other similarly famous figures of the third quarter of the twentieth century.

```{r mid-century-american-names, fig.height = 5, fig.cap = "Word frequency for names of four famous philosophers.", fig.alt = alt_text}
word_frequency_graphs(c("quine", "kuhn", "rawls", "kripke"))
alt_text <- word_frequency_graph_alt_text(c("quine", "kuhn", "rawls", "kripke"))

```

There are years when the word _Rawls_ is being used about once every thousand words. Even accounting for the stop words JSTOR excludes, that's once every two to three pages over all twelve journals! But _Kuhn_ only once gets above once every five thousand words, and then only barely.

<!--chapter:end:topic_comments/topic67.Rmd-->

```{r t68a}
jjj <- 68
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t68b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t68c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t68d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t68e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The smallest of our topics, and a sign that at least in philosophy of biology, the model had to carve things really finely to come up with ninety topics. One of the big challenges with setting the number of topics is that a choice that ends up with a relatively natural division in some fields can produce a very coarse-grained, or, as in this case, a very fine-grained division in other fields. This probably could have been put together with work on [mechanisms](#topic18) without a great deal of loss.

The thing that jumps out to me about this topic is how much impact it had on the citation measures. There are only eighty-four articles in the topic, so you'd expect only one to two to show up in the highly cited articles, but instead we get five. This is a measure of the influence of Ruth Millikan on philosophy, since so much of this topic is organized around her work.

<!--chapter:end:topic_comments/topic68.Rmd-->

```{r t69a}
jjj <- 69
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t69b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t69c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t69d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t69e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

At first the graph for this surprised me. I didn't think work on feminist philosophy in these twelve journals peaked in the 1970s and had been on a downward trajectory since. But, as with a few other topics, the key thing here is that the model does a better job of figuring out what debate people are talking about than it does at figuring out what side they are taking in it. For example, Moore's "[Refutaion of Idealism](https://philpapers.org/rec/MOOTRO)" ends up in [idealism](#topic02), and a lot of anti-feminist work ends up here.

In particular, there was a massive debate in Philosophy and Public Affairs during the 1970s about discrimination and affirmative action. Despite philosophers not having paid an enormous amount of attention to issues about equal opportunity before 1970, once affirmative action became a live possibility, it turned out that many philosophers were deeply committed to (a particular version of) equality of opportunity. And they were very keen to write about it. And that's where a lot of the numbers here come from.

Very little of the important work in feminist philosophy prior to 2013 took place in these twelve journals, which is why the numbers here are so low. If the [previous topic](#topic68) was a sign that we were carving things a little too finely, this topic is a sign that we've carved them just right. The amount of work in (or even engaging wth) feminist philosophy in these journals was low enough that when I ran these models with fewer than ninety topics, feminist philosophy typically got lumped in with other unrelated fields. I'm happy that this model found space for it on its own. But given the trends in the discipline, future iterations of this research project shouldn't have nearly as much problem finding work in feminist philosophy.

<!--chapter:end:topic_comments/topic69.Rmd-->

```{r t70a}
jjj <- 70
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t70b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t70c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t70d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t70e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is a small, and yet fairly disjunctive, topic. Most of the articles are on medical ethics, broadly construed. But there are two other topics that are lumped in here somewhat randomly.

One is that papers on trust ended up here. It's a little hard to know precisely what things trust should go with. It has connection to work on cooperation and hence (via Bratman), to work on intention.  It has connection to work on emotions.. It has connection to work on special obligations, e.g., to friends and family. What it doesn't really have is a particularly strong connection to medical ethics. Yet that's where the model decided to put some trust papers. And while they are a very small part of the topic, they make up a large part of its highly cited articles.

The other is that some of the Freud papers ended up here. The normal thing that happened with Freud papers in different models is that they would be semirandomly attached to some other topic. In this model they got split up, and then semirandomly attached to different topics. The result was that some of them are grouped with articles on [intention](#topic48), and some of them are here.

That's to say, this topic is a bit of a mess. For a lot of purposes, ninety topics worked reasonably well. Right here the model is saying it could have used a few more. There will always be bumps under the carpet in this kind of exercise.


<!--chapter:end:topic_comments/topic70.Rmd-->

```{r t71a}
jjj <- 71
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t71b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t71c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t71d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in this topic
temp_dt
```

```{r t71e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles in this topic. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Most model runs I did ended up with the counterpart of this topic being much larger. Usually one of two things happened.

One was that there was a topic that was really based around the inaugural issue of _Philosophy and Public Affairs_, and had both "A Defense of Abortion" and "Famine, Affluence and Morality" as cornerstones. That didn't happen here, as "Famine, Affluence and Morality", and the many papers it inspired, are put in with [population ethics](#topic83).

The other was that there was a topic that included everything that related to self-defense. And while this topic has some self-defense articles, it doesn't have many. There is only one article by Michael Otsuka, two by Jonathan Quong, only one directly on self-defense by Jeff McMahan, and none by Seth Lazar. Self-defense ends up getting a bit split in this model. Some of it is here, some under [responsibility](#topic35), and some under [war](#topic41).

The happy consequence of this is that we get a topic that really is all about Judith Jarvis Thomson's work. It's not about all of her work of course—that covers ever so many fields.

```{r thomson-articles}
# Burp
author_dt(c("Judith Jarvis", "Judith Jarvis Thomson"), "Judith Jarvis Thomson", "thomson-articles")
```

But this topic does include many of the papers on two of her signature contributions: her innovative defense of abortion and her discussions of the trolley problem. Her original paper on the trolley problem is not in this study, since it wasn't published in one of the twelve journals I'm looking at. But a lot of papers that follow up on it, including Thomson's own second thoughts in "Turning the Trolley" are here.

There is a very natural sense in which this topic is centered around Thomson's work. I don't have a formal analysis of this concept of "centering" I just used. But most topics are not centered around any one philosophers work. There are topics based around [Marx](#topic23), [Kant](#topic32), [Russell](#topic43), [Hume](#topic45), [Frege](#topic64), [Frankfurt](#topic77) and [Parfit](#topic83), as well as two that are arguably centered around Rawls—[Liberal Democracy](#topic52) and [Egalitarianism](#topic65). But there is no other case, I think, where any female philosopher is as central to as big an area of philosophical research as we see in this case. There are some women, such as Margaret MacDonald, who were important influences on the journals as editors. But I'm not sure any woman was more influential on the journals as a writer than Thomson.

<!--chapter:end:topic_comments/topic71.Rmd-->

```{r t72a}
jjj <- 72
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t72b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t72c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t72d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t72e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The model is carving things very finely at this point. There is a topic on [denoting](#topic43) and another on [sense and reference](#topic64). There is also a topic on [wide content](#topic85). And here there is a whole topic on belief reports. This is interesting because belief reports are interesting to philosophers primarily because of how they relate to names, descriptions, indexicals and natural-kind terms.

One funny thing to notice here is how little overlap there is between the extensive work on belief ascriptions, set out in this topic, and the work on knowledge ascriptions, which will come up [very soon](#topic74). To measure how much overlap between the topics there is, for each article, I calculated the minimum of the probability they are in this topic, and the probability that they are in the topic on [knowledge](#topic74) ascriptions. Here are the five articles that score highest by this measure.

```{r belief-knowledge-overlap-a}
reports <- relabeled_gamma %>%
  filter(topic == 72 | topic == 74) %>%
  group_by(document) %>%
  dplyr::summarise(m = min(gamma)) %>%
  arrange(-m) %>%
  slice(1:10)

individual_article(reports$document[1])
```

```{r belief-knowledge-overlap-b}
individual_article(reports$document[2])
```

```{r belief-knowledge-overlap-c}
individual_article(reports$document[3])
```

```{r belief-knowledge-overlap-d}
individual_article(reports$document[4])
```

```{r belief-knowledge-overlap-e}
individual_article(reports$document[5])
```

Except perhaps the Dretske, none of them really feel like they fit in this topic. Or, to put things more provocatively, somehow all this work on belief ascriptions didn't even have a huge influence on work on ascriptions of very similar propositional attitudes like knowing.

Since 2013 there has been, to my eyes, less work on topics like puzzling Pierre and Cicero/Tully. But it's notable that the last keyword in the list at the top, and the last of the five articles I just displayed, both concern lying. That has been a big topic since 2013, and if we extended the study in time, perhaps we would see this turn into a topic more centered around lies and less around puzzles about belief ascription.

<!--chapter:end:topic_comments/topic72.Rmd-->

```{r t73a}
jjj <- 73
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t73b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t73c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t73d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t73e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The story of philosophy of science over the past several decades was one of increasing specialization. First work on philosophy of science as a general category was replaced by work in the separate sciences, especially physics and biology. As I'll discuss in chapter \@ref(sortingchapter), the model thinks of work in philosophy of physics and philosophy of biology as having less in common with each other than they have with other areas of philosophy.

The existence of this topic is a sign of yet more specialization in recent years. The model sees work on thermodynamics as separate from other work in philosophy of physics. And this is separate from work on space and time, and from work on quantum mechanics.

It also is the first topic that has a fairly unambiguous upward trajectory through 2013. A lot of topics I've discussed recently, and indeed several I'll discuss in the next few sections, felt like they had reached a plateau that they were still holding into 2013. But this looks to me like it's going up. It will be interesting to look back in a few years and see if that's still the case.

<!--chapter:end:topic_comments/topic73.Rmd-->

```{r t74a}
jjj <- 74
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t74b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t74c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t74d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t74e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

It's perhaps a bit surprising that theory of knowledge—everything from scepticism to Gettier cases to testimony to closure principles to Williamsonian knowledge-first epistemology—is in just one topic. And that it's just 1.3 percentof all the articles.

In part this is because of how new a topic it is. There just isn't very much that feels like contemporary epistemology from before World War II. Of the 426 articles in this topic, only six of them are between World War II. And five of those feel like fairly borderline cases, where the model couldn't really make up its mind. There is one notable exception.

```{r testimony-article}
individual_article("10.2307_2249544")
```

I couldn't find any other references to **Dhirendon** Mohon Datta, but that's a typo. (Or a nonstandard transliteration.) **Dhirendra** Mohan Datta wrote a number of books. (The details about Datta that follow are from sources that I was pointed to in helpful conversations with Michael Bench-Capon, Bryce Huebner and Michael Kremer.)

He wrote [_The Philosophy of Mahatma Ghandi_](https://www.amazon.com/Philosophy-Mahatma-Gandhi-Dhirendra-Mohan/dp/0299010147) [@Datta1953], based on his time working with Ghandi. 

He also wrote [_The Six Ways of Knowing: A Critical Study of the Vedānta Theory of Knowing_](https://archive.org/details/TheSixWaysOfKnowing1960D.M.Datta/mode/2up) [@Datta1960], which overlaps with this paper. Indeed, on page 337 of the second edition of that book, he refers to "Testimony as a Method of Knowing" as his article. (Thanks to Michael Bench-Capon for spotting this.) I've linked to the second edition, which is the only one I could find online. If the first edition is like the second, it would be decades ahead of its time. (And the _Mind_ article suggests that that's true.)

And he coauthored a prominent textbook: [_An Introduction to Indian Philosophy_](https://www.amazon.com/dp/B01C2IHREY/ref=dp-kindle-redirect) [@ChatterjeeDatta].

When he retired, Datta's colleagues and students put together a festschrift for him, which is available through the Internet Archive: [_World Perspectives in Philosophy, Religion and Culture_](https://archive.org/details/in.ernet.dli.2015.126145/page/n7/mode/2up) [@Singh1960]. These books normally have a somewhat detailed biography of the honoree, but this one only has a skimpy four-page stub. Still, we learn something interesting from it. Most of the Indian philosophers who published in _Mind_ and _Philosophical Review_ in the first half of the century spent a fair bit of time in Britain or United States.^[Datta also had [an invited paper](https://www.jstor.org/stable/2181796?seq=1#metadata_info_tab_contents) in _Philosophical Review_.] Datta did not; he acquired his very extensive knowledge of philosophy in England from his teachers (especially his brother) and his own reading.

There is a really interesting book to be written about the ways in which late twentieth-century English language philosophy resembles (some strands in) Indian philosophy more than it resembles early twentieth-century English language philosophy. And Datta could be a key part of that story. But that's for another book and, preferably, an author with more knowledge of Indian philosophy than I have. So I'll leave Datta here for now.

I've written much more on postwar epistemology in chapter \@ref(epistemologychapter). The main takeaway from the longer chapter is that work on "Gettier cases" makes up a surprisingly small amount of the work in epistemology. It does make up a large percentage of the work in epistemology in the early years. (And for epistemology, the early years extend into the 1980s.) But it's never as big a percentage of work done in philosophy as I suspect a lot of people believe.

<!--chapter:end:topic_comments/topic74.Rmd-->

```{r t75a}
jjj <- 75
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t75b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t75c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t75d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t75e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

A funny thing about contemporary academia is that there is very little work on (single-person) decision theory outside of philosophy but huge amounts of work on (multiperson) game theory. Inside philosophy, the situation is completely reversed. Game theory is a tiny topic. I keep trying to do something about this, primarily by writing game theoretic papers and teaching game theory courses, but I'm not making a big impact.

This topic actually covers three quite disparate subjects:

- General work on the nature of games, some of it related to the discussion of games and play in Wittgenstein.
- The use of prisoners' dilemma and other game-theoretic tools to analyse social interactions. (This is what accounts for the spike in _Philosophy and Public Affairs_).
- The use of evolutionary game theory to explain some puzzling biological phenomena.

The last of these is the largest of the three, which is why I've put the topic in philosophy of science. But none of them are very large, and collectively they barely make up enough to deserve being a topic.

<!--chapter:end:topic_comments/topic75.Rmd-->

```{r t76a}
jjj <- 76
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t76b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t76c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t76d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t76e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The model separates out the theory of [knowledge](#topic74) from the theory of [justification](#topic76), but they end up with the very same graph. And it's not like the division is particularly clean; some of the main papers from the analysis-of-knowledge debate end up here. 

This topic has even fewer papers from the early years that are clearly in the topic. The first paper that gets a topic probability above one-third is by Harry Frankfurt! This says something interesting about the background to Gettier's 1963 paper. Just a few years before that paper, there was virtually no discussion of beliefs being justified. It wasn't that Gettier showed that a familiar concept couldn't play a role in the analysis of knowledge. He effectively introduced the concept of justification.

Another way to think about the division the model is making here is that the earlier topic is knowledge-first epistemology, this is belief-first epistemology, and the upcoming topic on [formal epistemology](#topic84) is credence-first epistemology. The model thinks 'justified' and 'justification' are, respectively, $10^5$ and $10^8$ times more likely to appear in this topic than inkKnowledge. But it also thinks 'believe', 'believing', 'beliefs' and 'believed' are, respectively, $10^3$, $10^5$, $10^{14}$ and $10^{15}$ times more likely to be here. On the other hand, it thinks 'know' and 'knows' are $10^6$ and $10^{14}$ times less likely to be here than in Knowledge, while 'skeptic' and 'williamson' are also $10^{14}$ and $10^{20}$ times less likely. Obviously a lot of these words are used in a lot of the articles seen above, which is why the model has ambivalent verdicts about the papers. But that's the distinction it is drawing. 

With that in mind, the trends over the last few years of the three main epistemology topics are interesting.

```{r modern-epistemology-graph, fig.height = 5, fig.cap= "Trends in epistemology in the last twenty-five years of the study.", fig.alt = alt_text}
t <- weight_ratio %>%
  filter(year > 1988, topic == 74 | topic == 76 | topic == 84) %>%
  mutate(topic = as.numeric(topic)) %>%
  inner_join(the_categories, by = "topic") %>%
  mutate(topic = as.factor(topic))

ggplot(t, aes(x = year, y = y, color=subject, group=subject)) + 
#    scale_colour_discrete(labels = subject) + 
    spaghettistyle +
    theme(legend.title = element_blank(), 
          legend.position = "right",
          legend.spacing.x = unit(0.2, 'pt'),
          legend.text = element_text(size = rel(1))) +
    geom_point(size = 1, alpha = 1) +
    labs(x = element_blank(), y = "Proportion of all journal articles", title = "Epistemology Topics") +
    scale_x_continuous(minor_breaks = 10 * 188:201,
                       expand = expansion(mult = c(0.01, 0.01))) +
    scale_y_continuous(expand = expansion(mult = c(0.01, .03))) +
  geom_smooth(size = 0.5, se = F, method = "loess", formula = "y ~ x")

alt_text <- "A scatterplot, with trendlines, of the proportion of papers in recent years on justification, knowledge and formal epistemology in the last twenty-five years. Through the 1990s, justification is much higher than the other two, but it stays fairly flat at around 2.5 percent throughout the period. In the early 2000s, Knowledge rises rapidly to almost catch up with justification. Then in the late 2000s formal epistemology rises, and even passes, justification."

```

Justification was for a while the biggest of the three topics. But while it didn't fall, it also hasn't shared in the rapid rise of knowledge or, especially, formal epistemology. The dip in knowledge in the last few years looks to me like a blip, but that will be interesting to check back on in a little while.

<!--chapter:end:topic_comments/topic76.Rmd-->

```{r t77a}
jjj <- 77
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t77b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t77c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t77d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t77e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Harry Frankfurt's article "[Alternate Possibilities and Moral Responsibility](https://philpapers.org/rec/FRAAPA-8)" had as much impact on the trajectory of the journals as any other article in the data set. 

It essentially launched a new topic, the one seen here. Now there would always have been a free-will topic—that's a perennial philosophical concern—but Frankfurt's work meant that there was a detectable new topic about the issues Frankfurt's examples raised.

And it ensured that subsequent work on free will would be in ethics, not metaphysics. When I was an undergraduate, free will was covered in the metaphysics unit of intro, not the ethics unit. I suspect that was because free will was associated with possibility, and possibility is a paradigmatically metaphysical topic. But Frankfurt showed that possibility had less to do with moral responsibility than previously thought. And once that happened, it was clear that free will was more tightly tied to the moral notion (responsibility) than the metaphysical notion (possibility). This results in a topic that is naturally placed in ethics, and indeed was largely discussed in _Ethics_.

The model, however, does two other odd things here. One is that it puts Gettier's paper in this topic. I think it got thrown by the word 'jones'. The other is that it put a whole bunch of work on philosopy of fiction in here as well. Fiction turned out to be a tricky topic for models; on different runs it turned up in all sorts of different places. Here it goes with Frankfurt cases, so strictly speaking this is a disjunctive topic. But after looking at what the articles are about, it's something like an eighty-twenty split in favor of Frankfurt case articles. So that's how I'm identifying the topic.

<!--chapter:end:topic_comments/topic77.Rmd-->

```{r t78a}
jjj <- 78
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t78b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t78c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t78d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t78e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Given the importance of concepts to late twentieth-century philosophy, this is a surprisingly small topic. I think that what's happened here is that it has just gotten squeezed by other topics. There isn't a single article that's more than ten pages long with a probability of being in this topic of over 0.5.

Part of the story about this topic is similar to the story with the topic on [arguments](#topic55). More philosophers write about concepts in papers on other topics than write papers directly about concepts. This isn't surprising, but it did surprise me a bit that there aren't more papers directly about concepts in the journals. There are several important books on concepts, and there are papers in specialist journals like _Mind and Language_, and that led me to expect that there would be more here than turned out to be the case.

<!--chapter:end:topic_comments/topic78.Rmd-->

```{r t79a}
jjj <- 79
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t79b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t79c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t79d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t79e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This topic would look very different if the study ran forward a few years. Work on race is distinctive enough that the model wants to put it somewhere on its own, but small enough that it needs supplementing with other work to get to be a topic. Even with supplementation, it's the smallest topic (by weighted sum) of the ninety. That would not be the case if I ran the study through the present day, I'm sure.

Putting together papers about DNA with papers about race isn't the strangest thing the model has done. (Though it is a little random; this wasn't a common pairing.) Even philosophers who deny that DNA has much if anything to do with race will talk about why it does not. And the model is just tracking word associations. If everyone is talking about why X and Y aren't connected, the model just sees the words for X and Y turning up a lot in the same papers, and connects them.

I don't really know enough philosophy of biology to know why the biology papers in this topic were split off from the much larger topic of [evolutionary biology](#topic82). It looks like a distinction without much of a difference to me, but maybe it's tracking an important topic distinction.

Note that Lindley Darden is coauthor of the most cited article in this topic. She's also coauthor of the most cited article in [mechanisms](#topic18). There are eight philosophers who have an authorship share in the top-cited article in two or more topics. David Lewis is the only one to have the most cited article in three different topics. The other six are G. E. Moore, Thomas Nagel, Paul Boghossian, Amartya Sen, H. L. A. Hart, and Hilary Putnam. So Darden is the only woman to achieve this feat, relative at least to this model. Her work doesn't get nearly as much attention as the other seven philosophers I just mentioned, at least among nonspecialists. That's unfortunate, I think, given its quality and the philosophical importance of the topics she addresses.

<!--chapter:end:topic_comments/topic79.Rmd-->

```{r t80a}
jjj <- 80
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t80b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t80c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t80d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t80e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The top graph makes it look like this is a story of more or less continuous growth from the early 1960s onwards. But under the surface the story is a little more complicated. The topic includes articles about the nature of modality, and in particular the nature of possible worlds, but also articles about the nature of dispositions. 

There are a lot of ways to carve up metaphysics into topics. This one makes some sense, but is also a bit puzzling. Both work on modality and work on dispositions are more connected to work on counterfactual conditionals than they are to each other. The end result is convenient for my project; I get two debates that are both in metaphysics, and reasonably close to each other, in the one topic. It's just funny to see them both separated from counterfactuals in this way.

David Lewis is as important to the journals from 1965 onward as any other philosopher, but he doesn't have any one topic that is really centered around his work. This is as close as it gets. Twenty-one of the articles here are either by Lewis, or reference him in their title. (There are other articles with "Lewis" in the title, but they are about C. I. Lewis, not David Lewis.) My biggest regret of this project was that I couldn't include the _Australasian Journal of Philosophy_ because it wasn't in the JSTOR database. That would have given a much better sense of how Lewis's papers related to the general themes of the era.

<!--chapter:end:topic_comments/topic80.Rmd-->

```{r t81a}
jjj <- 81
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t81b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t81c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t81d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t81e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This includes a bunch of relatively modern work on the nature of reasons, the nature of desires, and the interaction between them. This could say more about me than about the model, but I was surprised this topic wasn't both bigger and later. It isn't small—either the thirty-second or thirty-third largest depending on which measure is used. But plenty of model runs had a topic centered on reasons as the latest topic to appear, and having nine topics come after it was not something I saw in many runs. What happened here, I think, was that this model was more inclusive than other models I ran at including 1970s work (like the Watson and Wiggins articles in the highly cited list), but also split off some papers into the [norms topic](#topic90) that could have been here.

One nice thing is that the model seems to have not gotten tricked at all by the terms _internalism_ and _externalism_. I would have thought they would have thrown it for a loop. But it seems to have been able to figure out that papers about internalism versus externalism about reasons are not the same as papers about internalism versus externalism about justification. This was pleasing and a fairly hard test for the utility of the model in classifying papers.

<!--chapter:end:topic_comments/topic81.Rmd-->

```{r t82a}
jjj <- 82
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t82b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t82c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t82d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t82e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This topic is one of the most notable, and one of the most exciting, developments in philosophy in the past few decades. My sense is that one of the reasons this became such a big topic in philosophy is that there was getting to be less and less support within biology departments for the kind of theoretical work about the foundations of evolution that is being done here. Whether that's true or not, large numbers of philosophers have started taking up this work, and the results are impressive.

From the perspective of the model, one striking thing about this topic is the nature of the "highly cited articles" list. It isn't the longest such list, though it is well above average. But most of the other topics that have a long list of highly cited articles have many of those articles only loosely connected to the topic. (See topic 24 for the most notable example of that.) It's striking to see so many highly cited articles that the model is more than 50 percent confident are in one particular topic.

It's easy to think of this as a somewhat "specialist" topic. Apart from a few articles in the _Journal of Philosophy_, pretty much all the work is in the two philosophy of science journals. But I think it's a somewhat parochial view inside philosophy to think of this as a specialized subject. When walking around a large university, it would be easy to find more people who could follow, and were interested in, these papers than in the papers on reasons or on knowledge. It's true that a relatively small percentage of _philosophers_ could follow, or even are interested in following, the papers in this topic. But that might say more about philosophers than about the material.

<!--chapter:end:topic_comments/topic82.Rmd-->

```{r t83a}
jjj <- 83
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t83b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t83c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t83d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t83e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

I've called this population ethics, but it's really contemporary utilitarianism. For various reasons the term _consequentialism_ became more popular over this time period, and the model doesn't really know that this is a near synonym for 'utilitarianism', so it puts the consequentialist articles and the utilitarian articles in separate categories. I didn't feel like naming the topic after what is something of a terminological change. I could have called it numbers and ethics, which would have been accurate enough if a little frivolous. Instead, I focused on Parfit's centrality to the topic and called it population ethics.

It's mildly surprising to me that "[Famine, Affluence and Morality](https://philpapers.org/rec/SINFAA)" is in here. Of course, it is a consequentialist article. But most of this topic lives at a much higher level of abstraction. Partially what happened was that the model I'm using didn't really settle on a single applied ethics topics. Other runs of the model had topics where "[Famine, Affluence and Morality](https://philpapers.org/rec/SINFAA)" and "[A Defense of Abortion](https://philpapers.org/rec/THOADO-2)" as twin pillars of a big applied ethics topic. But here they got split, and so Singer's paper ends up with quite a bit of high theory around it.

And I know I'm being repetitive on this, but it's just shocking to me to see how little of this work is turning up in the "generalist" journals. The left-two columns in the graph suggest this was an incredibly niche, minor part of contemporary philosophy, with perhaps a small bubble of interest in the 1990s and early 2000s that had faded. And that would be completely the wrong picture. That's why I felt it was essential to include the "specialist" journals in this study; it's not possible to get an accurate picture of the discipline without them.

<!--chapter:end:topic_comments/topic83.Rmd-->

```{r t84a}
jjj <- 84
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t84b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t84c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t84d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t84e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

When I was a graduate student, this was the stuff I worked on—indicative conditionals and probabilistic models of rational agents. At the time, I thought of myself as working at the intersection of philosophy of science and philosophy of language. It was only when I started applying for jobs that I thought, well, since what I'm doing is working on stuff about rationality, maybe I should just insist that it counts as epistemology and see if I get away with it. Twenty-something years later, no one in my position has such worries; this is clearly part of epistemology.

One way to see how much it has changed is to look at where the papers are being published. Here's a version of the facet graph above but restricted to the period since 1980, and with trendlines added.

```{r formal-epistemology-journal-distribution, fig.cap = "Recent formal epistemology.", fig.alt = alt_text}
facet_labels <- chap_two_facet_labels %>%
  mutate(year = 1983, gamfre = yupper)

facet_labels$journal <- factor(facet_labels$journal, levels = journal_order)

indiv_journal_graphs +
  coord_cartesian(xlim = c(1980,2013)) +
  geom_smooth(method = "loess", formula = "y ~ x", se = F, size = 0.1) +
  geom_text(data = facet_labels,
            mapping = aes(label = short_name),
            vjust = "inward", 
            hjust = "inward",
            fontface = "bold", 
            size = 3,
            colour = "grey40")

alt_text <- "A version of figure 2.188 restricted to the years from 1980-2013. The data shows that this topic stayed relatively flat in nine journals, but took off in Mind, Philosophical Review, and Noûs."
```

I think this makes clear something that was intuitively plausible: that it's the generalist journals leading the charge here. As work on Bayesianism has moved from being philosophy of science to epistemology, it has become a much bigger part of what _Mind_, _Philosophical Review_ and _Noûs_ have published. (Though, to be sure, a lot of what's happened in _Noûs_ has been since 2013.) I'm a little surprised there isn't more movement on the _Analysis_ graph; I would have guessed the Sleeping Beauty debate alone would have pushed it higher. But otherwise this looks like a case where the general picture—that this went from something of a specialist topic to a "generalist" one—is borne out by the data.

<!--chapter:end:topic_comments/topic84.Rmd-->

```{r t85a}
jjj <- 85
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t85b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t85c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t85d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t85e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Contemporary debates about semantic externalism were kicked off by Saul Kripke's "[Naming and Necessity](https://philpapers.org/rec/KRINAN)" and Hilary Putnam's "[Meaning and Reference](https://philpapers.org/rec/PUTMAR-2)". _Naming and Necessity_ isn't in this study, though its impacts are felt in several places. But "Meaning and Reference" is, and it is even in this category. Here are the model's views on where to place "Meaning and Reference".

```{r meaning-and-reference}
individual_article("10.2307_2025079")
```

Its largest topic is this one, but the model also notes it is about definitions and modality, which makes sense. But the probability that it is in this topic is comfortably largest. And it's an incredibly influential paper, so I would have guessed that it would have been quickly followed by a flood of similar papers.

But that's not remotely what happened. The model sees very little work on this topic for another decade. There is a bit of discussion in the mid-1980s, then it is Michael McKinsey's 1991 paper "[Anti-Individualism and Privileged Access](https://philpapers.org/rec/MCKAAPhttps://philpapers.org/rec/MCKAAP)", that really starts the discussion going. Just to make this vivid, let's focus on the last forty years of those graphs above, starting from Putnam's original paper.

```{r wide-content-recent-graph, fig.cap = "Recent work on wide content.", fig.alt = alt_text}
facet_labels <- chap_two_facet_labels %>%
  mutate(year = 1976, gamfre = yupper * 1.2)

facet_labels$journal <- factor(facet_labels$journal, levels = journal_order)

indiv_journal_graphs +
  coord_cartesian(xlim = c(1973,2013)) +
  geom_smooth(method = "loess", formula = "y ~ x", se = F, size = 0.1) +
  geom_text(data = facet_labels,
            mapping = aes(label = short_name),
            vjust = "inward", 
            hjust = "inward",
            fontface = "bold", 
            size = 3,
            colour = "grey40")
alt_text <- "A version of figure 2.191 restricted to the years from 1973 to 2013. It shows that in each journal, the peak year for this topic was in or around the 1990s, not in the immediate aftermath of the famous works on wide content from the 1970s."
```

The lack of life in this topic through the 1970s and much of the 1980s was one of the biggest surprises to me of the whole project. In recent years it feels like topics can catch fire immediately after the publication of a high-profile paper. But that is really not what happened in debates about wide content. Putnam's paper is one of the most influential of its time, and its time is the most important few years in the history of philosophy, but that influence was not felt for many years after its publication.

<!--chapter:end:topic_comments/topic85.Rmd-->

```{r t86a}
jjj <- 86
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t86b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t86c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t86d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t86e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Stop me if you think you've heard this one before. This is a topic that Bertrand Russell wrote [an important paper](https://philpapers.org/rec/RUSV-2) on many years ago. But that paper made very little impact on the journals at the time, and it is barely visible on the graphs.

Vagueness was a huge topic in the 1990s and early 2000s, driven first by Timothy Williamson's book [Vagueness](https://philpapers.org/rec/WILV), and then by the work coming out of the [Arché research center](https://www.st-andrews.ac.uk/arche/) at St Andrews. As someone working in that field, it felt like it faded a bit after that time, and the dip in the graph at the very end supports that. I thought there were still plenty of interesting questions here to discuss, but the field seemed to move on.

This was a tricky topic to categorize. I felt it should be philosophy of language, but I really didn't want to go off my judgments in a field I work in. I'll talk a bit more about this in chapter \@ref(sortingchapter), but it turned out the model itself had fairly strong views about this topic, and that's why it is in logic and mathematics.

There are some metaphysics papers scattered in here as well. Some of these are papers on metaphysical vagueness, though since logic journals publish papers on metaphysical vagueness, I don't feel too bad classifying them as logic and mathematics. But the model also included the small early 2000s topic of truthmakers in here as well. I'm not entirely sure why it did that, other than the fact that the boom in truthmaker discussions happens at roughly the same time as the boom in vagueness discussions, and both of them owe a lot to work by Russell that hadn't previously troubled the journals. But the truthmaker papers make up a small portion of the topic, and it's easiest to think of this as just being the vagueness papers.

<!--chapter:end:topic_comments/topic86.Rmd-->

```{r t87a}
jjj <- 87
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t87b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t87c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t87d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t87e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The common theme to most of these articles is that they live at the overlap between philosophy and cognitive science. There is a big focus on the nature of computation, and this occasionally means the model gets confused and pulls in articles that aren't really about cognitive science, like Nick Bostrom's "[Are We Living in a Computer Simulation?](https://philpapers.org/rec/BOSAWL-4)". But most of them, tracing back to Kitcher's discussion of Marr's theory of vision, are safely inside cognitive science.

As someone who is involved with the [Weinberg Institute for Cognitive Science](https://lsa.umich.edu/weinberginstitute) at the University of Michigan, I'm pleased to see this work get this level of attention from the model.

I suspect if we ran the model forward in time this topic would get a bit bigger, though armchair impressions are not always reliable here. There is a lot of work being done at the intersection of cognitive science and philosophy, but I'd have to sit down with the twelve journals to have much of a sense of how much of that work is being done in those journals. It's possible that more of it is taking place in journals like _Mind and Language_ that are outside the study.

I'm not surprised to see Andy Clark be so central to the subject. He turns up on the keywords, the characteristic articles and (separately) the most cited articles. Given his important recent work on predictive processing, and the importance of predictive processing to recent debates in philosophy of mind, I do not think that would change if we extend the model forward in time.

<!--chapter:end:topic_comments/topic87.Rmd-->

```{r t88a}
jjj <- 88
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t88b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t88c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t88d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t88e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

The general theme here is models in philosophy of science. And I'm not completely sure whether this is a good topic my model to have landed on, or whether it has gotten confused by the polysemous nature of "model".

Some of the articles here are on causal modeling, and that's certainly an important topic in philosophy of science. And one of the interesting stories of the period this topic is centered on is the drift of causation from being a metaphysics topic to a philosophy of science topic.

And some of the articles here are about the use of models in special sciences (e.g., economics, climate science), and what the epistemology of learning from models looks like.

And some of the articles are about objectivist versus Bayesian approaches to statistics.

And all of these have things in common. Indeed, any two of them could naturally be put into a topic together. Still, I can't quite shake the feeling that it is the word 'model', rather than any one meaning for that word, that is holding these papers together.

This wouldn't be the worst thing to happen—text-mining like this is always susceptible to getting confusing results when there are polysemous terms around. And it's kind of striking how little that has affected our story to date.

<!--chapter:end:topic_comments/topic88.Rmd-->

```{r t89a}
jjj <- 89
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t89b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t89c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t89d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t89e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

This is the central topic of late twentieth- and early twenty-first-century metaphysics, and unsurprisingly it has a huge number of highly cited articles. Eventually the interest in composition and constitution turned meta, and subsequent work often focussed on what was at stake in debates about composition and constitution, and then about whether there was a more general notion of grounding that could subsume these concepts and, perhaps, several others.

Some of the work here could plausibly be counted in philosophy of mind, since some of the papers are about the relationship between mind and body. This is in keeping with a common trend in late twentieth/early twenty-first-century philosophy—when there is work that crosses between the big disciplines in philosophy, it usually involves philosophy of mind. That stops in the 2010s; more work crosses between disciplines other than philosophy of mind. But for a while philosophy of mind is the glue that holds the other subjects together. Deontology and mereology don't have much in common, but deontology is linked to moral psychology, and mereology to the mind-body problem, and so they are connected via philosophy of mind.

<!--chapter:end:topic_comments/topic89.Rmd-->

```{r t90a}
jjj <- 90
source('topic_comments/topic_summary_data.R') # Get data
```

```{r t90b, fig.cap=paste0(fcap(the_categories$sub_lower[jjj]),"."),  fig.alt = alt_text, fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t90c, fig.cap=paste(fcap(the_categories$sub_lower[jjj]), "articles in each journal."), fig.alt = alt_text_journals}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t90d}
source('topic_comments/topic_summary_char_art.R') # Create table of all articles in topic
temp_dt
```

```{r t90e}
source('topic_comments/topic_summary_high_cites.R') # Generate table of highly cited articles. 
 # Have to do this at same time as previous script or a weird caching error occurs.
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n

**Comments**
	\n")
```

Our last topic has a heavily midwestern flavou. It is largely about normativity and objectivity. And the two figures most central to it are Robert Brandom and Allan Gibbard.

I'm not going to say much about this here because I'll be talking more about it at the end of the book in section \@ref(buzzwords-section). And the big theme there will be that very strange things started happening to this topic when I ran more and more refinements on the model. In particular, it started to track the distinctive vocabulary of contemporary philosophy, the fashionable buzzwords, rather than anything about content. 

It's somewhat clear how this happened. The topic is very contemporary. Explicit discussion of "norms" is a recent phenomenon, and Brandom and Gibbard are very much figures of late twentieth- and early twenty-first-century philosophy. Moreover, one of the actual topics discussed here is commitments. But "commitment" is also something of a buzzword; talking about the "commitments of an account" rather than the "consequences of a theory" is a way to mark one's philosophical writing as being up to date with modern terminology. But the model can't tell the difference between the topic of commitments and the stylistic word choice of "commitments". So the model broke when pushed too hard.

But that hasn't happened in the model being discussed here. Instead the topic is modern in a more pleasing way. It, like much work in the 2010s, breaks down the familiar philosophical categories. This topic is partially about ethics but just as much about philosophy of language. From the 1960s to the 1990s those topics didn't have much overlap, and it's pleasing to see that they are being joined back up here. The study ends too soon to capture the boom in work in ethics and epistemology in the 2010s, though this topic picks up a little of that in papers like "[Doxastic Deliberation](https://philpapers.org/rec/SHADD)". These papers that cut across traditional topics make my job harder, but they make philosophy so much more interesting.

<!--chapter:end:topic_comments/topic90.Rmd-->

# Summary Graphs {#summary-graphs}

In the previous [chapter](#all-90-topics) I discussed the ninety topics one at a time. The aim of this chapter is to find some way to graph them all at once. The short version of the chapter is that it's an impossible task, and there is too much data to really show all of the trends on one graph. The longer version is that there are a few graphs that are flawed in different ways, and between them they might help give some sense to what the trends were in the philosophy journals over this time.

In the first [section](#main-summary-graph) I'll present the (pair of) graphs that do the best job of showing what the trends were. In the second [section](#graph-choices) I'll go over the three binary choices that we have in thinking about how to present the data, leading to eight possible graphs. One of these is the one presented in the first section, but there are cases to be made for each of the other seven as well. So in the remaining sections, I'll go over each of those seven possible graphs.

<!-- Weight vs Raw -->
<!-- Sum vs Frequency -->
<!-- Articles vs Pages -->

## Main Summary Graph {#main-summary-graph}

The first graph shows the expected number of articles in each topic each year. I'll call the expected value the weighted sum, since the probabilities from the model are better thought of as weights. So for each topic-year pair, it looks at all the articles published that year, and sums the probability they are in that topic. The result is a point on this graph.

```{r setuptopiclabels, cache = FALSE}
# Labels for the Facet Graphs
subtopic.labs <- the_categories$subject
names(subtopic.labs) <- the_categories$topic

# Code for the Spaghetti graphs
spaghettigraph <- function(datainput, ylabel, titlelabel){
  ggplot(datainput, aes(x = year, y = y, color=topic, group=topic)) + 
    scale_colour_discrete(labels = fcap(the_categories$sub_lower[1:90])) + 
    spaghettistyle +
    theme(legend.title = element_blank(), 
          legend.position = "bottom",
          legend.key.height = unit(6, 'pt'),
          legend.spacing.x = unit(0.2, 'pt')) +
    geom_point(size = 0.5, alpha = 0.5) +
    labs(x = element_blank(), y = ylabel, title = titlelabel) +
    scale_x_continuous(minor_breaks = 10 * 188:201,
                       expand = expansion(mult = c(0.01, 0.01))) +
    scale_y_continuous(expand = expansion(mult = c(0.01, .03)))
}

# Code for the Facet Graphs
facetgraph <- function(datainput, ylabel, titlelabel){
  # Data for generating the backgrounds
  bg <- datainput %>%
    mutate(topic = as.numeric(topic)) %>%
    inner_join(the_categories, by = "topic") %>%
    select(-topic)
  
  # Place the labels where they won't hit dots
  # So well above all values from 1907-1982, and above values from outlier years
  topval <- max(
    datainput$y,
    filter(datainput, year > 1906, year < 1983)$y * 1.25
  )
  
  # Data for generating the labels - note we have to bounce data style of 'topic' or bad errors happen
  facet_labels <- the_categories %>%
    filter(topic < 91) %>%
    mutate(year = mean(datainput$year), y = topval) %>% # Change Mean to Min to move labels to left - these are centred
    mutate(topic = as.factor(topic))
  
  # Now the graph
  ggplot(datainput, aes(x = year, y = y, color=topic, group=topic))  +
# Use loess curves to create ninety background lines
    geom_smooth(data = bg, 
                aes(group=subject), 
                size = 0.1, 
                color = "grey85", 
                method = "loess", 
                alpha = 0.2,
                se = F,
                formula = "y ~ x") +
 # Draw each of the points in this graph
     geom_point(size = 0.15) +
  # Put them in 5 columns
    facet_wrap(~topic, ncol = 5) +
  # Add labels inside the graph for each of the subjects
    geom_text(data = facet_labels,
            mapping = aes(label = subject),
            vjust = "inward", 
            hjust = "inward",
            fontface = "bold", 
            size = 2) +
  # Add the style that I generated back in the index
      facetstyle +
  # Notate everything
      labs(x = element_blank(), y = ylabel, title = titlelabel) +
    # Fix check marks
      scale_y_continuous(expand = expansion(mult = c(0.01, .02)),
            minor_breaks = scales::breaks_pretty(n = 12),
            breaks = scales::breaks_pretty(n = 3))
}
```

```{r graph1a, fig.height=11.4, fig.width = 7.5, fig.cap = "All ninety topics—weighted sum of articles.", fig.alt = alt_text}
spaghettigraph(weight_numerator, "Weighted number of articles", "Weighted Sum of Articles in Each Topic per Year")  
alt_text <- "A plot showing the importance of all topics over time on a single graph, as measured by weighted sum. The underlying data is in Table B.1. It is mostly a mess of dots that doesn't show very much, but what information can be gleaned by looking is described in the text below."
```

Now there is too much data there to take all of it in. And if your eyes can correlate the colors on key with colors on the graph, they are keener than mine. But still there are some things we can take away from the graph. And I want to stress that these things are really very resilient. I ran a lot of these models to find one that best reflects the trends in the journals, and all of the following features were stably true across the model runs.

First, the colors change over the course of the graph. The topics that are big deals in the early years are very different to the topics that are big deals in the later years.

Second, there are a lot more articles being published in recent years than in earlier years. This is in part because the universe of journals has grown, and in part because I focused on journals that are alive today. But it's significant and a point that I'll have cause to return to several times in this chapter.

Third, there is nothing like the dominance of ordinary language philosophy in the mid-century. If you cut down the number of topics to under thirty, then something like idealism becomes a single topic that is similarly large in the early years. (It basically collapses idealism and life and value from the main model of this book, and adds a bunch of pragmatism too.) And if you keep the number of topics under about fifty (or even sixty), then sometimes the model will put all of epistemology into a single topic, and its size in the 2000s is as big as ordinary language philosophy in the 1950s. But in this model—and in the vast majority of other models I looked at—ordinary language philosophy after the war is bigger than any other model at any time.

But the fourth result is the one that surprised me most of all, and one that I don't really have a simple story about. There is a white triangle toward the bottom right corner of the graph, starting around 1960, peaking around 1980, and ending around 2000. This is again a very resilient result—almost all the model runs I did had something similar. Around 1980, the model sees all the topics being at least somewhat represented in the journals. But the farther from 1980 one gets, in either direction, the less likely it is to think that all the topics are there. 

I'm going to [come back to this much later in the book](#the-bump), because I think it's fascinating. But apart from making this rise in the minimal values visible, this graph is otherwise something of a mess. Things are a bit clearer if we view the individual topics separately.

```{r graph1b, fig.height=18.2, fig.width = 7.5, fig.cap = "The ninety topics—weighted sum of articled (faceted).", fig.alt = alt_text}
facetgraph(weight_numerator, "Weighted Number of Articles", "Weighted Sum of Articles in Each Topic per Year")
alt_text <- "The same data as above, but with each topic shown as a separate facet"
```

This is a bit clearer on the relative scale of the different topics over time. There are four notable shapes of graph here.

One is the graph with a high peak, but a rapid rise and fall either side of it. Some of these are predictable, like verification, or meaning and use. Others are more surprising, like promises and imperatives. I expected ordinary language to be like this, but it isn't. The stylistic changes that were brought about didn't totally stick - the graph does go down a little bit - but they don't totally go away either.

A second is the graph with a sudden rise to a new equilibria. There are a few such topics in the model used in this book, such as explanations, population ethics and personal identity. Other models had a much larger number of these graphs, but once there are as many as ninety topics they start to get less common.

A third is the graph that just rises and rises as the years go along. That's almost all of the last fifteen graphs here.

A fourth, which is almost nonexistent, is the graph that is such a mess that you need to fit a curve to it to see any trends. I'll do some graphs with trendlines in a bit, but for now I want to note how little there is for them. Although the model wasn't told about the age of different articles, and was just trying to classify them as best it could, it mostly came up with totals for each topic per year that were approximately continuous. That's interesting, and tells us something about the way in which debates in journals really do seem to follow trends.

It is also somewhat useful to see the graph animated. This shows all the lines one at a time.

<video width="800" height="800" controls>
  <source src="wsa.mp4" type="video/mp4">
</video>

```{r animation1}
## It didn't work very well to have animations inside the bookdown code
## The compiling got way too slow
## So I built the animations separately and hand coded them into the text with HTML
## Here is the code I used to create the graphs
## I won't include this for later ones, but it's really just a matter of changing
## * The first line, for the input
## * If there is a date filter, adding that
## * Then change the captions
# 
# datainput <- weight_numerator %>%
#   mutate(topic = as.numeric(topic)) %>%
#   inner_join(the_categories, by = "topic") %>%
#   arrange(year, topic) %>%
#   mutate(topic = as.factor(topic))
# 
# subject_order <- slice(datainput, 1:90)$subject
# 
# datainput$subject <- factor(datainput$subject, levels = subject_order)
# 
# p <- ggplot(datainput, aes(x = year, y = y, color = topic, group = year)) + 
#   geom_point() +
#   spaghettistyle +
#   labs(x = element_blank(), 
#        y = "Weighted Number of Articles", 
#        title = "Weighted Sum of Articles in Each Topic per Year", 
#        caption = "Hello World") +
#   scale_x_continuous(minor_breaks = 10 * 188:201,
#                      expand = expansion(mult = c(0.01, 0.01))) +
#   scale_y_continuous(expand = expansion(mult = c(0.01, .03))) +
#   theme(plot.caption = element_text(face = "bold",
#                                     size = 16,
#                                     hjust = -0.02,
#                                     margin = margin(t = 10, b = 35)),
#         plot.title = element_text(margin = margin(t = 35)))
#   
# anim <- p +
#   transition_states(subject)+
#   shadow_mark(color = "grey85", alpha = 0.2) +
#   labs(caption = '{closest_state}')
# 
# animate(anim,
#         nframes = 1800,
#         renderer = av_renderer("wsa.mp4"), # Using anim_save on later line doesn't work with av_renderer() on a mac
#         width = 800,
#         height = 800)
```

## Graph Choices {#graph-choices}

In the previous [section](#main-summary-graph) I presented one way of graphing the trends in these ninety topics. In making that graph I made three major choices, and in each case there is a good argument for doing things the other way. In this section I'm going to say what these choices are, and in the subsequent sections I'll show what the graphs look like with the other choices. This is getting particularly deep in the weeds, and one wouldn't lose a lot by jumping ahead to the next chapter rather than going over these three questions.

First, should the Y-axis be a probability sum, or a count? That is, for each topic-year pair, do we work out the expected number of articles in that topic from that year (given the LDA-generated probability function), or do we count the number of articles from that year whose probability of being in this topic is maximal? I'll call these options the weighted count and raw count respectively.

Second, do we present the result as a sum, or do we normalize the result by presenting it as a ratio of the total number of articles from that year? I'll call these the sum and frequency options.

Third, do we take articles or pages to be the basic unit? In practice, taking articles as the basic unit means adding up how many articles are in a topic, while taking pages as the basic unit involves weighting each article by its page length. (And, if using frequencies, suitably increasing the denominator being used for normalization.)

These three choices totally cross-cut each other, so we get eight possible things to go on the Y-axis.

| Number | Short Description | Long Description |
| -----: | :---------------- | :----------------|
|  1     | Weighted sum of articles | What we already saw, the expected number of articles in a topic in a year |
|  2     | Raw sum of articles | How many articles in each topic each year, where 'in' is defined as having a higher probability of being in that topic than any other |
| 3      | Weighted frequency of articles | The value in 1, divided by the number of articles in that year |
|  4     | Raw frequency of articles | The value in 2, divided by the number of articles in that year |
|  5     | Weighted number of pages | For each article in a year, the probability of being in that topic, times its length in pages |
|   6    | Raw number of pages  | The sum of the pages of the articles in a topic (in the sense of 2) in a given year |
| 7      | Weighted frequency of pages | The value in 5, divided by the total number of pages in that year |
|  8     | Raw frequency of pages | The value in 6, divided by the total number of pages in that year |

Table: (\#tab:eight-types) Eight types of graph.

So why did I go with option 1? Well, all of the other options have flaws that made me want to try something different.

The raw counts are too uneven, and require trendlines to be added to the graph. And they are fairly arbitrary. If the model says that something has probability 0.21 of being in topic X, 0.18 of being in topic Y,  0.16 of being in topic Z, and so on for several other topics, it feels like throwing away information to simply classify it in X. This is especially true if Y and Z are very similar topics, and the model could easily have merged them, while X is fairly different.

Given the variation in the number of articles and pages being published each year, it makes sense to express trends as a proportion of the annual whole, rather than as a count. The problem is that there is too much of a monoculture (or perhaps biculture) in the early years of the data. Idealism and psychology routinely account for more than 30% of the articles (or pages) in a year. So any graph we have would have to have a Y-axis that stretches that high. But post-1970, the difference between the prominent and less prominent topics is the difference between being 1 percent and 3 percent of the total. There is no natural way to represent those things on a single graph.

There are a few ways around this, and the proportion-based measures make so much sense that I'll use most of the following tricks somewhere in the book.

- We can present the different topics on different charts and hope that a combination of labelling the axes carefully and putting reminders about the axis labels in the text will make the differences apparent. I did that in the previous [chapter](#all-90-topics).
- We can separate out the journals so prominent modern topics are a reasonable percentage of a particular journal. I did that in the previous [chapter](#all-90-topics) too.
- We can leave off the early years of the data set, so the outliers go away. That works, but it literally involves removing data, and by the time we're done, the quantity of articles and pages has stabilized enough that it's less important to normalize.
- We can literally leave off some data points, setting the limit to the Y-axis below where outlier points are, and just list the outlier points in the text. For the purposes of these eight graphs there are too many points for that to be feasible, but I will use this trick from time to time later in the book.

While all of these work, none of them seem perfect, especially if we want a single graph to show everything. So I think it's best, at least for this chapter, to use sums not frequencies.

Third, it is much simpler to use article counts, but there is some value to using pages as well. If one topic is frequently the subject of very short articles, especially in _Analysis_, and another topic is never discussed in short articles, then the difference between the article count and the page count will be substantial. And there is a sense in which the page count more reflects what is going on in the journal as a whole.

To give you a sense of how this can happen, here are the weighted article counts and page counts for two topics just restricting attention to the 2000s (i.e., 2000–2009).

```{r pages-and-articles}
page_ratio <- inner_join(weight_numerator, page_weight_numerator, by = c("year", "topic")) %>%
  filter(year > 1999, year < 2010) %>%
  mutate(pages = y.y/y.x) %>%
  arrange(-pages)

page_ratio_summary <- page_ratio %>%
  group_by(topic) %>%
  dplyr::summarise(a = sum(y.x), p = sum(y.y)) %>%
  mutate(r = p/a) %>%
  arrange(topic) %>%
  mutate(topic = as.numeric(topic)) %>%
  inner_join(the_categories, by = "topic") %>%
  mutate(a = round(a, 2), p = round(p, 2)) %>%
  select(Subject = subject, Articles = a, Pages = p)

kable(slice(page_ratio_summary, c(52, 59)), caption = "Article and page counts.") %>% kable_styling(full_width = F)
```

The articles on truth were largely (though not exclusively) in _Analysis_. The articles on liberal democracy were largely (though again not exclusively) in _Philosophy and Public Affairs_. Which of these is said to have a bigger presence in the journals in the 2000s is very much a function of whether the measure used is based on articles or pages. If one has a strong view about the journals in the 2000s, one might be able to use this to test which of the two is a more accurate measure.

For my money, both seem like reasonably interesting facts about the journals. There were more articles about truth, and more pages about liberal democracy, and that's all there is to say. A philosopher working on one of these topics but not the other would probably feel their home topic was the bigger deal. But it seems to me like both measures are useful.

I've usually used articles not pages though for a few reasons. 

One is that the LDA assigns probabilities to articles, not individual pages, and it seems potentially misleading to use these article-based measures to implicitly say that there were, say, 1,369 journal pages on liberal democracy in the 2000s. If my data included not just words in articles, but words on pages, I could have set the LDA up to assign topics to each page, and then it would have made more sense to count the pages. But that's not the data available.

Another is that the number of journal pages has been growing more rapidly than the number of journal articles. So if we measure by pages, the case for normalizing (i.e., showing frequencies not counts) is even stronger. But the problems with normalizing remain.

These aren't super conclusive reasons, but they were enough that I'm mostly using article-based, rather than page-based, measures in this book.

But that said, I think all eight graphs are interesting, and in the rest of this chapter I'll show what they look like.

## Raw Sum of Articles {#graph-rsa}

Here is what it looks like if we just count how many articles are in each topic each year.

```{r graph2a, fig.height=11.4, fig.width = 7.5, fig.cap = "All ninety topics—raw sum of articles.", fig.alt = alt_text}
spaghettigraph(count_numerator, "Raw Number of Articles", "Raw Sum of Articles in Each Topic per Year")
alt_text <- "A plot showing the importance of all topics over time on a single graph, as measured by raw sum. The underlying data is in Table B.2. It is mostly a mess of dots that doesn't show very much, but what information can be gleaned by looking is described in the text below."
```

There are so many overlapping dots that it isn't that helpful a graph. It's a bit clearer when the topics are broken out.

```{r graph2b, fig.height=18.2, fig.width = 7.5, fig.cap = "The ninety topics—raw sum of articles (faceted).", fig.alt = alt_text}
facetgraph(count_numerator, "Raw Number of Articles", "Raw Sum of Articles in Each Topic per Year")
alt_text <- "The same data as above, but with each topic shown as a separate facet"
```

Note a couple of things about this as compared to the graphs back in section \@ref(main-summary-graph). One is that the Y-axis is much higher. The raw count measure tends to amplify trends. Another is that the graphs are much noisier. There are still enough trends that I don't think it would really help to add trend-lines, but there are a lot more graphs that look just like scattered points, especially over ten-to-twenty year patches. I think this is basically a flaw in the underlying measure being amplified.

The animation makes things a bit clearer here because the original features so many overlapping dots.

<video width="800" height="800" controls>
  <source src="rsa.mp4" type="video/mp4">
</video>

## Weighted Frequency of Articles {#graph-wfa}

```{r graph3a, fig.height=11.4, fig.width = 7.5, fig.cap = "All ninety topics—weighted frequency of articles.", fig.alt = alt_text}
spaghettigraph(weight_ratio, "Weighted frequency of articles", "Weighted Frequency of Articles in Each Topic per Year")
alt_text <- "A plot showing the importance of all topics over time on a single graph, as measured by weighted frequency. The underlying data is in Table B.3. It is mostly a mess of dots that doesn't show very much, but what information can be gleaned by looking is described in the text below."
```

There are some dots in the upper-left quadrant, and then a bunch of dots bouncing along the bottom tenth of the graph. It isn't very informative, and breaking it out by topic doesn't help a lot.

```{r graph3b, fig.height=18.2, fig.width = 7.5, fig.cap = "The ninety topics—weighted frequency of articles (faceted).", fig.alt = alt_text}
facetgraph(weight_ratio, "Weighted Frequency of Articles", "Weighted Frequency of Articles in Each Topic per Year")
alt_text <- "The same data as above, but with each topic shown as a separate facet"

```

In this case, cutting down the to the last seventy-five years produces a much more readable graph.

```{r graph3c, fig.height=11.4, fig.width = 7.5, fig.cap = "All ninety topics—weighted frequency of articles.", fig.alt = alt_text}
#Burper
spaghettigraph(filter(weight_ratio, year > 1938), "Weighted frequency of articles", "Weighted Frequency of Articles in Each Topic per Year")
```

There are too many overlapping dots here to see much, but the animation is a little clearer.

<video width="800" height="800" controls>
  <source src="wfa.mp4" type="video/mp4">
</video>

And the result is an animated version of some graphs that appeared a lot in the previous chapter.

## Raw Frequency of Articles {#graph-rfa}

The problems of the previous section are just exacerbated if we go to raw frequencies.

```{r graph4a, fig.height=11.4, fig.width = 7.5, fig.cap = "All ninety topics - Raw Frequency of Articles", fig.alt = alt_text}
spaghettigraph(count_ratio, "Raw frequency of articles", "Raw Frequency of Articles in Each Topic per Year")
alt_text <- "A plot showing the importance of all topics over time on a single graph, as measured by raw frequency. The underlying data is in Table B.4. It is mostly a mess of dots that doesn't show very much, but what information can be gleaned by looking is described in the text below."
```

```{r graph4b, fig.height=18.2, fig.width = 7.5, fig.cap = "The ninety topics—raw frequency of articles (faceted).", fig.alt = alt_text}
facetgraph(count_ratio, "Raw frequency of articles", "Raw Frequency of Articles in Each Topic per Year")
alt_text <- "The same data as above, but with each topic shown as a separate facet."

```

All this tells us is that there is a lot more diversity, and a lot more specialization, in journals in the last thirty years than there was 120 years ago. Everything else gets lost in the noise.

It's only a little clearer in the graph that only shows the last seventy-five years.

```{r graph4c, fig.height=11.4, fig.width = 7.5, fig.cap = "All ninety topics - Raw Frequency of Articles", fig.alt = alt_text}
spaghettigraph(filter(count_ratio, year > 1938), "Raw frequency of articles", "Raw Frequency of Articles in Each Topic per Year")
```

The animation is a bit more revealing.

<video width="800" height="800" controls>
  <source src="rfa.mp4" type="video/mp4">
</video>

But what it reveals is primarily that these raw counts are very unstable. That's because the measure they are built on is subject to severe tipping-point effects. Whether an article gets probability 0.26 for being in one category and probability 0.25 for being in another, or the other way around, really just depends on where the algorithm is stopped. (It's bob-of-the-head stuff in horse racing terms.) But it makes all the difference to these raw counts. This is why I've tried, contrary to most work that I've seen that uses topic modeling, to deemphasize these raw counts in favor of the weighted counts.

The rest of the graphs look at what happens when focusing on pages rather than articles. I'm using articles as the basic unit of measure for everything else in this book, but it's worth spending a little time seeing how things look if focusing on pages instead.

## Weighted Sum of Pages {#graph-wsp}

This is probably the second-best graph. Even the graph with ninety dots on it shows some trends.

```{r graph5a, fig.height=11.4, fig.width = 7.5, fig.cap = "All ninety topics—weighted sum of pages.", fig.alt = alt_text}
spaghettigraph(page_weight_numerator, "Weighted sum of pages", "Weighted Sum of Pages in Each Topic per Year")
alt_text <- "A plot showing the importance of all topics over time on a single graph, as measured by weighted sum of pages. The underlying data is in Table B.5. It is mostly a mess of dots that doesn't show very much, but what information can be gleaned by looking is described in the text below."

```

Interestingly, the growing length of articles takes away the bump that is visible in figure \@ref(fig:graph1a); now it seems like all topics get ten to fifteen weighted pages (at least) every year. The length increase is itself quite remarkable. Here's the average article length over years. 

```{r page-average, fig.cap = "Average article length.", fig.height=6, fig.width=7.5, out.width = '80%', fig.alt = alt_text}
page_average <- inner_join(page_demonimator, article_demonimator, by = "year") %>%
  mutate(y = d.x/d.y) %>%
  select(year, y)

ggplot(page_average, aes(x = year, y = y)) + 
  spaghettistyle +
  geom_point(size = 0.5, colour = hcl(h = 15, l = 65, c = 100)) + 
  labs(x = element_blank(), y = "Average number of pages in articles", title = "Average Article Length")

alt_text <- "The average number of pages in articles over time. It starts around 12, then rises unevenly to a peak of around 19 around 1910. It then falls almost linearly to around 9 in the mid-1960s. The it bounces, rising linearly at almost the same rate it fell. The new peak is around 20 at the end of the data in 2013, but it doesn't look like a peak; it looks like that's just where the data ends, and the trend would probably continue into the future."
```

And that same effect means that some of the big twenty-first-century topics are now outpacing ordinary language philosophy. I'll come back to this article length increase in a bit, but first let's see what this graph looks like with every topic having its own facet.

```{r graph5b, fig.height=18.2, fig.width = 7.5, fig.cap = "The ninety topics—weighted sum of pages (faceted).", fig.alt = alt_text}
facetgraph(page_weight_numerator, "Weighted Sum of Pages", "Weighted Sum of Pages in Each Topic per Year")
alt_text <- "The same data as above, but with each topic shown as a separate facet."

```

The acceleration in thelast fifteen topics is much more pronounced. And ordinary language doesn't look like it has a rise and fall any more—it has a rise that it holds on to. Norms looks like it is about to eat everything, and maybe it is. This is even more vivid in the animated version of the graph.

<video width="800" height="800" controls>
  <source src="wsp.mp4" type="video/mp4">
</video>

It's worth pausing for a minute about what's driving this. As I showed above, page lengths increased substantially over the last few decades of the data set. That graph is fairly noisy at first, then a sharp dip takes us to a minimum in the early 1960s, and from then it is a steep rise. (One that is not, in my experience, abating anytime soon.) But an average covers up a lot of things. For instance, _Noûs_ used to publish abstracts of APA presentations as research papers. These were often one page, and could really pull down averages. Here is a slightly more instructive way of looking at the data. The following graph shows various deciles of lengths over time. So the bottom line is the length such that 10 percent of articles are shorter than (or equal to) its length, the top line is the length such that 90 percent of articles are short than (or equal to) its lengths, and so on for the in between lines.

```{r length-distribution, fig.cap = "Distribution of article lengths.", fig.height=6, fig.width=7.5, out.width = '80%', fig.alt = alt_text}
p <- ((1:5)/5) - 0.1
p_names <- map_chr(p, ~paste0("d.", .x*100, "%"))
p_funs <- map(p, ~partial(quantile, probs = .x, na.rm = TRUE)) %>% 
  set_names(nm = p_names)
length_deciles <- articles %>% 
  group_by(year) %>% 
  summarize_at(vars(length), p_funs) %>%
  pivot_longer(cols = starts_with("d."), names_to = "decile", names_prefix = "d.", values_to = "length")
ggplot(length_deciles, aes(x = year, y = length, color = decile)) + 
  spaghettistyle +
  geom_point(size = 0.5, alpha = 0.2) +
  geom_smooth(se = F, method = "loess", size = 0.2) +
  theme(legend.title = element_blank(), legend.position = "bottom") +
  labs(x = element_blank(), y = "Number of pages", title = "Distribution of Article Lengths")

alt_text <- "Another graph of article lengths over time. This shows the 10th, 30th, 50th, 70th and 90th percentile of page lengths in each year. All five graphs have the same shape, though they are much noisier than the previous graph. They all rise to an initial peak around 1910, then fall through the 1960s, then increase fairly rapidly through the end of the data period in 2013."
```

Some of this could be explained by having a bunch of one-page notes, but not all of it. For much of the 1950s and 1960s, fewer than 10 percent of papers were over twenty pages. Now twenty pages is the median article length, in a universe of journals that includes _Analysis_. For that to be explained by having a bunch of very short articles, the olive line (at 30 percent) would have to be hugging the bottom of the graph, and clearly it isn't. 

Articles are getting much much longer.

## Raw Sum of Pages {#graph-rsp}

```{r graph6a, fig.height=11.4, fig.width = 7.5, fig.cap = "All ninety topics—raw sum of pages.", fig.alt = alt_text}
spaghettigraph(page_count_numerator, "Raw sum of pages", "Raw Sum of Pages in Each Topic per Year")
alt_text <- "A plot showing the importance of all topics over time on a single graph, as measured by raw sum of pages. The underlying data is in Table B.6. It is mostly a mess of dots that doesn't show very much, but what information can be gleaned by looking is described in the text below."

```

As always, moving from the weighted count to the raw count just exacerbates the trends. Let's see that broken out into topics.

```{r graph6b, fig.height=18.2, fig.width = 7.5, fig.cap = "The ninety topics - Raw Sum of Pages (faceted)", fig.alt = alt_text}
facetgraph(page_count_numerator, "Raw Sum of Pages", "Raw Sum of Pages in Each Topic per Year")
alt_text <- "The same data as above, but with each topic shown as a separate facet"

```

Now ordinary language philosophy has a boom and bust pattern again. There are a few topics, of which it is the most prominent, where the model sees them never quite going away, but not being the center of attention once their peak passes. Oddly it also sees this pattern for wide content (which I sort of get), and causation (which I don't). None of these see a fall in the weighted graphs, but they do in the raw graphs. I guess I think the weighted graphs are more aptly reflecting the real trends here.

It's much easier to see the later topics on the animation.

<video width="800" height="800" controls>
  <source src="rsp.mp4" type="video/mp4">
</video>

One interesting thing that is visible here (and also in the facet graph), is that [norms](#topic90) doesn't really explode at the end as it does on the weighted graph. This is related to something I'll return to in the last chapter of the book; that topic is picking up a little on changes in linguistic fashion in the literature.

## Weighted Frequency of Pages {#graph-wfp}

Again, the frequency graphs are dominated by the topics popular with _Mind_ in its early years.

```{r graph7a, fig.height=11.4, fig.width = 7.5, fig.cap = "All ninety topics—weighted frequency of pages.", fig.alt = alt_text}
spaghettigraph(page_weight_ratio, "Weighted frequency of pages", "Weighted Frequency of Pages in Each Topic per Year")
alt_text <- "A plot showing the importance of all topics over time on a single graph, as measured by weighted frequency of pages. The underlying data is in Table B.7. It is mostly a mess of dots that doesn't show very much, but what information can be gleaned by looking is described in the text below."

```

There is psychology, then idealism dominates, then there is ordinary language philosophy, and the rest is almost all just noise. (Except for a burst of interest in norms at the end.) It's a bit clearer if we graph the ninety topics distinctly, but not a lot.

```{r graph7b, fig.height=18.2, fig.width = 7.5, fig.cap = "The ninety topics - Weighted Frequency of Pages (Faceted)", fig.alt = alt_text}
facetgraph(page_weight_ratio, "Weighted Frequency of Pages", "Weighted Frequency of Pages in Each Topic per Year")
alt_text <- "The same data as above, but with each topic shown as a separate facet"

```

And it's a bit clearer still if we restrict to the last seventy-five years and animate it, but still not particularly useful.

<video width="800" height="800" controls>
  <source src="wfp.mp4" type="video/mp4">
</video>

These graphs aren't particularly informative, but we'll end with some graphs that may if anything be less informative.

## Raw Frequency of Pages {#graph-rfp}

```{r graph8a, fig.height=11.4, fig.width = 7.5, fig.cap = "All ninety topics—raw frequency of pages.", fig.alt = alt_text}
spaghettigraph(page_count_ratio, "Raw frequency of pages", "Raw Frequency of Pages in Each Topic per Year")
alt_text <- "A plot showing the importance of all topics over time on a single graph, as measured by raw frequency of pages. The underlying data is in Table B.8. It is mostly a mess of dots that doesn't show very much, but what information can be gleaned by looking is described in the text below."

```

As always, moving from the weighted count to the raw count just exacerbates the trends. Here are the individual topics; there are a lot of low lines here.

```{r graph8b, fig.height=18.2, fig.width = 7.5, fig.cap = "The ninety topics—raw frequency of pages (faceted).", fig.alt = alt_text}
facetgraph(page_count_ratio, "Raw frequency of pages", "Raw Frequency of Pages in Each Topic per Year")
alt_text <- "The same data as above, but with each topic shown as a separate facet"

```

And we'll end with the animated version, again restricted to the last seventy-five years.

<video width="800" height="800" controls>
  <source src="rfp.mp4" type="video/mp4">
</video>

<!--chapter:end:03-summary_graphs.Rmd-->

# Categories {#categorychapter}

The ninety topics we've looked at so far are informative, but they are arguably too detailed to provide a useful big-picture view of what's happening in philosophy. To do that I divided up the ninety topics into 12 categories. Or, to be more precise I divided eighty of them up into categories, and the other ten were split between two categories. I'll return in the next chapter to how this division was made, and in particular how the splitting was done. For now, let's look at the categories and then look at their representation in the literature over time.

```{r category_setup_local, fig.height=12, fig.width = 10}
# Burp
require(readr)
the_categories <- read_csv("category-summary-22031848-90-r15.csv")

category_table <- tribble(
  ~topic, ~subject, ~category, ~keywords
)

for (j in 1:90){
  relevant_keywords <- distinctive_topics %>%
    slice(((j-1)*15+1):((j-1)*15+5))
  keywords_for_table <- paste(relevant_keywords$term, sep=", ", collapse = ", ")
  if (!the_categories$cat_num[j] == 13){
    category_table <- category_table %>%
      add_row(topic = j, 
              category = fcap(str_to_lower(the_categories$cat_name[j])), 
              subject = fcap(the_categories$sub_lower[j]), 
              keywords = keywords_for_table)
  }
  if (the_categories$cat_num[j] == 13){
    temp_category_tibble <- tibble(
      topic = (j*100+1):(j*100+2))
    temp_category_tibble <- temp_category_tibble %>%
      inner_join(the_categories, by = "topic")
    temp_category <- fcap(paste(str_to_lower(temp_category_tibble$cat_name), sep = "/", collapse = "/"))
    category_table <- category_table %>%
      add_row(topic = j, 
              category = temp_category,
              subject = fcap(the_categories$sub_lower[j]), 
              keywords = keywords_for_table)
    
  }
}

kable(category_table,  
	col.names = c("Topic", "Subject", "Category", "Keywords"))
```

## Graphs

First, let's look at how many articles are in each category in each year. I'll say an article is "in" a category if the probability the model gives to the article being from that category is greater than the probability it gives to the article being from any other category. And the probability an article is from a category is just the sum of the probability that it is from one of the topics (or subtopics) that make up the category. (What's a subtopic? I'll get to that in [the next chapter](#subtopics-section).)

```{r category-count, fig.cap = "Number of articles in each of the twelve categories over time.", fig.alt = alt_text}
# Burp
category_count <- category_gamma %>%
  group_by(document, cat_name) %>%
  dplyr::summarise(n = sum(gamma)) %>%
  top_n(1, n) %>%
  ungroup() %>%
  select(document, category = cat_name) %>%
  inner_join(articles, by = "document") %>%
  select(document, year, category) %>%
  group_by(year, category) %>%
  dplyr::summarise(n = n_distinct(document)) %>%
  ungroup() %>%
  complete(year, category, fill = list(n= 0))

category_count_graph <- ggplot(category_count, aes(x = year, y = n, color=fcap(str_to_lower(category)), group=fcap(str_to_lower(category)))) +
    labs(x = element_blank(), y = "Number of Articles") +
    scale_y_continuous(expand = expansion(mult = c(0.01, .03)),
#                     minor_breaks = scales::breaks_pretty(n = 15),
                      minor_breaks = 10 * 1:20,
#                     breaks = scales::breaks_pretty(n = 3)) +
                      breaks = 50 * 1:3) +
    scale_x_continuous(expand = expansion(mult = c(0.01, .03)),
                     minor_breaks = 10 * 1:201,
                     breaks = 40 * 1:50) +
    spaghettistyle +
    theme(legend.title = element_blank(), legend.position = "bottom")  
  
category_count_graph +  
  geom_point(size = 0.5, alpha = 0.5)

alt_text <- "A scatterplot showing the raw number of articles in each year in each of the 12 categories. The graph is too busy to see much usable information. The data for this graph, and the next, are in Table C.1 in appendix C."
```

That's a lot of dots, and I'm not sure it's easy to see a trendline in there. Let's add some automatic trendlines to see if it's any clearer.

```{r category-count-trends, fig.cap = "Number of articles in each of the twelve categories over time (with trendlines).", fig.alt = alt_text}
# Burp
category_count_graph + 
  geom_point(size = 0.5, alpha = 0.2) +
  geom_smooth(se=F, method = "loess", formula = "y ~ x", size = 0.2)

alt_text <- "A version of the previous scatterplot with trendlines added. The pattern in the trendlines is described in the text."
```

That's a bit clearer, but only a bit. Most categories are growing, which isn't surprising because there are more and more articles each year. Philosophy of science starts rising early and keeps on rising. Ethics shoots up in the postwar years, and gets to a high point around 1980. Mind grows, but loses rank. metaphysics and epistemology both grow, though metaphysics's growth comes earlier. Logic and mathematics is strongest in mid-century. History gets smaller over time. (This certainly tells us more about the journals than the field.) And idealism grows until at least World War I, then falls dramatically.

In principle it's better to do this with probabilities rather than just these counts though, to reflect the fact that the model itself is probabilistic. So next I'll show the expected number of articles in each category in each year. That is, for each category and year, I'll take the sum across all articles from that year of the probability that the article is in that category. And the probability that an article is in a category is just the sum of the probabilities of it being in the topics and subtopics that make up the category.

```{r category_graph_setup}

category_gamma_graph <- category_gamma %>%
  inner_join(articles, by = "document") %>%
  select(journal, year, category = cat_name, gamma) %>%
  group_by(journal, year, category) %>%
  dplyr::summarise(g = sum(gamma)) %>%
  ungroup() %>%
  complete(journal, year, category, fill = list(g = NA))

category_year <- category_gamma_graph %>%
  group_by(category, year) %>%
  dplyr::summarise(y = sum(g, na.rm=TRUE))

year_denominator <- category_gamma_graph %>%
  group_by(year) %>%
  dplyr::summarise(d = sum(g, na.rm = TRUE))

category_frequency <- inner_join(category_year, year_denominator, by = "year") %>%
  mutate(f = y / d)
```

```{r category-weight-graph,  fig.cap = "Weighted number of articles in each of the twelve categories over time", fig.alt = alt_text}
category_weight_graph <- ggplot(category_year, aes(x = year, y = y, color=category, group=category)) +
    labs(x = element_blank(), y = "Number of Articles") +
    scale_y_continuous(expand = expansion(mult = c(0.01, .03)),
#                     minor_breaks = scales::breaks_pretty(n = 15),
                      minor_breaks = 10 * 1:20,
#                     breaks = scales::breaks_pretty(n = 3)) +
                      breaks = 50 * 1:3) +
    scale_x_continuous(expand = expansion(mult = c(0.01, .03)),
                     minor_breaks = 10 * 1:201,
                     breaks = 40 * 1:50) +
    spaghettistyle +
    theme(legend.title = element_blank(),
          legend.position = "bottom")  

category_weight_graph + 
  geom_point(size = 0.5, alpha = 0.5)

alt_text <- "A scatterplot showing the weighted number of articles in each year in each of the 12 categories. The graph is only a bit clearer than figure 4.1. The data for this graph, and the next, are in Table C.2 in appendix C."

```

That's also a bunch of dots, though interestingly the trends are more visible without trend lines. Let's see what happens when we add the trend lines back in.

```{r category-weight-graph-trendlines, fig.cap = "Weighted number of articles in each of the twelve categories over time (with trendlines).", fig.alt = alt_text}
category_weight_graph + 
  geom_point(size = 0.5, alpha = 0.2) +
  geom_smooth(se=F, 
              method = "loess", 
              formula = "y ~ x",
              size = 0.2)

alt_text <- "A version of the previous scatterplot with trendlines added. The pattern in the trendlines is described in the text."
```

That's pretty similar to the other trendline graph. The big difference is that the lines are more compressed. (This is most visible in the scales.) But the rank at any given time is fairly similar.

The big challenge here is seeing which rises are due to a category getting more prominent, and which are due to the field getting bigger. So let's take that graph and divide all the values by the number of articles in a given year. What we end up with, for each year-category pair, is the average probability an article from a year falls into that category. The numbers are thrown off a fair bit by the presence of a lot of psychology articles in _Mind_ before 1900, so I'll start in 1900.

```{r category-frequency-graph, fig.cap = "Proportion of articles in each category each year.", fig.alt = alt_text}
category_frequency_graph <- ggplot(filter(category_frequency, year > 1899), 
                                   aes(x = year, y = f, color=category, group=category)) +
    labs(x = element_blank(), y = "Proportion of Articles") +
    scale_y_continuous(expand = expansion(mult = c(0.01, .03)),
#                     minor_breaks = scales::breaks_pretty(n = 15),
                      minor_breaks = 0.025 * 1:20,
#                     breaks = scales::breaks_pretty(n = 3)) +
                      breaks = 0.1 * 0:5) +
    scale_x_continuous(expand = expansion(mult = c(0.01, .03)),
                     minor_breaks = 10 * 1:201,
                     breaks = 40 * 1:50) +
    spaghettistyle +
    theme(legend.title = element_blank(),
          legend.position = "botto") 

category_frequency_graph + geom_point(size = 0.5, alpha = 0.5)  + theme(legend.title = element_blank())

alt_text <- "A scatterplot showing the weighted frequency of articles in each year in each of the 12 categories. The graph is quite a bit clearer than the earlier scatterplots, and its shape is described in the text below. The data for this graph, and the next, are in Table C.3 in appendix C."
```

It's striking how many trendlines are already visible, but it's clearer with actual trendlines added.

```{r category-frequency-graph-trendlines, fig.cap = "Proportion of articles in each category each year (with trendlines).", fig.alt = alt_text}
category_frequency_graph + 
  geom_point(size = 0.5, alpha = 0.2) + 
  geom_smooth(se=F, method = "loess", formula = "y ~ x", size = 0.2) 

alt_text <- "A version of the previous scatterplot with trendlines added. The pattern in the trendlines is described in the text."
```

And it's really striking how many of the graphs are relatively simple to describe.

- Aesthetics is always low, though it's high point is midcentury.
- Epistemology is almost invisible until 1950, then rises linearly afterward.
- Ethics has a low equilibrium before 1930, a high equilibrium after 1970, and rises rapidly between those points.
- History of philosophy falls from 1930 to 2000, though maybe it has stopped falling.
- Idealism falls rapidly until it hits 0 around 1980. (Though note that this is a consequence of my excluding the nineteenth-century data points - it arguably rises in the early twentieth-century.)
- Logic and mathematics reaches a high point around 1955, with a striking rise before then and fall afterwards.
- Metaphysics has a steady base through the early 1960s, and then a steady rise after that. (This is not remotely in keeping with my prior view about metaphysics in the journals, and we'll come back to how plausible this is.)
- Apart from a small blip around 1930, philosophy of language has a low equilibrium before 1950, and a higher equilibrium afterwards.
- Philosophy of mind falls as the psychology articles gradually fade (and the relative size of _Mind_ fades) but is very constant from 1940 or so onwards.
- Philosophy of religion is very small, but probably gradually falling.
- Philosophy of science rises rapidly through 1940, then hits an equilibrium, then rises rapidly again after 1970.
- Social and political has a peak around 1920, then a dip and another peack 1940, then gradually fades. (Again, this is not remotely like what I expected, though the explanation isn't that hard.)

In the next chapter I'll go over how the categories were constructed, which in turn will explain some of these trends. But some of them are just interesting facts about the nature of philosophy in these twelve journals.

It might be easier to see these twelve categories separated out.

```{r category-frequency-graph-facet, fig.cap = "Proportion of articles in each category (with facets).", fig.alt = alt_text}
tt <- filter(category_frequency, year > 1899)
category_graph_labels <- tt %>%
  group_by(category) %>%
  summarise(year = median(year)) %>%
  mutate(f = max(tt$f) * 1.2)
ggplot(tt, 
       aes(x = year, y = f, color=category, group=category)) +
  geom_text(data = category_graph_labels,
            mapping = aes(label = category),
            vjust = "inward", 
            hjust = "inward",
            fontface = "bold", 
            size = 3) +
  labs(x = element_blank(), y = "Proportion of Articles", title = "Proportion of Articles in Each Category Per Year") +
  geom_point(size = 0.15) + 
  facet_wrap(~category, ncol=3) +  
  scale_x_continuous(expand = expansion(mult = c(0.05, 0.05))) +
  scale_y_continuous(expand = expansion(mult = c(0, .03))) +
  facetstyle

alt_text <- "A version of Figure 4.5 where each category is shown on a separate graph. The data is in Table C.4 in appendix C, and the trends are described in the text below."
```

It's really striking how continuous these graphs are. This is not something that's programmed in—we're just seeing one data point after another rather than trendlines. The numbers here are sums across journals, and across topics/subtopics. And within each of those components, there is nothing like this continuity. But the large year-to-year variation mostly washes away at the category level.

## Split Topics

Normally at this point I would include a long discussion of the methodology that produced graphs like this. But the methodology is so complicated that I put it in a whole separate [chapter](#sortingchapter). But there is one point I need to clarify before discussing the individual topics.

Several topics didn't easily fit into one or other category. For several of them, I just had to choose the least bad category to put it in. But for some, it turned out to be possible to split the topic into two **subtopics**, and categorize the subtopics. I somewhat arbitrarily imposed a limit on myself of doing this ten times, so the following ten topics got broken into two subtopics each.

```{r sub-topic-list}
subtopic_table <- the_categories %>%
  filter(cat_num == 13 | topic > 100)
subtopic_function <- function(x){
  tibble(a = subtopic_table$subject[x], b = subtopic_table$subject[2*x+9], c = subtopic_table$subject[2*x+10])
}
subtopic_kable <- lapply(1:10, subtopic_function) %>% bind_rows()
kable(subtopic_kable, 
	col.names = c("Original Topic", "First Subtopic", "Second Subtopic"))
```

So we end up with one hundred topics and subtopics to classify; the eighty original topics that weren't split, and the twenty subtopics just generated. How I split them up is a bit of a saga, and I'll run through it in the next [chapter](#sortingchapter).

In what remains of this chapter, I'll go one by one through eight of the twelve topics. For each topic, I'll look at its trends, and how those are generated by its constituent topics, and by the twelve journals. I'm leaving out four categories from this discussion.

- Idealism is a single topic, and I've already discussed it.
- Aesthetics and philosophy of religion are just a pair of topics, and there isn't much to say that isn't in the discussion of their constituent parts.
- I'm doing a deeper look at epistemology in [a later chapter](#epistemologychapter), so there's no need for another discussion here.

```{r tables_for_category_individual_sections}
# Burp
category_section_main_tibble <- category_gamma %>%
  inner_join(articles, by = "document") %>%
  select(document, topic, gamma, journal, year)

category_by_topic <- category_section_main_tibble %>%
  group_by(topic, year) %>%
  dplyr::summarise(g = sum(gamma)) %>%
  ungroup() %>%
  inner_join(the_categories, by = "topic") %>%
  select(topic, year, g, category = cat_name, name = subject, name_lower = sub_lower) %>%
  inner_join(year_denominator, by = "year") %>%
  mutate(f = g/d)

journal_denominator <- articles %>%
  select(document, journal, year) %>%
  group_by(journal, year) %>%
  dplyr::summarise(d = n_distinct(document))

category_by_journal <- category_section_main_tibble %>%
  inner_join(the_categories, by = "topic") %>%
  select(document, journal, year, gamma, category = cat_name) %>%
  group_by(category, journal, year) %>%
  dplyr::summarise(g = sum(gamma)) %>%
  ungroup() %>%
  inner_join(journal_denominator, by = c("year", "journal")) %>%
  mutate(f = g/d)  

subtopic.labs <- the_categories$subject
names(subtopic.labs) <- the_categories$topic

category_by_journal$journal <- factor(category_by_journal$journal, levels = journal_order)

```

```{r graph-style-for-categories}
# Burp
cg <- function(cate, cate_num){
  yupper <- max(category_by_journal$f, na.rm=TRUE)

facet_labels <- chap_two_facet_labels %>%
  mutate(year = 1944.5, f = yupper)

facet_labels$journal <- factor(facet_labels$journal, levels = journal_order)

ggplot(data = filter(category_by_journal, category == cate), aes(x = year, y = f))  +
      facetstyle +
      scale_y_continuous(breaks = c(0.25, 0.5),
                       minor_breaks = 0.05 * 1:14) +
      geom_point(size = 0.15, colour = hcl(h = (cate_num-1)*30+15, l = 65, c = 100)) +
      theme(legend.position="none",
          panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank()) +
      labs(x = element_blank(), y = "Weighted Proportion of Articles") +
      facet_wrap(~journal, ncol = 3) +
    geom_text(data = facet_labels,
            mapping = aes(label = short_name),
            vjust = "inward", 
            hjust = "middle",
            fontface = "bold", 
            size = 3,
            colour = "grey40")
}

cg_alt_text <- function(cate, cate_num){
cate_lower <- str_to_lower(cate)
temp <- category_by_journal %>% 
  filter(category == cate) %>% 
  group_by(journal) 
temp_mean <- temp %>% 
  summarise(m = percent(mean(f), accuracy = 0.1))
temp_max <- temp %>% 
  slice_max(f, n = 1) %>% 
  mutate(maxf = percent(f, accuracy = 0.1)) %>% 
  select(journal, maxf, maxyear = year)
temp_min <- temp %>% 
  slice_min(f, n = 1) %>% 
  mutate(minf = percent(f, accuracy = 0.1)) %>% 
  select(journal, minf, minyear = year)
temp_main <- temp_mean %>% 
  left_join(temp_max, by = "journal") %>% 
  left_join(temp_min, by = "journal")
  ttt <- paste0(
    "Twelve scatterplots showing which percentage of the articles in each journal in each year are in the category ",
    cate_lower,
    ". A brief summary of the data follows. "
  )
  for (ijk in 1:12){
    ttt <- paste0(ttt,
                  "In an average year in ",
                  temp_main$journal[ijk],
                  ", ",
                  temp_main$m[ijk],
                  " of the articles are in the category ",
                  cate_lower,
                  ". ",
                  str_to_sentence(cate),
                  " is most prevalent in ",
                  temp_main$journal[ijk],
                  " in ",
                  temp_main$maxyear[ijk],
                  " when it accounts for ",
                  temp_main$maxf[ijk],
                  " of the articles in the journal. And it is least prevalent in ",
                  temp_main$minyear[ijk],
                  " when it accounts for ",
                  temp_main$minf[ijk],
                  " of the articles in the journal. "
                  )
  }
  ttt
}

cgt <- function(cate, cate_num){

yupper <- max(filter(category_by_topic, category == cate)$f, na.rm = TRUE)

facet_labels <- category_by_topic %>%
  group_by(topic, name, category) %>%
  summarise(f = 0.065) %>%
  mutate(year = 1944.5)

print(
  ggplot(data = filter(category_by_topic, category == cate, f < 0.0601, year > 1900), aes(x = year, y = f))  +
    geom_text(data = filter(facet_labels, category == cate),
            mapping = aes(label = name),
            vjust = "inward", 
            hjust = "middle",
            fontface = "bold", 
            size = 3,
            colour = "grey40") +
    facetstyle +
    geom_point(size = 0.15, colour = hcl(h = (cate_num-1)*30+15, l = 65, c = 100)) +
    theme(legend.position="none",
          panel.grid.minor = element_line(color = "grey90", size = 0.03),
          panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank()) +
    labs(x = element_blank(), y = "Weighted Proportion of Articles") +
    facet_wrap(~topic, ncol = 3) +
    scale_y_continuous(breaks = c(0.02, 0.04),
                       minor_breaks = 0.005 * 1:11,
                       lim = c(0, 0.0651))
)

# Table of Data Points that are excluded by the topic graph
if (max(filter(category_by_topic, category == cate, year > 1900)$f > 0.0601)){
print(
     kable(filter(category_by_topic, category == cate, f > 0.0601, year > 1900) %>%
          select(name_lower, year, f) %>%
          mutate(f= round(f, 4)) %>% 
          mutate(name_lower = fcap(name_lower)),
        caption = paste0("Points excluded from topic graph for ",str_to_lower(cate)),
        col.names = c("Subject", "Year", "Frequency"))
)
}

}

cgt_alt_text <- function(cate, cate_num){
cate_lower <- str_to_lower(cate)
temp <- category_by_topic %>% 
  filter(category == cate) %>% 
  filter(year > 1900) %>% 
  group_by(name_lower) 
temp_mean <- temp %>% 
  summarise(m = percent(mean(f), accuracy = 0.1))
temp_max <- temp %>% 
  slice_max(f, n = 1) %>% 
  mutate(maxf = percent(f, accuracy = 0.1)) %>% 
  select(name_lower, maxf, maxyear = year)
temp_main <- temp_mean %>% 
  left_join(temp_max, by = "name_lower")
  ttt <- paste0(
    nrow(temp_main),
    " scatterplots showing which percentage of the articles in all journals in each year from 1900 onwards are in the each of the topics category ",
    str_to_lower(cate),
    ". A brief summary of the data follows. "
  )
  for (ijk in 1:nrow(temp_main)){
    ttt <- paste0(ttt,
                  "In an average year, ",
                  temp_main$m[ijk],
                  " of the articles are in the ",
                  temp_main$name_lower[ijk],
                  "topic. ",
                  str_to_upper(str_sub(temp_main$name_lower[ijk],1,1)),
                  substring(temp_main$name_lower[ijk],2),
                  " is most prevalent in ",
                  temp_main$maxyear[ijk],
                  " when it accounts for ",
                  temp_main$maxf[ijk],
                  " of the articles in all journals. "
                  )
  }
  ttt
}
```

## Ethics {#ethics-category-section}

```{r ethics-journals, fig.cap = "Proportion of each journal's yearly publications in Ethics.", fig.alt = alt_text}
cg("Ethics", 3)
alt_text <- cg_alt_text("Ethics", 3)
```

There are a lot of ethics articles in _Ethics_. That's not surprising. But what's really the story here is that the proportion of _Ethics_ that is devoted to ethics keeps rising. The two most recently added journals, _Noûs_ and _Philosophy and Public Affairs_, respectively have good and great ethics coverage, so that helps the upward trend as well. But the big story here is the consolidation of the key specialist journal. We will see this trend a few times as we move along.

Let's look at how this is broken up into individual topics. I'm going to do a graph like this for every category, and they will all have the following restrictions.

- The nineteenth-century is excluded, and the years start in 1901.
- The Y-axis is capped at 0.06, i.e., 6 percent, and if there are points that are thereby excluded, I'll list them separately in a table.

Both restrictions are motivated by the problems that affected the frequency graphs in the previous chapter. (What I'm doing here is a topic-by-topic version of the [weighted frequency of articles]({#graph-wfa}) graph.) There isn't enough diversity in the early years, so some topics can be 10, 20, even 30 percent of the total. And there isn't a good way to represent those numbers on a graph while also making the differences between 1 percent and 3 percent (which is what we care most about here) particularly visible. So I'll just list separately the problematic data points.

```{r ethics-topics, fig.height=10.2, fig.cap = "Topics in ethics.", fig.alt = alt_text}
cgt("Ethics", 3)
alt_text <- cgt_alt_text("Ethics", 3)
```

A quick note about the he subtopic "OLP Ethics". The "OLP" here is for ordinary language philosophy. So this subtopic is the ethics part of ordinary language philosophy. I'm moderately confident that this really is part of ethics—it includes papers like Philippa Foot's "[Moral Beliefs](https://philpapers.org/rec/FOOVB)". But I'm not 100 percent sure about this, since it includes a lot of things that don't feel exactly like ethics papers as we'd currently understand that. In general, ordinary language philosophy makes this kind of analysis hard.

Note how there are a few topics that peak one after the other: first value, then promises and imperatives, then duties, and now norms. But none of this rise and fall is visible in the overall graph, which is fairly steady. Some of these rises and falls comes from changing interests among folks in the field, but some I think comes from simply changes in fashionable terminology.

## History {#history-category-section}

```{r history-journals, fig.cap = "Proportion of each journal's yearly publications in history of philosophy.", , fig.alt = alt_text}
cg("History of Philosophy", 4)
alt_text <- cg_alt_text("History of Philosophy", 4)
```

None of these lines seem to be trending up, and several of them are trending down. But there are differences in the ways that they are trending.

- _Ethics_ is a slow, steady decline.
- _Journal of Philosophy_ is a very rapid decline over the 1960s, between a high equilibrium before that, and a much lower equilibrium after.
- _Mind_ falls off a cliff around World War II, around the same time Ryle takes over.
- _Philosophy and Phenomenological Research_ falls away around 1990, as they stop taking quite as seriously the part of their name that says "phenomenological". (I don't know how closely this tracks with Ernest Sosa becoming editor of PPR.)

So part of what's going on is what I mentioned in the previous section. Specialist journals are focusing more on their specialty, and this means doing less of everything else. And in part this feels like the long influence of Gilbert Ryle. This might seem odd given that Ryle published in history of philosophy, including several papers on early modern and on phenomenology, but there is some independent evidence of this. This is from an article his successor as _Mind_ editor, D. M. Hamlyn, [wrote about Ryle's editorship](https://www.jstor.org/stable/23955672?seq=5#metadata_info_tab_contents).

> [Ryle] said, for example, that, except for Greek Philosophy, he tended to not accept papers on the history of philosophy, and he also said something about what was normally the desirable maximum length for papers. I must say that I found what he had to say on the first point rather odd, though he was absolutely right on the second. [@Hamlyn2003, 5-12]

Whatever Hamlyn thought, it doesn't seem to have affected the publications much. You can't see his ascension (in 1971) in the data here. And I think this decision of Ryle's (or of the board of _Mind_ at the time Ryle was appointed) had a large impact on the role of journal articles in history of philosophy. (Though note Hamlyn says in that paper that Ryle left him a backlog of 2 years' material to publish. So any break would be 1973 not 1971. Both these facts, that _Mind_ was only publishing its backlog in 1972 and 1973, and that they were still papers accepted by Ryle, are quite interesting in the context of the differences in citation rates between _Mind_ and its US counterparts in those years. But that's a story for another study.)

```{r history-topics, fig.height=6.2, fig.cap = "Topics in history of philosophy.", fig.alt = alt_text}
cgt("History of Philosophy", 4)
alt_text <- cgt_alt_text("History of Philosophy", 4)
```

Two things jump out here. 

One is that when history of philosophy was a major part of the journals, it was driven by the weird mix from [other history](#topic04), not the big names. Remember that the paradigm article from Topic 4 was about James Marsh. 

The other is that [Dewey and pragmatism](#topic05) is also a big part of the story, at least when history is prominent in the journals.

But note that both of these things suggest that to some extent the strength of history before World War II is a touch overstated. Other History includes a bunch of papers that are, if history, very much the first draft of history. For example, for several decades _Philosophical Review_ published an annual piece on philosophy in France, usually written by André Lalande. (A bunch of these papers can be accessed [via PhilPapers](https://philpapers.org/s/%22Philosophy%20in%20France%22%20Lalande), but I haven't looked at many of them personally.) It's kind of history—it surveys what has happened in a place in a historical period. It's just that the period was, you know, the year that just passed. So not really that historical. All of these articles are classed as history by the algorithm. That doesn't explain what happened with _Mind_, of course, but there is stuff there that is only history for want of somewhere else to put it.

And while many of the pragmatism papers are genuinely history papers—they are about works by Dewey several decades after those works were written—some of them are simply works of pragmatist philosophy. (And of course Dewey is still alive, and even publishing, into the 1940s. I think it's possible to do historical work on a figure while they are still alive, but it's a bit odd.)

So I don't think we should think there was really a golden age for publishing history of philosophy in these journals that has now ceased. They did publish things that were historical. And papers on Marsh or on Geulincx are really a kind of history that we just don't see in the current journals. Maybe we need to bring back that kind of history of philosophy. (As I'll discuss at the [end of the book](#imprint-section), there is some evidence that we are bringing that kind of history back.)

## Logic and Mathematics


```{r lm-journals, fig.cap = "Proportion of each journal's yearly publications in logic and mathematics.", fig.alt = alt_text}
cg("logic and mathematics", 6)
alt_text <- cg_alt_text("Logic and Mathematics", 6)
```

The story of this category is a rise until midcentury and a fall afterwards. The journals start to give us a few clues as to what happened.

- _Analysis_ starts off being very welcoming to logic and mathematics—so much so that it messes up the scales for these graphs a bit—but gets progressively less so over time.
- _Noûs_ follows the same pattern.
- _Mind_ under Hamlyn really stops doing any of this kind of work, though it returns in a hurry once Hamlyn leaves.
- _Philosophy of Science_ moves strongly away from doing logic and mathematics work. And given the number of papers _Philosophy of Science_ publishes in the 1990s and 2000s, this affects the numbers a lot.

So part of the story here, perhaps a large part, is a familiar refrain. The specialist journals are getting more specialist, and this is crowding out other things. But the downward trajectory in _Analysis_ and _Noûs_ matters too. Let's look by topics.

```{r lm-topics, fig.cap = "Topics in Logic and Mathematics", fig.alt = alt_text}
cgt("Logic and Mathematics", 6)
alt_text <- cgt_alt_text("Logic and Mathematics", 6)
```

That makes a bit clearer what happened. Two topics associated with positivism—[definitions](#topic06) and [verification](#topic15)—are in the category. And they peaked in midcentury and then fell.

They peaked at slightly different times, which I wouldn't have expected. I guess a lot of the work in definitions is really trying to make positivism work, by getting a working account of definitions. And once verification itself becomes a topic, it's because people are starting to think that the verification principle doesn't have a defensible disambiguation. It's not so much logic fell away, as that _logical_ positivism did. And we knew that would happen.

It's only up to 2 percent, but there is an interestingly steady rise in work on theories of [truth](#topic59). These play an important role in contemporary theories of logic, especially in motivating nonclassical theories. The optimistic take on this category is that the topics that have fallen are now basically at 0, so they can't fall farther, while other topics are steadily rising. I'm not sure I buy that—I suspect we'll see more falls if we extend this study beyond 2013—but it's one reason for optimism.

## Metaphysics {#metaphysics-category-section}

```{r metaphysics-journals, fig.cap = "Proportion of each journal's yearly publications in Metaphysics", fig.alt = alt_text}
cg("Metaphysics", 7)
alt_text <- cg_alt_text("Metaphysics", 7)
```

The short version is that metaphysics is at worst flat in every journal, and rising rapidly in _Analysis_, _Philosophical Quarterly_ and _Philosophy and Phenomenological Research_. That's fine, but shouldn't we see positivism showing up somewhere that put downward pressure on metaphysics at least somewhere? If that happens, it isn't visible in the journal-level data. What happens at the topic-level?

```{r metaphysics-topics, fig.height = 6.2, fig.cap = "Topics in Metaphysics", fig.alt = alt_text}
cgt("Metaphysics", 7)
alt_text <- cgt_alt_text("Metaphysics", 7)
```

So there are two big topics—[modality](#topic80), and [composition and constitution](#topic89)-that only take off after positivism goes away. The same is probably true of [causation](#topic54) as well, but it's much smaller. I was surprised that causation was so much smaller than the other very modern categories; maybe that's in part because some of the causation work ended up getting classified as philosophy of science.

But we still haven't answered the question about where positivism shows up. I thought that the graph here would be U-shaped, with lots of metaphysics on either side of a positivism-driven dip in the middle. We can recreate that gap if we declare that all the work in idealism is really metaphysics.

```{r metaphysics-and-idealism-frequency, fig.cap = "Proportion of each year's articles that are in idealism or metaphysics.", fig.height = 5, fig.alt = alt_text}
cate_num <- 7
m_and_i_table <- category_frequency %>%
  filter(category == "Metaphysics" | category == "Idealism") %>%
  group_by(year) %>%
  dplyr::summarise(f = sum(f))

m_and_i_graph <- ggplot(data = m_and_i_table, aes(x = year, y = f)) +
      spaghettistyle +
      geom_point(size = 0.5, colour = hcl(h = (cate_num-1)*30+15, l = 65, c = 100)) +
      theme(legend.position="none") +
      ylim(0, 0.4) +
      labs(x = element_blank(), y = "Weighted Proportion of Articles")
  
m_and_i_graph

alt_text <- "A scatterplot showing the proportion of articles that are in either idealism or metaphysics. It starts around 10%, rises to over 30% in the early 20th century, then falls very linearly back to around 10% by 1940. It is then fairly stable, with perhaps a small upwards trend over the last 50 years that isn't obviously significant."
```

That's more like what I expected, though the postpositivism rise is much smaller than the prepositivism fall. It obviously looks different if using counts rather than proportions.

```{r metaphysics-and-idealism-count, fig.cap = "Weighted number of each year's articles that are in idealism or metaphysics.", fig.height = 5, fig.alt = alt_text}
m_and_i_count_table <- category_frequency %>%
  filter(category == "Metaphysics" | category == "Idealism") %>%
  group_by(year) %>%
  dplyr::summarise(f = sum(y))

m_and_i_count_graph <- ggplot(data = m_and_i_count_table, aes(x = year, y = f)) +
      spaghettistyle +
      geom_point(size = 0.5, colour = hcl(h = (cate_num-1)*30+15, l = 65, c = 100)) +
      theme(legend.position="none") +
      labs(x = element_blank(), y = "Weighted Number of Articles")
  
m_and_i_count_graph

alt_text <- "The same graph as the previous one, but showing the number of articles, not the proportion of articles, in Idealism or Metaphysics. It is a fairly linear trajectory upwards, getting to around 50 papers/year by the end of the study."
```

And now we're back to a steady rise, with no obvious place to locate the rise of positivism.

I think the big story here is that positivism has much less impact on the journals thanoneyou might think from the standard story about the history of twentieth century philosophy. If positivists are given credit for killing idealism, then positivism has an enormous influence. But I'm not sure what the case is for that. It's not like Moore was a card-carrying positivist, and he surely had something to do with the downfall of idealism.

Anyway, let's keep track of this when looking at subsequent topics, because it would be very surprising to see no impact from positivism on the literature.

## Philosophy of Language

```{r language-journals, fig.cap = "Proportion of each journal's yearly publications in Philosophy of Language", fig.alt = alt_text}
cg("Philosophy of Language", 8)
alt_text <- cg_alt_text("Philosophy of Language", 8)
```

There isn't much trend there. Within _Analysis_ I guess there is a downward trend, though it's so random before its wartime hiatus that it's hard to say. _Philosophical Review_ starts publishing philosophy of language articles in the 1940s then especially in the 1950s. (This is probably connected to Norman Malcolm moving to Ithaca.) Afterwards there is a bit of year-to-year variation, largely because the Review publishes so few articles. Before Ryle takes over there isn't a lot of philosophy of language in _Mind_, then it is fairly steady apart from a blip upward in the 1990s. But otherwise there isn't much trend here to speak of. Let's instead look at the topics.


```{r language-topics, fig.height = 6.2, fig.cap = "Topics in Philosophy of Language", fig.alt = alt_text}
cgt("Philosophy of Language", 8)
alt_text <- cgt_alt_text("Philosophy of Language", 8)
```

That's more interesting. Four of the topics are almost absent before 1940. A fifth, centered around [On "Denoting](https://philpapers.org/rec/RUSOD)", has a flurry of early century activity, then picks back up again in the 1950s. All five of them continue to be important to the present day, though a few of them look slightly down from their peaks.

But the dominant theme here is Wittgensteinian philosophy of language. It has a remarkable rise and then an equally remarkable fall. What surprises me is how much work on this topic the model finds pre-1900. It could just be noise—we're talking at most 2 percent of a small number of articles. But the model doesn't think that Fregean or Quinean philosophy of language turns up in these articles, so it's a bit interesting that it sees some Wittgensteinian work there.

The rise and fall of Wittgensteinian philosophy of language is not symmetric. The rise took away from other parts of philosophy, but when the fall happened the interest simply shifted to other areas in philosophy of language. The result was that philosophy of language was stronger afterward than before, though the work that ended up being central was not the work that made philosophers take philosophy of language seriously.

There would be more topics here, and the overall graph would trend more sharply upwards, except the model itself thought that some topics that could have been in philosophy of language are better placed in logic and mathematics. I'll come back to that in the next chapter.

## Philosophy of Mind

```{r mind-journals, fig.cap = "Proportion of each journal's yearly publications in Philosophy of Mind", fig.alt = alt_text}
cg("Philosophy of Mind", 9)
alt_text <- cg_alt_text("Philosophy of Mind", 9)
```

Again, it's hard to see much of a trend here. _Mind_ stops publishing work that is (by contemporary standards) more psychology than philosophy, and that leads to a bit of a fall. Then there is an odd fall in the 1990s, simultaneous with an upsurge in how many philosophy of language papers _Mind_ publishes. But otherwise it's like every journal decided at its foundation what proportion of its space will go to philosophy of mind, and it has stuck to that pretty closely ever since. Just what topics in philosophy of mind that they've covered is much less stable. 

```{r mind-topics, fig.height = 10.2, fig.cap = "Topics in Philosophy of Mind", fig.alt = alt_text}
cgt("Philosophy of Mind", 9)
alt_text <- cgt_alt_text("Philosophy of Mind", 9)
```

The [psychology](#topic01) topic, which I had always associated with _Mind_ pre-1900, hangs around into the 1930s. It doesn't make up a big portion of what _Mind_ does in the twentieth century, but it is an important part of the early years of _Philosophical Review_. Some of the other topics are arguably papers that could just have easily been classified with Psychology. That's true for the handful of [color/colour](#topic40) papers in the 1880s and the pre-1900 work on [emotions](#topic28). It's also somewhat true of the early work on [self-consciousness](#topic12), though some of that work is also idealist influenced.

I've already talked about the [physicalism](#topic09) topic at some length and won't repeat it here; this was a very large surprise.

OLP mind is the part of [ordinary Language philosophy](#topic24) dealing with philosophy of mind. Unlike the ethics part of ordinary language philosophy, the model thinks this was just picking up on an existing trend, rather than building something wholly new.

The more recent topics are not too surprising. Thankfully journals are now publishing more work on [cognitive science](#topic87). I would have guessed that the graph for [minds and machines](#topic58) would have started earlier, and stayed higher, than the graphs for conceivability arguments and for [concepts](#topic78), but it's not too surprising.

The Freud subtopic is tiny, but that's in part because some of the Freud articles got slid over into [intention](#topic48). I don't know what the model was thinking there, but it didn't affect the overall category graphs.

## Philosophy of Science

```{r science-journals, fig.cap = "Proportion of each journal's yearly publications in Philosophy of Science", fig.alt = alt_text}
cg("Philosophy of Science", 11)
alt_text <- cg_alt_text("Philosophy of Science", 11)
```

The trend here is fairly simple, and it resembles what I said about the [ethics category](#ethics-category-section). The specialist journals have become more specialized, and the other journals haven't changed a lot. Now in this case, that's in part because most of the journals were doing very little of (what we'd now call) philosophy of science. I think they would have disagreed with that description; some of what I've called metaphysics, and some of what I've called logic and mathematics, they would have called philosophy of science. But by modern lights they weren't doing very much of it, and they still aren't. The big, and important, exception is _Journal of Philosophy_, and it hasn't cut back its philosophy of science coverage.

```{r science-topics, fig.height = 12.2, fig.cap = "Topics in Philosophy of Science", fig.alt = alt_text}
cgt("Philosophy of Science", 11)
alt_text <- cgt_alt_text("Philosophy of Science", 11)
```

The model carves up the philosophy of science topics very finely, and so it's hard to eyeball a lot of trends. [Quantum](#topic66) goes up rapidly to a peak in the 1990s. (I did not realise it wasn't still growing.) [Evolutionary biology](#topic82) starts later, and maybe is still growing. The [realism](#topic67) wars of the 1980s didn't quite end, but maybe they are fading away. On the other hand, there is more attention being paid to [models](#topic88), and some of that is related to the realism wars.

## Social and Political

```{r social-journals, fig.cap = "Proportion of each journal's yearly publications in Social and Political Philosophy", fig.alt = alt_text}
cg("Social and Political", 12)
alt_text <- cg_alt_text("Social and Political", 12)
```

With the exception of _Philosophy and Public Affairs_, the trend seems to be down in every journal. This surprised me a lot. In part this was because the journals stopped publishing work on [history and culture](#topic10). In part it was because the model insisted that [life and value](#topic03), what I thought of as idealist ethics, was really a topic in social and political, not ethics. But in part it was because the study ended before the boom in social and political work in the mid-2010s.

```{r social-topics, fig.cap = "Topics in Social and Political", fig.alt = alt_text}
cgt("Social and Political", 12)
alt_text <- cgt_alt_text("Social and Political", 12)
```

This, I think makes the story about what happened from 1950-2013 make more sense. There was a Rawlsian boom—it shows up clearly in [egalitarainism](#topic65) and [liberal democracy](#topic52). There was even a small bit of interest in [feminism](#topic69). But the drop-off of interest in [Marx](#topic23), and in history and culture, more than made up for it. Given how often I've heard people complain about the excessive amount of Rawlsiana in the late twentieth-century journals, this was rather of a surprise. Of course, you may think that Marxism, and the French Revolution, were slightly more important subjects of detailed study than _A Theory of Justice_, no matter how good a book it is.

<!--chapter:end:04-categories.Rmd-->

# Sorting into Categories {#sortingchapter}

This chapter goes over how I got from the ninety topics that were generated by the model to the twelve categories that were the focus of the previous [chapter](#categorychapter). It goes fairly deep in the weeds, and the audience for it is really just (a) people who don't trust that I got the coding behind the previous chapter right, and (b) people who would like to learn about how to do a similar project of their own in the future. (I expect substantial overlap between these categories.)

The short version of what I was trying to do is easy. Put each of these 90 topics into one of N familiar categories, for some small value of N, so the trends are easily visible on a single graph. In practice, it got more complicated than that, as will become clear below.

Note that one of the challenges here is working out the value for N, and working out which categories it should include. One can see in the previous chapter how I answered this. But I just want to stress at the start that these answers were things that came out of the sorting methodology, not things I settled on before trying to sort the topics. So let's turn to how I did, or in the first instance didn't do, that sorting.

## Two Failed Attempts

The first thing I tried was to look at the correlations between topics. That is, for any two topics, measure the correlation between the probability the model assigns to each article being in the first topic, and the probability it assigns to each article being in the second topic. If the topics are part of a common category, this should be reasonably high.

There are some interesting results from looking at the data this way, and later I'll [talk about them more](#correlation-section). But it doesn't work well as a way of generating categories. For one thing, there are too many false positives. For another, the approach fails just when it is most needed—when the task is to sort topics that are intuitively on the border between two categories. I did rely on correlations at one point below, but mostly it was a bad idea.

So then I tried simply categorizing the topics by hand. And this got a lot of the way, but there were just too many hard cases for it to be reliable. That said, thinking about how to categorize the topics by hand led to two crucial realizations.

First, some of the topics seem so disjunctive that they don't fit naturally into any topic, but they do seem like they should be divisible in a way that makes them easier to classify.

Second, it's important to not get too "realist" about what we're trying to do here. It's a bad idea to start with the question, "Is this topic really in category X or category Y". That leads to the following mistake. Over a series of close calls, the topic in question is put in category X not Y. Even if every one of those placements is defensible, the conjunction of them is not.

The aim here is not to match some Platonic ideal of correct classification. The aim is to tell a story about what happened to philosophy over time. And if every close call gets decided the same way, that story won't be any good.

In sporting terms, this is a case where what's really needed are "make-up calls". To track how well philosophy of science, for example, was represented in these journals, then about half the close calls involving whether to put a topic in the philosophy of science category should be resolved in favour of saying it is in that category. That principle is something I'll come back to a few times in what follows.

## Splitting Up Topics {#subtopics-section}

Some of the topics just look disjunctive, no matter [how hard I tried](#methodology-chapter) to get rid of disjunctiveness. And this affects the categorization. 

Consider, for instance the [sets and grue](#topic37) topic. This just looks like it is made up of two parts—discussion of set theory, and discussion of the grue paradox. And while both of these are connected to Nelson Goodman, and more generally involve technical challenges facing a certain kind of midcentury empiricist, they aren't really connected to each other. The set theory discussion looks like it should go in either metaphysics or logic; the grue discussion looks like it should go in epistemology or philosophy of science. But putting the whole topic in any one of these four seemed mistaken.

Fortunately, there is a nice technique to resolve this problem. And it involves yet more applications of the LDA model. Take the articles that are in this topic (i.e., have a higher probability of being in this topic than in any other), and use the LDA technique to sort them into a two-topic model. I'll call this a **binary sort** in what follows. So instead of taking all `r nrow(articles)` articles and sorting them into ninety (or more) topics, just take the `r nrow(filter(relabeled_articles, topic == 37))` articles and sort them into two topics. If we're lucky, one side of the sort will be the set theory articles, and the other side will be the grue articles.^[One advantage of doing things this way rather than looking for a more and more fine-grained model of the whole universe is speed. It would be somewhat interesting to see what happened if we sorted the `r nrow(articles)` into 120 topics. But that would take something like 12 hours on a good personal computer. The binary sort I described in the text takes well under 12 seconds.]

And it turns out we are more or less lucky in just that way. Here are the keywords and paradigm articles for topic 1 in this binary sort.^[An embarrassing admission: Due to a coding error, I ended up using 1954 for the seed for these binary sorts, not 22031848 like I've used for everything else. I only realised this after I'd done so much work building on them that it would have been too much to go back and change it - especially since the value of a random seed shouldn't matter too much. But it was annoying to have had this change slip in.]

```{r set-theory-binary}
load("binary_lda/lda_37.RData")

# Get 5 key articles for each topic
temp_gamma <- tidy(binary_lda, matrix = "gamma") %>%
  arrange(-gamma)
temp_binary_one <- temp_gamma %>%
  filter(topic == 1) %>%
  top_n(15, gamma) %>%
  inner_join(articles, by = "document") %>%
  select(citation)
temp_binary_two <- temp_gamma %>%
  filter(topic == 2) %>%
  top_n(15, gamma) %>%
  inner_join(articles, by = "document") %>%
  select(citation)

# Get Word Betas
temp_topics <- tidy(binary_lda, matrix = "beta")

# Work out words with largest difference in topic probability
temp_score <- temp_topics %>%
  group_by(term) %>%
  dplyr::summarise(sumbeta = sum(beta)) %>%
  arrange(desc(sumbeta))
  
# Find Largets of each (provided at least 1/1000 probability)
temp_topics <- merge(temp_topics, temp_score) %>%
  filter(sumbeta > 0.001) %>%
  mutate(score = beta/sumbeta) %>%
  arrange(-score, -sumbeta)
  
temp_first <- temp_topics %>%
  filter(topic == 1) %>%
  slice(1:10)

temp_first_topics <- temp_first$term %>%
  paste(collapse = ", ")

temp_second <- temp_topics %>%
  filter(topic == 2) %>%
  slice(1:10)

temp_second_topics <- temp_second$term %>%
  paste(collapse = ", ")

temp_confidence <- 2 * nrow(filter(temp_gamma, gamma > 0.99)) / nrow(temp_gamma)
```

**First Subtopic**

Keywords
:    `r temp_first_topics`

_Characteristic Articles_

```{r set-theory-grue-articles}
for (i in 1:15){
  cat(i,". ", temp_binary_one$citation[i], " \n", sep="")
}
```

**Second Subtopic**

Keywords
:    `r temp_second_topics`

_Characteristic Articles_

```{r set-theory-set-articles}
for (i in 1:15){
  cat(i,". ", temp_binary_two$citation[i], " \n", sep="")
}
```

The first subtopic isn't exclusively articles about grue, but the keywords suggest that they are going to be a big chunk of what's there. And the second subtopic looks like set theory. The binary sort worked.

So now instead of asking how the articles in this topic should be classified, we can ask the two questions of how the articles in these subtopics should be classified.^[Just to be clear, I'm using 'topic' for the elements of the ninety-way partition that the original model generated, and 'subtopic' for elements of the two-way partition that these new binary sorts generate.] And while neither question is trivial, they seem at least a bit more tractable. Ultimately, I ended up putting set theory in logic and mathematics, and grue in philosophy of science. Just why I made those choices is for later sections, but for now I just wanted to show how the subtopics were generated.

There is one more thing we can note about this binary sort - the model is very confident in its answers. In the original ninety topic model, there is precisely one article that the model gives a probability greater than 0.99 to being in a particular topic.^[It's "[Contextualism, Hawthorne's Invariantism and Third-Person Cases](https://philpapers.org/rec/BRUCHI)" by Anthony Brueckner.] In the binary sort I just described, `r scales::percent(temp_confidence)` of the articles are such that the model gives them a probability at least 0.99 of being in one particular topic. Now obviously it's easier to be more confident in a two-way sort than a ninety-way sort. But this gives us a check of how disjunctive the model itself thinks the topic is. And I'll use that to check whether it really makes sense to split a topic up in this way.

## Summarising the Ninety Binary Sorts

So which topics should be split up in this way? To answer this, I wanted to look at three questions:

1. How confident was the binary sort that it had really found a division in the data?
2. What were the subtopics that the binary sort generated?
3. Were these subtopics from different categories?

If the answer to 1 is negative, then this technique seems too random to usefully be applied. If the answer to 3 is negative, then splitting the topic into subtopics is more trouble than its worth. And answering question 3 requires answering question 2.. So here's a summary of what the splits in each topic looked like.

```{r binary-split-function}
binary_summary <- function(i)
  {load(paste0("binary_lda/lda_",i,".RData"))

  # Get confidence
  temp_gamma <- tidy(binary_lda, matrix = "gamma")
  temp_confidence <- 2 * nrow(filter(temp_gamma, gamma > 0.99)) / nrow(temp_gamma)

  # Get Word Betas
  temp_topics <- tidy(binary_lda, matrix = "beta")

  # Work out words with largest difference in topic probability
  temp_score <- temp_topics %>%
    group_by(term) %>%
      dplyr::summarise(sumbeta = sum(beta)) %>%
    arrange(desc(sumbeta))
  
  # Find Largets of each (provided at least 1/1000 probability)
  temp_topics <- merge(temp_topics, temp_score) %>%
    filter(sumbeta > 0.001) %>%
    mutate(score = beta/sumbeta) %>%
    arrange(-score, -sumbeta)
  
  temp_first <- temp_topics %>%
    filter(topic == 1) %>%
    slice(1:7)

  temp_first_topics <- temp_first$term %>%
    paste(collapse = ", ")

  temp_second <- temp_topics %>%
    filter(topic == 2) %>%
    slice(1:7)

  temp_second_topics <- temp_second$term %>%
    paste(collapse = ", ")

  temp_topic_name <- the_categories$subject[i]
  return(tibble(Topic = i, Subject = temp_topic_name, Confidence = temp_confidence, First = temp_first_topics, Second = temp_second_topics))}
```

```{r binary-split-table}
binary_split_table <- lapply(1:90, binary_summary) %>% bind_rows()
```

```{r binary-split-dt}
cat("<table style=\'margin-bottom:0px\'>",
    paste0("<caption>",
           "(#tab:binary-split-dt)",
           "Characteristics of the ninety binary sorts.",
           "</caption>",
           "</table>", sep =" ")
)

datatable(binary_split_table, 
          rownames = FALSE) %>% 
      formatSignif('Confidence',3) %>%
      formatStyle(1:5,`text-align` = 'left') 
```

The first column is the topic number, to help line up with the discussion in [chapter 2](#all-90-topics). The second is the name I gave to the topic. The third is how proportion of articles in the topic that the binary sort put in one or other subtopic with probability of at least 0.99. As can be seen by you scroll down, or sorting by that column, these binary sorts are often very decisive.

The next two columns are the keywords for the two subtopics. That is, they are the (relatively common) words with the highest ratio of their probability of being in one subtopic to the probability of being in another. This is enough to get a sense of what division the subtopic is making.

So pretty clearly [topic 38](#topic38) should be split up. The first subtopic is broady speaking in philosophy of biology, and its centered around issues about animal cognition and [Morgan's Canon](https://www.oxfordreference.com/view/10.1093/oi/authority.20110803100110842). The second subtopic is about some issues in (loosely speaking) Kripkean metaphysics.

And [topic 70](#topic70) looks fairly disjunctive as well. The first subtopic is about applied ethics; the second subtopic is about psychoanalysis. That's easy enough to split as well.

A lot of the others don't look like they are dividing across topic boundaries. Both sides of the [speech acts](#topic63) topic are in philosophy of language, as are both sides of [belief ascriptions](#topic72). This was actually a little disappointing about speech acts; I was hoping that the model would tease apart the Austin-inspired work from the middle of the twentieth century from the Langton-inspired work from more recent years. But it didn't find that division. (I was hoping this because a few other models I'd run had found this division. But sadly this one didn't.)

The model splits [decision theory](#topic55) into two parts, one centered around the Pasadena problem and the other around the two envelope paradox. It is far from obvious how to categorise decision theory, but it doesn't seem that it would get any easier by following this division. So I'll leave that in one piece.

The really complicated one here is [arguments](#topic55). The keywords of the first subtopic suggest it is primarily about conceivability arguments for dualism. The second subtopic is a bit more of a mixture. There is a hint (backed up by looking at the articles) that it includes some articles about arguments for incompatibilism. But the big thing about this binary sort is that it is very asymmetric. The reason it has such a high confidence measure is that most of the articles are firmly in the second subtopic.

```{r arguments-binary}
load("binary_lda/lda_55.RData")

# Get 5 key articles for each topic
dualism_temp_gamma <- tidy(binary_lda, matrix = "gamma") %>%
  arrange(-gamma)
arguments_clear_cases <- dualism_temp_gamma %>%
  filter(topic == 2, gamma > 0.99)
```

In fact of the `r nrow(dualism_temp_gamma)/2` articles, `r nrow(arguments_clear_cases)` of them have a probability greater than 0.99 of being in the second subtopic. A better way to think about what's happening here is that the binary sort didn't so much split the topic in two, as carve out a distinctive subset from the whole. I'll treat the first subtopic as being about conceivability arguments in particular, and the second as being about arguments in general.

Let's look at the rest of the topics that I decided to split up into subtopics:

```{r binary-split-dt-part-two}
kable(binary_split_table %>% 
		slice(c(24, 35,36,37,77,79, 90)) %>%
		mutate(Confidence = round(Confidence, 3)),
	  caption = "Seven disjunctive topics.",
	  align=c("l", "l", "l", "l", "l")
)
#datatable(binary_split_table %>% slice(c(24, 35,36,37,77,79, 90)), 
#          rownames = FALSE,
#          caption = "Seven Disjunctive Topics") %>%
#      formatSignif('Confidence',3) %>%
#      formatStyle(1:5,`text-align` = 'left') 
```

[Ordinary language](#topic24) is a mess, and the confidence measure isn't as high as I'd like to make a division, but the subtopics look pretty clearly disjoint. The first is about ethics, the second is about mind. And since we already have a separate topic for [contemporaneous British philosophy of language](#topic22), splitting this topic into ethics and mind seems like a sensible plan.

[Freedom and free will](#topic35) is really the one case where the model got thrown by the fact that two different philosophical debates use a common word. But the subtopics bail us out here, splitting it into debates about free will and debates about political freedom.

[Crime and punishment](#topic36) could arguably have been left alone. But it looked to me like the first subtopic concerns issues in ethics, and especially about forgiveness as an interpersonal relationship, and the second is about social and political philosophy.

I've already gone over [sets and grue](#topic37) at some length.

I don't quite know what happened in the original model with [Frankfurt cases](#topic77). Most of the models I built had a topic centered around Frankfurt cases. (And the ones that didn't had a distinct topic for free will, it was just that Frankfurt wasn't especially central to them.) But no model other than this one threw stuff about fiction in with them. (There was one time when the model insisted on putting works about fiction in with philosophy of biology work on function. And I spent a lot of time worrying that it was being overly influenced by the overlapping letters.) Anyway, the subtopics bail us out here—philosophy of fiction has little to do with the free will debates that this topic is primarily about.

[Races and DNA](#topic79) makes a bit more sense as a topic, but is still hard to classify. But the subtopics are easy to classify; the first is about philosophy of science, the second about social and political philosophy.

And the work on [norms](#topic90) divides reasonably neatly into language norms and ethical norms. Possibly if we ran the clock forward and included papers after 2013 we'd see more papers on epistemic norms here, and that would complicate the neat division.

So those are the divisions I made. It's helpful to have them as a table, not least because I've already been using their names in the previous chapter.

```{r table-of-subtopics}
subtopic_table <- tribble(
  ~topic, ~subject, ~sub_one, ~sub_two
)

for (j in 1:10){
  kt <- round(the_categories$topic[90+2*j]/100)
  new_subtopic_row <- tribble(
      ~topic, ~subject, ~sub_one, ~sub_two,
      the_categories$topic[kt], fcap(the_categories$sub_lower[kt]), fcap(the_categories$sub_lower[89+2*j]), fcap(the_categories$sub_lower[90+2*j])
  )
  subtopic_table <- bind_rows(subtopic_table,new_subtopic_row)
}

kable(subtopic_table, 
      col.names = c("Topic", "Subject", "First Subtopic", "Second Subtopic"),
      align=c("l", "l", "l", "l"),
      caption = "The ten topics that are divided into subtopics."
      )
```

There is one last technical point to notice. The binary sort tells us how to divide the articles that are in a topic into one or other subtopic. But to calculate the weighted sums, we also have to assign some weights to the articles that are primarily in other topics, but which have some probability of being in this topic. (This is especially pressing for the ordinary language and arguments topics.)

Happily there is a way to handle this. The topicmodels package lets us apply an LDA model out of sample. That is, once there is a model, we can ask it how probable it is that some new article, which wasn't used for generating the model, falls in one topic or another. For each of these ten topics, I went back and looked at all `r nrow(relabeled_articles)` articles, and asked for how probable it is that they are in one of these subtopics or another. I then multiplied that probability by the probability that they were in the original topic to get the probability that they landed in a subtopic. And those probabilities are what went into the weighted-sum graphs in the last chapter.

## Classifying the One Hundred Topics and SubTopics

Now there are one hundred things to be classified: eighty topics and twenty subtopics. The subtopics are individually easier to classify, so that's nice. On the other hand, I started with a hard task of classifying ninety things, and now I have the hard task of classifying one hundred things, and this doesn't look like progress.

So it's time to introduce the tool that I primarily used as a guide to classification. It's another kind of binary sort, like I used in the last section. Except now instead of applying it to all the articles in one topic, I apply it to all the articles in two categories. The intuition here is that if I've really got a good categorization, all the topics/subtopics within each of the categories should cluster together. If the topics/subtopics are not clustering, then that is a reason to be sceptical of the categorization.

Here is a clean example of how it might work. I generated a two topic LDA out of all the articles in the categories ethics and metaphysics. And for each topic, I asked what the average probability was that the article was in topic 1. Remember which topic gets labelled 1 is arbitrary, so this is just asking how close, on average, each of them was to one arbitrarily chosen end of the binary sort.

```{r category-confirm-function}
# Names and numbers of the categories, mostly for labelling purposes
cat_names <- the_categories %>%
  group_by(cat_num) %>%
  top_n(1, topic) %>%
  arrange(cat_num) %>%
  select(cat_num, cat_name)

# Sorting the articles into each category
articles_by_category <- category_gamma %>%
  group_by(document) %>%
  top_n(1, gamma) 

cat_check <- function(x, y){
# First a line break because Kable needs it
cat("\n\n")
cat("### ",str_to_sentence(cat_names$cat_name[x])," vs. ",str_to_sentence(cat_names$cat_name[y]), " {-#catbound", x, y, "}", sep="")
cat("\n\n")
  
# Retrieve the two category LDA
load(paste0("category_lda/lda_",x,"_",y,".RData"))

# Just get the article probabilities out of it
temp_bg <- tidy(binary_lda, matrix = "gamma") %>%
  select(document, btopic = topic, bgamma = gamma)

# Retrieve the articles from each category
articles1 <- articles_by_category %>%
  filter(cat_num == x)
articles2 <- articles_by_category %>%
  filter(cat_num == y)

# Put the probablities of the articles together
temp_article_list <- bind_rows(articles1, articles2) %>%
  inner_join(temp_bg, by = "document")

# Find averages by topic
temp_output <-  temp_article_list %>%
      group_by(topic, btopic) %>%
      dplyr::summarise(m = mean(bgamma)) %>%
      inner_join(the_categories, by = "topic") %>%
      filter(btopic == 1) %>%
      arrange(m) %>%
      mutate(cat_name = str_to_sentence(cat_name), sub_lower = str_to_sentence(sub_lower)) %>% 
      ungroup() %>%
      select(cat_name, sub_lower, m) %>%
      mutate(m = round(m, 3))

# Output to a kable

print(kable(temp_output, 
      col.names = c("Category", "Subject", "Mean Probability"),
      caption = paste0("Comparing articles in ",str_to_lower(cat_names$cat_name[x])," and ",str_to_lower(cat_names$cat_name[y]), "."),
      align=c("l", "l", "l")) %>% kable_styling(full_width = F)
      )
}
```

```{r example-category-sort}
x <- 3
y <- 7
  
# Retrieve the two category LDA
load(paste0("category_lda/lda_",x,"_",y,".RData"))

# Just get the article probabilities out of it
temp_bg <- tidy(binary_lda, matrix = "gamma") %>%
  select(document, btopic = topic, bgamma = gamma)

# Retrieve the articles from each category
articles1 <- articles_by_category %>%
  filter(cat_num == x)
articles2 <- articles_by_category %>%
  filter(cat_num == y)

# Put the probablities of the articles together
temp_article_list <- bind_rows(articles1, articles2) %>%
  inner_join(temp_bg, by = "document")

# Find averages by topic
temp_output <-  temp_article_list %>%
      group_by(topic, btopic) %>%
      dplyr::summarise(m = mean(bgamma)) %>%
      inner_join(the_categories, by = "topic") %>%
      filter(btopic == 1) %>%
      arrange(m) %>%
      mutate(cat_name = str_to_sentence(cat_name), sub_lower = str_to_sentence(sub_lower)) %>% 
      ungroup() %>%
      select(cat_name, sub_lower, m) %>%
      mutate(m = round(m, 3))

# Output to a kable

print(kable(temp_output, 
      col.names = c("Category", "Subject", "Mean Probability"),
      caption = paste0("Comparing articles in ",str_to_lower(cat_names$cat_name[x])," and ",str_to_lower(cat_names$cat_name[y]), "."),
      align=c("l", "l", "l")) %>% kable_styling(full_width = F)
      )
```

That works pretty well. There is a very clean split between the ethics subjects and the metaphysics subjects. (I'll use "subject" from now on to refer to both the topics and the subtopics that are being classified.) The only one vaguely in between is Frankfurt cases, and that even makes sense; until recently free will was a subject in metaphysics textbooks as much as ethics textbooks.

Five caveats before I continue.

First, which category bunches near 0 and which bunches near 1 is completely arbitrary and doesn't mean anything. What matters is the bunching.

Second, the bunching matters much more than which is on either side of 0.5. Hopefully there will be a big gap in the means somewhere, and that will correspond to a category boundary. But especially when the categories are of different sizes, that gap might be a long way from the midpoint.

Third, this is just a tool. There are going to be cases it gets wrong, and I'll correct them by hand. But it's a surprisingly powerful tool, and I'll defer to it in a lot of close calls.

Fourth, I didn't just find these categories lying around and used this tool to confirm them. There was a lot of juggling around to get it to a point where most of these automatic classifications agreed with my classifications, and the ones that didn't were easy enough to explain. And the nature of LDAs is that any change somewhere creates changes everywhere. The methodology here involves a fair bit of trial and error. But some of the methodology is clear enough. Look back at that table. If I'd put Frankfurt cases in metaphysics, the very same test would tell me to move them into ethics. So this method doesn't just confirm that a classification gets things broadly right, it can say where a classification is going wrong. To be sure, it's basically an equilibrium method, and it doesn't rule out other equilibria. But it's interesting to have found even one.

Fifth, I'm not applying this to three categories: idealism, aesthetics and philosophy of religion. Both aesthetics and philosophy of religion really don't have borderline cases. And the technique I'm using doesn't work so well when the categories are of very uneven size. Therefore, it often gives wonky results for those two categories. (It also on occasion gives some of the cleanest splits - but it feels random when it does.) Still, I wasn't worried about those two categories. And idealism is a special case that I'll come back to [at the end of the chapter](#idealismsection).

I'm looking at nine categories to check that the boundaries between them are drawn roughly correctly. And that means there are nine choose two, i.e., thirty-six, boundaries to look at. These are in subsections, not sections, so they don't show up in the sidebar. I've included links here to the list of boundaries that I'll survey.

1. [Epistemology vs. Ethics](#catbound23)
2. [Epistemology vs. History of philosophy](#catbound24)
3. [Epistemology vs. Logic and mathematics](#catbound26)
4. [Epistemology vs. Metaphysics](#catbound27)
5. [Epistemology vs. Philosophy of language](#catbound28)
6. [Epistemology vs. Philosophy of mind](#catbound29)
7. [Epistemology vs. Philosophy of science](#catbound211)
8. [Epistemology vs. Social and political](#catbound212)
9. [Ethics vs. History of philosophy](#catbound34)
10. [Ethics vs. Logic and mathematics](#catbound36)
11. [Ethics vs. Metaphysics](#catbound37)
12. [Ethics vs. Philosophy of language](#catbound38)
13. [Ethics vs. Philosophy of mind](#catbound39)
14. [Ethics vs. Philosophy of science](#catbound311)
15. [Ethics vs. Social and political](#catbound312)
16. [History of philosophy vs. Logic and mathematics](#catbound46)
17. [History of philosophy vs. Metaphysics](#catbound47)
18. [History of philosophy vs. Philosophy of language](#catbound48)
19. [History of philosophy vs. Philosophy of mind](#catbound49)
20. [History of philosophy vs. Philosophy of science](#catbound411)
21. [History of philosophy vs. Social and political](#catbound412)
22. [Logic and mathematics vs. Metaphysics](#catbound67)
23. [Logic and mathematics vs. Philosophy of language](#catbound68)
24. [Logic and mathematics vs. Philosophy of mind](#catbound69)
25. [Logic and mathematics vs. Philosophy of science](#catbound611)
26. [Logic and mathematics vs. Social and political](#catbound612)
27. [Metaphysics vs. Philosophy of language](#catbound78)
28. [Metaphysics vs. Philosophy of mind](#catbound79)
29. [Metaphysics vs. Philosophy of science](#catbound711)
30. [Metaphysics vs. Social and political](#catbound712)
31. [Philosophy of language vs. Philosophy of mind](#catbound89)
32. [Philosophy of language vs. Philosophy of science](#catbound811)
33. [Philosophy of language vs. Social and political](#catbound812)
34. [Philosophy of mind vs. Philosophy of science](#catbound911)
35. [Philosophy of mind vs. Social and political](#catbound912)
36. [Philosophy of science vs. Social and political](#catbound1112)

```{r cat-sort-01}
cat_check(2, 3)
```

I'm doing these in alphabetical order, and that means the first cab off the rank is one of the trickiest. Three of the epistemology topics: [formal epistemology](#topic84), [knowledge](#topic74) and [justification](#topic76) are clear enough. And most of the ethics topics are clear enough. But there are two that are hard.

One is [arguments](#topic55). Why should this go with epistemology? Everyone uses arguments. I've put it in epistemology for three reasons. First, as you can see, the model puts it there. (And we'll keep seeing that as we look through the comparisons between epistemology and other categories.) Second, once you extract the dualism arguments from the topic, what's left are primarily papers about what we can learn from arguments. And those feel like epistemology papers to me. And third, there are a few other close calls where I put something that could go in epistemology elsewhere, and getting the overall shape of the graphs right felt like I needed to have one close call go this way.

The other strange one is [decision theory](#topic57). It's easy to think that would simply go with the other probability papers in epistemology or maybe philosophy of science. But it's ended up in ethics for two reasons. One is that, as the table shows, that's where the automatic sorter put it. But the other comes from thinking about what articles are left in that topic. Given that formal epistemology exists as a topic, and that [theory testing](#topic56) also exists, there isn't as much directly about probability in this topic. What is left is primarily papers about value functions. They are very technical questions about value functions, to be sure, but the papers actually in this topic on the whole are more about the "value" part of expected value than the "expected" part. And that isn't absurd to group in with ethics. Put another way, what we really have in this topic is _formal ethics_, and it makes sense that goes with ethics.

```{r cat-sort-02}
cat_check(2, 4)
```

History is a fairly heterogenous category, and this technique doesn't work as well with it as the other categories. But it is fairly happy with where the episteology/history boundary is drawn.

```{r cat-sort-03}
cat_check(2, 6)
```

I would have expected the model would have really wanted to put [arguments](#topic55) in with logic and mathematics. And it is a bit of a borderline case. But it still clearly puts arguments closer to the epistemology cluster than the logic cluster. And I was surprised that [verification](#topic15) didn't look a bit more like an epistemology topic. I'll come back to why it ends up in logic and mathematics not anywhere else, but for now it looks like it shouldn't be an epistemology topic.

```{r cat-sort-04}
cat_check(2, 7)
```

A lot of philosophers use the phrase "metaphysics-and-epistemology" almost as if it is one long word, with "em-and-ee" being shorthand for an alleged field within philosophy. We don't really see any such field turning up in this model. The gap between metaphysics and epistemology is as clear as any gap between categories.

```{r cat-sort-05}
cat_check(2, 8)
```

Here is the first occasion I've had to overrule the model. It wants to put [language norms](#topic90) in with epistemology not philosophy of language. And it could go either way. But I've put it with philosophy of language for a couple of reasons. One is that the paradigm articles in this subtopic, which are often by or about Brandom, feel more like language articles than epistemology articles to me. And the other is that when I ran the models with this shifted to epistemology, a lot of the neat divisions we've already seen got less neat. Still, this is one of the trickier classifications, and I suspect with another few years of data, the model would have found a norms topic that was properly divided into ethics and epistemology.

```{r cat-sort-06}
cat_check(2, 9)
```

There are two tricky cases here, and the model has relatively firm opinions on one of them.

One is what to say about [conceivability arguments](#topic55). The binary sort I just ran is not completely sure, but it prefers to put it back in with epistemology. I didn't do that for a few reasons. One is that the subject matter sure looks like philosophy of mind to me. If papers on zombies aren't fin de siècle philosophy of mind, I don't know what is. The other is that there are several reasons to think the model might have gotten confused here. It isn't surprsing that a technique that relies entirely on string matching puts the _knowledge_ argument in with epistemology. And that's doubly so when we conceive of epistemology as including the general study of _arguments_. So this looks like a philosophy of mind topic, and there are reasons to think the model won't be smart enough to see this. Therefore, I put it in philosophy of mind.

The other tricky case is [perception](#topic47). Or at least I thought it was tricky. When I was trying to sort the topics manually, I had no idea what to do with it. But the model doesn't have any doubts at all, and I was happy to let it resolve my uncertainty.

```{r cat-sort-07}
cat_check(2,11)
```

As someone whose earliest philosophical work sat right on the boundary between epistemology and philosophy of science, I thought that there would be more borderline cases here. But the model wasn't budging. It's really certain it wants to put the four epistemology subjcts at one end. And the closest subject to them, [game theory](#topic75), is one that I would have overruled and put back in philosophy of science if it hadn't got it right. So this looks like a clear split.

I've mentioned this before, but I keep being surprised at how well the model separated [theory testing](#topic56) from [formal epistemology](#topic84). These have a lot of overlap, and the model does think theory testing is closer to epistemology than most things in philosophy of science. But ultimately it knows how to sort articles into one or the other. I find that remarkable.

```{r cat-sort-08}
cat_check(2,12)
```

Maybe if we ran the tape forward to 2020 there would be more papers on the boundary between epistemology and social and political. Social epistemology has, after all, become a thing. But restricting attention to these twelve journals, up to 2013, this frontier wasn't heavily populated.

```{r cat-sort-09}
cat_check(3,4)
```

A little bit of overlap here, because social contract theory feels as much like a ethics topic to the model as a History topic. But we know that it goes best with the other History topics. I'm not sure why [value](#topic16) feels so historical to the model though; it could be that it has so many older papers in it, but that's not a great reason.

```{r cat-sort-10}
cat_check(3,6)
```

This one is pretty easy. Note that although the model is a little uncertain about [decision theory](#topic57), it is happy to include it in ethics. This is notable because so much of decision theory is about problems about infinity, so it would not have been a surprise for it to go with logic and mathematics.

```{r cat-sort-11}
cat_check(3,7)
```

This is the one I used as an example, and it seems there aren't really any borderline cases.

```{r cat-sort-12}
cat_check(3,8)
```

Nothing too surprising here. The contemporary work on slurs might eventually generate a topic that's a borderline case. Before I thought of dividing topics in two I was somewhat tempted to classify ordinary language philosophy as philosophy of language. We can see here how bad a mistake that would have been.

```{r cat-sort-13}
cat_check(3,9)
```

Another clean division, though the gap between the mind topics and the ethics topics was surprisingly small.

I thought [intention](#topic48), at least in the form that it is in these journals, was clearly a philosophy of mind topic. It's about a special kind of mental state. But it is fairly common to classify it in with ethics. And it is so clearly tied to action theory (which is more or less in ethics), and the doctrine of double effect (which is clearly in ethics) that you can see why. But I still want to treat papers about a distinctive mental state as being in philosophy of mind, and the model more or less agrees with me.

```{r cat-sort-14}
cat_check(3,11)
```

I didn't think of the ethics/philosophy of science boundary as being a particularly hard one to identify. But this ended up being trickier than I expected. 

One issue is that [teleology](#topic38) has enough language that goes with action theory that it ends up confusing the model. 

But the other thing is what to do with [decision theory](#topic57) and [game theory](#topic75). The model clearly doesn't like breaking them up. But the material on game theory that is here is primarily evolutionary game theory as used in philosophy of biology; that's clearly philosophy of science. And it would be a stretch to say that decision theory, which is well under 0.5, is a philosophy of science topic on this basis. So I think the overall best thing to do is what I actually did do—though I can see why others might prefer something else.

```{r cat-sort-15}
cat_check(3,12)
```

Here is where I really appreciated having a model to work with. I still didn't entirely go along with what the model suggested, but it helped see what were the easy cases and what were the hard cases.

If I had to guess I would have put [abortion and self-defence](#topic71) in with ethics and [feminism](#topic78) in with social and political. But I'm very glad to have a model, not just a guess, to rely on here. And I would have made the same division with the two topics closely connected to Parfit's work: [population ethics](#topic83) and [egalitarianism](#topic65). But again, I would have been nervous about relying on guesswork, and it was nice to see the model agree.

So what to say about the two subtopics in the middle: [political freedom](#topic35) and [forgiveness](#topic36)? I think these are in practice reasonably clear cases. If political freedom isn't a topic in social and political philosophy I don't know what is. And while there are some papers about social and structural matters in forgiveness, it is enough about individual relations that I think it should go in ethics.

I suspect what's happened with both of these is that the subtopics aren't as cleanly separated as they appear. There are enough papers about law in the forgiveness subtopic that the model won't quite push it all the way into ethics. And there are enough papers about free will in the political freedom subtopic that it won't push that topic the other way.

A lot of the boundaries here are fuzzy, and this one is fuzzier than most. But I think the division I ended up making looks reasonably plausible.

```{r cat-sort-16}
cat_check(4, 6)
```

This one shouldn't have been hard, and it wasn't.

```{r cat-sort-17}
cat_check(4, 7)
```

This one could have been harder, but clearly wasn't either. Note that the model is sure that neither Heidegger nor Dewey are usefully classified with contemporary metaphysics. That's what I would have said as well, but I was worried it was a biased take on what metaphysics is.

```{r cat-sort-18}
cat_check(4, 8)
```

And still this is going smoothly. This is all a bit surprising I think. It's not like there is much in common between the different parts of history, but the binary sorts still end up grouping them all together.

```{r cat-sort-19}
cat_check(4, 9)
```

And things are still going fairly smoothly. Given how important theories of mind are to some important historical figures, I thought there might be problems here. But it wasn't.

I don't know why [conceivability arguments](#topic55) ended up seeming so historical. It makes a bit more sense that Freud would feel a bit like a historical topic.

```{r cat-sort-20}
cat_check(4,11)
```

And after  the last four went so smoothly, this one is a mess. Of all the different boundaries, this is just about the one I was least worried about, and it's one of the more spectacular failures of my classification tool.

I think what's happened here is that the binary sort decided that philosophy of physics/philosophy of biology was a more salient dividing line than contemporary philosophy/history of philosophy. So all the physics topics, broadly construed, are at the top of the list, and the biology topics are at the bottom. At least in the parts of history of philosophy that these twelve journals cover, there is a lot more physics-like work than biology-like work, so there is more history at that end. (I'm a bit surprised Ancient ended up at the physics end, I guess.) But both topics from history and topics from philosophy of science that don't neatly fit on the physics-to-biology spectrum end up clustering in the middle.

Anyway, there weren't any actual borderline cases here, so it didn't affect the classification. But we'll have cause to worry about a similar breakdown in a trickier case soon.

```{r cat-sort-21}
cat_check(4,12)
```

This one also isn't neat, and I think that it's worth going through the topics one by one to double check that we've got everything plausibly located.

The big gap is between [Hume](#topic45) and [Marx](#topic23).The default should be that one side of that is history, the other side is social and political.

[History and culture](#topic10) really doesn't feature that many articles about the history of _philosophy_. It does feature a fair few papers about history, and about the philosophy of history, and I think that's what confused the model. It's an easy case, even if the model disagrees.

[Life and value](#topic03) is harder. It's not that it should be in history; there aren't that many particularly historical papers in it. Remember this topic is something like idealist moral and political philosophy. Now there are some references back to Hegel, and that feels historical. But there are more references to Hegel in [idealism](#topic02). And most of the papers here are trying to put forward first-order philosophical claims, not doing scholarly or exegetical work. I don't know why this ended up where it did, but it doesn't feel like a history category.

[Marx](#topic23) could have easily been a history topic. But most of the papers here are Marxist analysis of politics and society, not Marx exegesis. I could have gone either way on whether it counted as history or social and political, but I would have tentatively guessed the latter, and it seems the model agrees.

On the other hand, the vast bulk of the papers in [social contract theory](#topic31) are clearly history papers. They are history of social and political philosophy, which probably confused the model.

While this is a bit of a mess, the only one that's really problematic is Life and Value. And I suspect the issue is whether that should be in either of these categories.

```{r cat-sort-22}
cat_check(6, 7)
```

This was surprisingly straightforward. I would have put [universals and particulars](#topic14) in metaphysics, or maybe philosophy of language. (It is largely about predicates, after all.) But if the model wants to include it with logic and mathematics, I'm not going to disagree.

Conversely, I could just as easily have seen [modality](#topic80) go with logic as with metaphysics. But it feels more or less natural to include it in metaphysics given the way it is covered in the late twentieth century. (And it is in the first instance a late twentiety-century topic, at least the way this model classifies things.)

This ends up being a case where the binary sort resolves some hard cases, though probably it resolves them in the way I would have done regardless.

```{r cat-sort-23}
cat_check(6, 8)
```

When I started this project, I was planning to treat logic and language as a single category. I didn't want the headache of having to think about whether, say, _On Denoting_ was intended as a contribution to philosophy of language or to logic. But then an earlier version of this technique came up with an incredibly clean division of the topics in what I was calling logic and language into two categories, so I split them up. (And added mathematics to the title of the logic category, since it is the mathematics papers that seem to be most paradigmatic.)

And then I tinkered with things and the split wasn't so clean any more.

But surprisingly the two in the middle that are "out of order" seem like the cleanest cases in the whole list. When looking through the characteristic papers on [truth](#topic59), they are exemplars of what we'd call contemporary work on logic. And if [radical translation](#topic60) isn't a topic in philosophy of language, then I'm not sure what is. So I'm happy to overrule the model on those two cases.

But I'm also happy to have the model decide for me what to say about [denoting](#topic43), and [sense and reference](#topic64), which I could just as easily have classed as language. And from the other direction, I could easily have put [analytic/synthetic](#topic34), [definitions](#topic06) and [vagueness](#topic86) into language. But I can see why the model made the choices it did, and I suspect my initial judgments were the result of a somewhat partial acquaintance with each of these topics. So I deferred to it on all those cases.

```{r cat-sort-24}
cat_check(6, 9)
```

This seems clean enough. The model gets a bit thrown by [conceivability arguments](#topic55), presumably because it associates arguments with logic. But otherwise intuition and the model line up in these cases.

```{r cat-sort-25}
cat_check(6, 11)
```

There are a couple of puzzle cases here, but I think I'm happy to overrule the model in both cases.

One is that the model has a weak preference for putting [chance](#topic44) with logic not philosophy of science. This is clearly a mistake—it obviously goes with philosophy of science.

The other is that the model has a strong preference for putting [grue](#topic37) with logic. And this is a somewhat more plausible classification. But I think it's still wrong, for three reasons.

One is that the division here isn't really between philosophy of science and logic, but between philosophy of biology and logic. All the numbers on the right tell us is where a topic lands on the spectrum from work on, say, the units of selection problem to the semantic paradoxes. It is consistent to say that the grue paradox is more like the semantic paradoxes than it is like the units of selection problem while still saying it is a problem in philosophy of science.

Another is that the model, quite understandably, has a tendency to put any subject matter that includes lots of discussion of conjunctions and disjunctions in with logic. This isn't entirely wrong, but it is overkill I think. And that's part of what is driving the classification of grue.

And a third is that the model never really likes splitting up these subtopics, and sets is clearly a logic and mathematics topic, not a philosophy of science topic.

So I'm happy to think there are reasons that the model gets this one case wrong, and also happy that it basically agreed with me on the other twenty-five cases.

```{r cat-sort-26}
cat_check(6, 12)
```

A very easy division to make, at least up to 2013. Maybe some of the recent work by folks associated with Pittsburgh and Carnegie Mellon on topics like misinformation, or scientific communities, will complicate this boundary. But up to 2013 it wasn't complicated at all.

```{r cat-sort-27}
cat_check(7, 8)
```

After a lot of successes, here we have a case where the model is of no use at all. And there is one quite tricky case here: [modality](#topic80).

The model is a bit of a mess. There is no way that [meaning and use](#topic22) is really a metaphysics topic, contrary to what the model says. And it is very strange to think it puts [composition and constitution](#topic89) with philosophy of language. So I don't give much weight to what it says about modality. But I do give some weight to it, and so I want to look a bit into whether it is right to put it in metaphysics.

Obviously in philosophy of language we do spend a lot of time talking about modals. And the topic, as you can see from the characteristic articles, has a lot of discussion about conditionals. Because pf this, there are reasons to call it a language topic.

But there are stronger reasons to call it a metaphysics topic. Here are some simple statistics to back this up. Of the 370 articles in it, forty-eight of them (about 13 percent) include the string "world" in the title, and fifty-two of them (about 14 percent) include the string "actual" in the title. These feel like they are metaphysics topics. Twenty-four articles are either by Lewis or have "Lewis" in the title, and these are almost all about modal realism. (Though three are by or about C. I. Lewis, and a couple are about counterfactuals.) Fifty of the articles have "dispos" in the title, and while some of them are kind of about conditionals, most of them are clearly metaphysics papers.

While I could see calling this a language topic, it feels more like metaphysics to me, and that's how I've classified it. 

```{r cat-sort-28}
cat_check(7, 9)
```

Once again the model doesn't provide particularly useful guidance. And the problem is clear enough - it thinks of metaphysics of mind as more part of metaphysics than of philosophy of mind. And that's totally understandable, but I think it isn't true to how philosophy currently thinks of things.

The really tricky case here is [color/colour](#topic40). That could easily go into metaphysics, since a large number of the papers are about what colors really are. But I've put it in mind partially because that's how I conceive of it, and partially to balance out some close calls going in opposite directions. It's a small topic so it doesn't make a huge difference to the category statistics, and I think this is the right way to go, but it would be easy to classify it differently. 

```{r cat-sort-29}
cat_check(7, 11)
```

The binary split the model found here wasn't between philosophy of science and metaphysics, but between physics and biology. So I have to sort some of the cases by hand.

There ended up being two spacetime subjects. What I've called [space and time](#topic50) is mostly relativistic space and time, and [classical space and time](#topic20) is pre-relativistic. Both of these could have gone just as easily in metaphysics as in philosophy of science. So it seemed like some in-between verdict was called for, and the most natural was to put relativistic work into philosophy of science, and prerelativistic work into metaphysics. The binary sort thinks I should also worry about the classification of quantum physics, but a quick look at the articles in that category—or even at which venues it includes—should persuade you otherwise.

```{r cat-sort-30}
cat_check(7, 12)
```

After several hard cases in a row, it's nice to have a simple classification. In 2020, social ontology is a big subject matter, and saying just where metaphysics ends and social and political philosophy begins is hard. In these journals through 2013, it's not so hard.

It's kind of striking how not hard it is. [composition and constitution](#topic89) would be one of the most tricky borderline subjects if I ran this study on works in contemporary philosophy. But through 2013 it's just about the easiest example for the model to classify. philosophy sometimes seems like it is spinning in place, but sometimes it changes fast, and hopefully future studies like this one will be able to track some of those changes.

```{r cat-sort-31}
cat_check(8, 9)
```

This table really surprised me. I think of mind and language as close to a single subject matter. At Michigan one of our most popular big lecture courses is called "Mind and Language". There is a really important journal called [_Mind and Language_](https://onlinelibrary.wiley.com/journal/14680017). If I was trying to study contemporary philosophy using data mining and not the history of philosophy, I probably would have included that journal because it is so important. It feels like this should be one of the trickiest boundaries to draw.

And yet, it wasn't. The only subject that gave the model any pause was the subtopic on [conceivability arguments](#topic55). I would have paused a fair bit over [wide content](#topic85), but the model didn't really worry about it.

The actual classifications the model makes all look right to me. I'm just surprised it was so definitive.

```{r cat-sort-32}
cat_check(8, 11)
```

This one, on the other hand, should have been easier. And it mostly was. Once again the model struggled with a subtopic, in this case [teleology](#topic38), but otherwise I don't see much here to quibble about. Maybe looking forward [game theory](#topic75) will become more of a borderline case, but for the most part these are separate disciplines.

```{r cat-sort-33}
cat_check(8, 12)
```

And this really brings out the difference between philosophy in 2020 and philosophy in these journals up to 2013. Social and political philosophy of language is one of the fastest growing fields in philosophy. The literature on slurs alone is big enough to be a subject in its own right. But add in work on silencing, on propaganda, on trust, lying and deception, and so on, and you have a huge body of work that should be hard to clearly sort into one of these categories. And in the journals up to 2013, we see virtually none of it. Let's look back at this when we have some more data and see how populated _this_ boundary gets.

```{r cat-sort-34}
cat_check(9, 11)
```

This came out a little neater than I expected. It got the subtopic on [teleology](#topic38) wrong again, but otherwise it looks pretty good. Given the amount of scientific work that turns up in philosophy of mind, especially in the nineteenth and twenty-first centuries, I thought this would confuse the model more than it did.

```{r cat-sort-35}
cat_check(9, 12)
```

Not many surprises here. The model doesn't quite know what to do with philosophical articles about [Freud](#topic70). I don't know what to do with philosophical articles about Freud. But otherwise it doesn't see a great deal of overlap. Maybe if we ran this study forward some of the recent work on implicit bias would confound it a little.

```{r cat-sort-36}
cat_check(11, 12)
```

The only tricky case here is [game theory](#topic75). Since game theory is in its nature about the study of social groups, it isn't surprising that the model wans to put it with science and political philosophy. But looking at the particular articles in that topic, which largely focus on evolutionary game theory, I think it's a much better fit with the other philosophy of biology articles in philosophy of science.

### Summary

I've spent a bit of time on going over all thirty-six of these boundaries for two reasons.

One is that the classifications of topics into categories is crucial for generating the category graphs. And there were a lot of choices to be made in generating that classification that could have gone either way, and I wanted to lay out a bit why I made the choices that I did.

The other is that looking at these boundaries is a pretty interesting perspective on how the future might fail to resemble the past. One tried and trusted way to make philosophical progress is to start with two areas that aren't in a lot of contact, and see what happens when you use the tools and methods of each to look at the questions of the other. We've gone through a couple of decades of doing that with ethics and epistemology, and I think the results have been very rewarding. The period since this study ends has seen an explosion of work intersecting science and political philosophy with any number of the other fields here: epistemology, metaphysics and philosophy of language being particularly important.

In general, thinking about where the sharp boundaries are in a study like this, and about what kind of work would make those boundaries less sharp, is a useful way to think about where more work could be usefully done.

## Idealism {#idealismsection}

The last thing that needs to be discussed is why I haven't put [idealism](#topic02) into any of the categories. There are a few different factors that went into this decision, and while none of them are decisive, I think it is ultimately a defensible choice.

The first thing to note is that Idealism could go in any number of the categories. It's a metaphysical view, and I thought about putting it in metaphysics. But it also involves distinctive takes on epistemology, and philosophy of mind. (Note that one of its distinctive words is "consciousness".) And it has connections to philosophy of religion (though not always Christian religion), and to aesthetics. If there are so many things it could sort of go in, it feels arbitrary to put it in any one.

Now for other issues that it felt arbitrary to put in one place rather than another, I used binary sorts to split them into classifiable topics. But that didn't work here. In fact, the binary sort I made for Idealism had the lowest proportion of articles the sort was confident about. I had a quick look at whether there was a three-way or four-way sort of these papers into more familiar topics, but that didn't help. So any choice here would be arbitrary.

And it would be an arbitrary choice with huge ramifications for how several of the graphs look. If I draw the graphs from the last chapter with idealism in one or other category, that category looks like it spent most of the twentieth century in almost terminal decline. There are several small topics that were somewhat arbitrarily classified, but they typically didn't affect the shape of the graphs that much. That's in part because I tried to balance out the close calls. But idealism is too big for that; it dominates the early decades so much that nothing can make up for it. And recall that the point here is not to get some objectively correct classification of philosophical topics, it's to tell a sensible story about twentieth century philosophy. Saying that idealism is part of metaphysics, and the story of twentieth century philosophy is the continuing retreat of metaphsyics, is really not a sensible story. And the same would be true anywhere else I put idealism. The story we get from the graphs is the continuing growth of realism, and that I think is a sensible story.

(As an aside, one aspiration I have for future versions of this book is that it will include a simple app where readers can see how the category graphs would look if they make different decisions about where the topics went. That is possible now with some coding skills, but I'd like to be able to make it easy for anyone to draw their own category graphs.)

Finally, Idealism would be a real outlier topic in any category it was in. To check whether this was true, I built binary sorts out if Idealism plus each of the other big categories, and looked at the average probability of being in topic 1 for every topic. In almost every case, Idealism was off on an island.

```{r cat-sort-i2}
cat_check(5, 2)
```

```{r cat-sort-i3}
cat_check(5, 3)
```

```{r cat-sort-i4}
cat_check(5, 4)
```

```{r cat-sort-i6}
cat_check(5, 6)
```

```{r cat-sort-i7}
cat_check(5, 7)
```

```{r cat-sort-i8}
cat_check(5, 8)
```

```{r cat-sort-i9}
cat_check(5, 9)
```

```{r cat-sort-i11}
cat_check(5, 11)
```

```{r cat-sort-i12}
cat_check(5, 12)
```

The only one that isn't clearly ruled out is history of philosophy. And while there were historical elements to Idealism, they sure did talk about Hegel a lot, for example, it doesn't feel like that's primarily what they were doing.

Ultimately, I'm using the categories here from late twentieth century philosophy. And idealism just doesn't map on to those particularly well. To some extent we can say the same thing for all of the topics that are centered around the first few decades of the study. But it is most striking for idealism. So I've given it its own category, rather than forcing it into a framework that it doesn't belong in.

<!--chapter:end:05-category_methodology.Rmd-->

# Epistemology {#epistemologychapter}

```{r epistemology_setup}
# This is the LDA for epistemology
# It loads as epistemology_lda
load("epistemology.RData")


epistemology_all_journals_gamma <- tidy(epistemology_lda, matrix = "gamma")

epistemology_all_journals_classifications <- epistemology_all_journals_gamma %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  ungroup()

epistemology_all_journals_titles_and_topics <- merge(
  epistemology_all_journals_classifications, 
  articles, 
  by = "document")

epistemology_year_topic_mean <- epistemology_all_journals_titles_and_topics %>% ungroup() %>% 
  group_by(topic)  %>% 
  dplyr::summarize(date = mean(year)) %>% 
  mutate(rank = rank(date))

epistemology_relabeled_articles <- merge(epistemology_all_journals_titles_and_topics, epistemology_year_topic_mean) %>% 
  select(-topic) %>% 
  dplyr::rename(topic = rank) %>% 
  mutate(mcitation = case_when(
    journal == "Philosophy of Science" & fpage > 10000 ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ ",vol,":S",fpage-10000,"–S",lpage-10000,"."),
    journal == "Proceedings of the Aristotelian Society" & year - vol > 1905 ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ (Supplementary Volume) ",vol,":",fpage,"–",adjlpage,"."),
#    TRUE ~ paste0(authall," (",year,") \"", title,"\" ",journal," ",vol,":",fpage,"-",lpage,".")
    TRUE ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ ",vol,":",fpage,"–",adjlpage,".")
  )
)

epistemology_yearcount <-  epistemology_relabeled_articles  %>%
  group_by(year) %>%
  dplyr::summarise(n = n_distinct(document))

epistemology_yeartopics <- epistemology_relabeled_articles  %>%
  group_by(year, topic) %>%
  dplyr::summarise(tn = n_distinct(document)) %>%
  ungroup() 

epistemology_yeargraphs <- merge(epistemology_yearcount, epistemology_yeartopics) %>%
  mutate(frequency = round(tn/n,3))

epistemology_yeargraphs$topic <- as.factor(epistemology_yeargraphs$topic)

epistemology_relabeled_gamma <- merge(epistemology_all_journals_gamma, epistemology_year_topic_mean) %>%
  as_tibble() %>%
  select(-topic) %>%
  dplyr::rename(topic = rank)

epistemology_relabeled_gamma <- merge(epistemology_relabeled_gamma, articles, by = "document") %>%
  select(document, gamma, topic, year, journal, length) %>%
  mutate(length = case_when(
    is.na(length) ~ 1,
    TRUE ~ length
  ))

epistemology_high_cite_gamma <- merge(highly_cited, epistemology_relabeled_articles, by = "document") %>% 
  arrange(desc(Cites))

epistemology_yeargamma <- epistemology_relabeled_gamma  %>%
  group_by(year, topic) %>%
  dplyr::summarise(gamsum = sum(gamma)) %>%
  ungroup() %>%
  complete(year, topic, fill = list(gamsum = 0))

epistemology_yeargamma$topic <- as.factor(epistemology_yeargamma$topic)

epistemology_article_count <- epistemology_yeartopics %>%
  group_by(topic) %>%
  dplyr::summarise(n = sum(tn))

epistemology_article_gamma <- epistemology_yeargamma %>%
  group_by(topic) %>%
  dplyr::summarise(g = sum(gamsum))
```

```{r epistemology-graph-style}
epistemologystyle  <- spaghettistyle +
    theme(legend.text = element_text(size = rel(0.75)),
          panel.grid.major = element_line(color = "grey85", size = 0.08))
```

This chapter is a detailed case study of epistemology articles in the twelve journals. I'm including this partially for self-interested reasons: I work in epistemology and I wanted to see what the field looked like. But I'm also including it because the data here makes a striking point.

Main Thesis
:    The literature on the Gettier problem is not that big.

Now I don't mean to say it's small. There is a plausible case that it is (or at least was circa 2013), the largest sub-iterature within epistemology. And for a while it was a huge proportion of what goes on in epistemology. But that time has passed, and I suspect a lot of people haven't updated their view of the field.

Within these twelve journals, the literature on the Gettier problem consists of roughly one hundred articles. That's a third of a percent of all the articles in those journals. And given the importance of the question it addresses, _What is knowledge?_, a third of a percent seems fine to me. I think it's a widespread view in philosophy that the Gettier problem literature was much bigger than it should have been. And I think that's false; a third of a percent is a perfectly reasonable proportion of the available journal space.

To analyze the epistemology literature, it would help to know what the epistemology articles are. Since I've already said that I'm treating four topics—the larger half of arguments, knowledge, justification, and formal epistemology—as the epistemology topics, one might think the thing to do would be to just take articles from those topics. But this doesn't work for a couple of reasons. In theory, which topic has the highest probability isn't that significant. Whether an article's highest probability is in one of the epistemology topics depends on just how the model carves up the space of nonepistemology topics, and that seems wrong. In practice, this method declares that "[Is Knowledge Justified True Belief?](https://philpapers.org/rec/GETIJT-4)" [@Gettier1963] is not an epistemology article, and that isn't something we can live with.

A better approach is to sum the probabilities that the model gives to an article being in one of the four epistemology topics, call that its epistemology probability, and then say that an article is an epistemology article if its epistemology probability is above a threshold. But where should the threshold go? Since I want to convince readers that the Gettier problem literature isn't that big, I want to have a very inclusive definition of epistemology, so I capture all the articles in that literature. So that militates in favour of a low threshold.

There is also the fact that epistemology articles tend to naturally slide into a lot of different topics. If they discuss scepticism at all, the model thinks they might be talking about Hume. If there is any probability talk, the model thinks they might be doing theory of confirmation or theory of chance. If they talk about which propositions a subject does or doesn't know, the model thinks they might be talking about propositions. If they talk about values, or norms, or obligations, or permissions, the model thinks they might be doing ethics. And the house style of Anglophone epistemology is close enough to the style of the ordinary language philosophers that the model constantly thinks they might be just ordinary language philosophers.

Which is all to say that the cut-off ended up being much lower than I expected. I ended up setting it at just 0.2. This seems absurd, but rather than doing the pure theory, let's look at what this looks like in practice. Here are the last eight articles that are classified as epistemology under this measure (i.e., the eight articles with a probability of being epistemology just above 0.2).

```{r epistemology-last}
epistemology_gamma <- category_gamma %>%
  filter(topic == 5501 | topic == 74 | topic == 76 | topic == 84) %>%
  group_by(document) %>%
  dplyr::summarise(g = sum(gamma)) %>%
  filter(g > 0.2) %>%
  arrange(g) %>%
  inner_join(articles, by = "document") %>% 
  mutate(mcitation = case_when(
    journal == "Philosophy of Science" & fpage > 10000 ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ ",vol,":S",fpage-10000,"–S",lpage-10000,"."),
    journal == "Proceedings of the Aristotelian Society" & year - vol > 1905 ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ (Supplementary Volume) ",vol,":",fpage,"–",adjlpage,"."),
#    TRUE ~ paste0(authall," (",year,") \"", title,"\" ",journal," ",vol,":",fpage,"-",lpage,".")
    TRUE ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ ",vol,":",fpage,"–",adjlpage,".")
  )
  ) %>% 
    select(document, g, authall, title, mcitation)

  
epistemology_cuts <- category_gamma %>%
  filter(topic == 5501 | topic == 74 | topic == 76 | topic == 84) %>%
  group_by(document) %>%
  dplyr::summarise(g = sum(gamma)) %>%
  filter(g < 0.2) %>%
  arrange(-g) %>%
  top_n(8, g) %>%
  inner_join(articles, by = "document") %>%
  mutate(mcitation = case_when(
    journal == "Philosophy of Science" & fpage > 10000 ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ ",vol,":S",fpage-10000,"–S",lpage-10000,"."),
    journal == "Proceedings of the Aristotelian Society" & year - vol > 1905 ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ (Supplementary Volume) ",vol,":",fpage,"–",adjlpage,"."),
#    TRUE ~ paste0(authall," (",year,") \"", title,"\" ",journal," ",vol,":",fpage,"-",lpage,".")
    TRUE ~ paste0(authall,", ",year,", “", toTitleCase(title),",” _",journal,"_ ",vol,":",fpage,"–",adjlpage,".")
  )
  ) %>% 
  select(mcitation)

epistemology_last_in <- epistemology_gamma %>%
  top_n(8, -g) %>%
  select(mcitation)

  for (jj in 1:8){
    cat(jj,". ", epistemology_last_in$mcitation[jj], "\n", sep="")
  }
```

Those aren't all epistemology articles, but some of them are. The Ludwig clearly is, and the Nathan, and plausibly several others. What about the eight that just missed the cut (i.e., the ten with a probability of being in epistemology just below 0.2).

```{r epistemology-first}

  for (jj in 1:8){
    cat(jj,". ", epistemology_cuts$mcitation[jj], "\n", sep="")
  }
```

That's pretty good—we don't seem to be excluding any articles that should be included. So the threshold is at 0.2.

The result of all this is that we get `r nrow(epistemology_gamma)` articles to work with. They are not distributed evenly across the years, to put it mildly. Here is how many articles there are in each year.

```{r epistemology-how-many, fig.height= 5, fig.cap = "Number of epistemology articles each year.", fig.alt = alt_text}
ggplot(data = epistemology_yearcount, aes(x = year, y = n)) +
  epistemologystyle +
  geom_point(size = 0.5) +
  theme(legend.position="none") +
  labs(x = element_blank(), y = "Number of articles", title = "Epistemology Articles") +
  coord_cartesian(ylim = c(0, 75))

epistemology_prewar <- nrow(filter(epistemology_yearcount, year < 1946))

alt_text <- "A scatterplot showing how many articles in epistemology there are each year. There are almost none before 1945, and over 60 per year in recent years, with a fairly linear trend between them. The full data are in Table D.1."
```

The gaps are for the cases where there are zero papers. Through 1945, there are only twenty-four papers. So from now on I'm going to start graphs after World War II. And I'm not going to divide things up into journals; I am not going to use up space with a graph that shows that _Philosophy and Public Affairs_ doesn't publish much epistemology. But those twenty-four papers will stay in the analysis; I just won't present them on graphs.

The next step was to take the `r nrow(epistemology_gamma)` articles and, as one might have guessed, build an LDA model for them. After a little bit of trial and error, I decided to set the number of topics to forty. I wanted to get the number to be as small as possible, while still having a topic that in some plausible sense had only Gettier problem papers.

```{r epistemology_set_up_graphs}
year_articles <- articles %>%
  group_by(year) %>%
  dplyr::summarise(n = n_distinct(document)) %>%
  mutate(cs = cumsum(n))

epistemology_yearcount_postwar <- epistemology_yearcount %>%
  filter(year > 1945) 
epistemology_yeartopics_postwar <- epistemology_yeartopics %>%
  filter(year > 1945) %>%
  complete(year, topic, fill = list(tn = 0))
epistemology_yeargamma_postwar <- epistemology_yeargamma %>%
  filter(year > 1945) %>%
  inner_join(year_articles, by= "year") %>%
  select(year, topic, gamsum, all_n = n) %>%
  inner_join(epistemology_yearcount_postwar, by = "year") %>%
  mutate(gamrat_all = gamsum / all_n) %>%
  mutate(gamrat_local = gamsum / n)
```

```{r epistemology_set_up_words}
  epistemology_topics <- tidy(epistemology_lda, matrix = "beta")

  epistemology_relabeled_topics <- merge(epistemology_topics, epistemology_year_topic_mean) %>%
    as_tibble() %>%
    select(-topic) %>%
    dplyr::rename(topic = rank)

  epistemology_word_score <- epistemology_relabeled_topics %>%
    group_by(term) %>%
    dplyr::summarise(sumbeta = sum(beta)) %>%
    arrange(desc(sumbeta))

 epistemology_busy_topics <- merge(epistemology_relabeled_topics, epistemology_word_score) %>%
   filter(sumbeta > 0.0002 * 40) %>%
   mutate(score = beta/sumbeta) %>%
   arrange(desc(score))

 epistemology_distinctive_topics <- epistemology_busy_topics %>%
   group_by(topic) %>%
   top_n(15, score) %>%
   ungroup() %>%
   arrange(desc(-topic))
```

It divided the `r nrow(epistemology_gamma)` papers up into the following forty subjects. The subject column is my subjective description of what area the papers there seem to be about. The count column is how many articles are in that subject (i.e., the model gives more probability to them being in that subject than it gives to any other subject). The weighted count is the expected number of articles in that subject (i.e., the sum across all articles of the probability of the article being in that subject). And the year column is the average publication date of the papers that are _in_ the subject. I've renumbered the topics so they are arranged by year.

```{r epistemology-subject-table}
epistemology_subjects <- tibble(topic = 1:40)

epistemology_subjects$subject <- c(
  "knowledge of mind",
"ordinary language",
"statistics",
"surprise exam",
"belief",
"Gettier",
"conditionals",
"scepticism",
"evidence",
"know-how",
"rationality",
"degree of belief",
"logic and paradoxes",
"assertion",
"virtue",
"experts",
"infinity and regresses",
"skepticism",
"perception",
"conditionalisation",
"deception",
"ethics of belief",
"semantic externalism",
"processes",
"preferences",
"decision theory",
"triviality results",
"desires",
"aim of belief",
"epistemic value",
"transmission",
"testimony",
"luck",
"bootstrapping",
"epistemic modals",
"contextualism",
"sleeping beauty",
"assertion",
"disagreement",
"accuracy"
)

epistemology_year_topic_mean$topic <- as.numeric(epistemology_year_topic_mean$topic)
epistemology_article_gamma$topic <- as.numeric(epistemology_article_gamma$topic)

epistemology_subjects <- epistemology_subjects %>%
  inner_join(epistemology_article_count, by = "topic") %>%
  inner_join(epistemology_article_gamma, by = "topic") %>%
  inner_join(epistemology_year_topic_mean, by = c("topic" = "rank")) %>%
  select(-topic.y) %>%
  mutate(g = round(g, 2), date= round(date, 1))

epistemology_subjects_fcap <- epistemology_subjects %>% 
  mutate(subject = fcap(subject))

kable(epistemology_subjects_fcap, 
      col.names = c("Topic", "Subject", "Count", "Weighted Count", "Year"), 
      align = c('l', 'l', 'c', 'c', 'c'), 
#      sep = "",
      caption = "Topics in the forty category epistemology LDA.")
```

There are several weirdnesses here that are worth listing. (I don't think this eliminates the usefulness of the model, but I should be up front about the shortcomings.)

- The larger LDA had put epistemic modals in with epistemology (reasonably enough), then followed that up by putting indicative conditionals in as well. Indicative conditionals are a tricky subject to classify, and different models treated them differently. But given their links to work on probability, and to epistemic modals, it isn't surprising they end up in epistemology. Still, it means topic 7, and topic 36, are more philosophy of language topics than epistemology.
- Similarly I could easily put topic 19 (perception), and even topic 1 (knowledge of minds), in philosophy of mind. I'm working in this chapter with a fairly broad conception of epistemology.
- I don't know why this model split topics 14 and 38, which both look like norms of assertion. I think what's going on is topic 14 is pre-Williamsonian and topic 38 is Williamsonian. But it does look a bit like a pretty arbitrary split.
- I do know what's going on with topics 8 and 18, and it's a little hilarious. The model just does string recognition, so it doesn't know that 'sceptic' and 'skeptic' are stylistic variants. But it does know they are super important words. So each of them gets its own topic.
- Splitting off topic 16 (experts) from topic 32 (testimony) was a bit weird.
- Topic 24 includes both process reliabilism work, and work from recent cognitive science on mental processes. This isn't terrible, but it's not how I would have carved things up.
- I've called topic 4 the surprise Exam, but there is also a lot of work here on the sly Pete example. I'm not entirely sure what the model saw that put these puzzles together.

In the remaining sections of this chapter I include (automatically generated) statistics, graphs, keywords and key papers from these topics, so readers can investigate them more at leisure. For now I want to talk about the broad trends, some highlights, and then especially topic 6 (Gettier).

First are the overall graphs of the raw count and the weighted count. I've included trend lines for the raw count because otherwise there are a lot of overlapping dots. And I've capped the graph at 6 to make everything clear.

```{r epistemology-main-count-graph, fig.height=9, fig.width=7.5, fig.cap = "Number of articles in each epistemology topic.", fig.alt = alt_text}
epistemology_yeartopics_postwar$topic = as.factor(epistemology_yeartopics_postwar$topic)
ggplot(epistemology_yeartopics_postwar %>% drop_na(), aes(x = year, y = tn, color=topic, group=topic)) +
  geom_smooth(se = F, method = "loess", size  = 0.3, alpha = 0.3, formula = "y ~ x") +
  geom_point(size = 0.5, alpha = 0.5) + 
  epistemologystyle +
  coord_cartesian(ylim=c(0,6))+
  labs(x = element_blank(), y = "Number of articles") +
  scale_colour_discrete(labels = epistemology_subjects_fcap$subject) + 
  theme(legend.title = element_blank(), legend.position = "bottom")
epistemology_yeartopics_postwar$topic = as.numeric(epistemology_yeartopics_postwar$topic)

alt_text <- "A scatterplot showing how many articles are in each of the 40 epistemology topics in each postwar year. It's too messy to say much about, but what can be said is in the text below. The full data is in Table D.2."

```

The "missing" data points are:

```{r epistemology-missing-data}
epistemology_missing <- epistemology_yeartopics_postwar %>%
  filter(tn > 6) %>%
  inner_join(epistemology_subjects_fcap, by = "topic") %>%
  select(year, subject, count = tn)

kable(epistemology_missing, 
      col.names = c("Year", "Subject", "Count"),
      caption = "Epistemology topics left off figure 6.2.") %>% 
kable_styling(full_width = F)
```

These points are not shown, but they are influencing the curves.

The graph is a lot of stuff, but the basic picture is fairly straightforward.

The Gettier problem was a big deal through the late 1970s and early 1980s. It's perhaps worth noting here that the model treats work on Nozick's theory of knowledge as part of the Gettier problem literature, which is fair enough, and it explains a bit of its longevity. Then there is a bunch of work on conditionals. Then a lot of modern topics become significant, and several of them seem to be as significant to the philosophical literature in the 2010s as the Gettier problem was in the 1970s.

The picture doesn't change enough if we use weighted counts rather than counts, though this does let us remove the trendlines.

```{r epistemology-weighted-count-graph}
#| fig.height=9, fig.width=7.5, 
#| fig.cap = "Weighted number of articles in each epistemology topic.", 
#| fig.alt = alt_text
epistemology_yeargamma_postwar$topic = as.factor(epistemology_yeargamma_postwar$topic)
ggplot(epistemology_yeargamma_postwar, aes(x = year, y = gamsum, color=topic, group=topic)) +
  geom_point(size = 0.5, alpha = 0.5) + 
  epistemologystyle +
#  geom_smooth(se = F, method = "loess", size  = 0.3, alpha = 0.3) +
  coord_cartesian(ylim=c(0,6.02)) + 
  labs(x = element_blank(), y = "Number of articles") +
  scale_colour_discrete(labels = fcap(epistemology_subjects$subject)) + 
  theme(legend.title = element_blank(), legend.position = "bottom")
epistemology_yeargamma_postwar$topic = as.numeric(epistemology_yeargamma_postwar$topic)

alt_text <- "The same graph as before, but using weighted counts rather than raw counts. Again, it's generally too messy to make out many clear trends, so the next graph separates the lines out, then some general results are expressed in the text. The full data for the graph is in Table D.3."
```

In this case there are just two missing data points.

```{r epistemology-gamma-missing-data}
epistemology_gamma_missing <- epistemology_yeargamma_postwar %>%
  filter(gamsum > 6.02) %>%
  inner_join(epistemology_subjects_fcap, by = "topic") %>%
  select(year, subject, count = gamsum)

kable(epistemology_gamma_missing, 
      col.names = c("Year", "Subject", "Weighted Count"),
      caption = "Epistemology articles left off Figure 6.3.") %>% 
kable_styling(full_width = F)
```

It's much easier to see what's happening here with the subjects separated out. Again, I've left off those two data points so they don't throw off the scale of the whole graph.

```{r epistemology-facet-graph}
#|fig.height=15.2, fig.width=7, 
#|fig.cap = "Forty epistemology topics.", 
#|fig.alt = alt_text
epistemology_facet_labels <- epistemology_subjects_fcap %>%
  select(topic, subject) %>%
  mutate(gamsum = 7.5, year = 1947, topic = as.factor(topic))

epistemology_yeargamma_postwar$topic = as.factor(epistemology_yeargamma_postwar$topic)

epistemology_facet_bg <- epistemology_yeargamma_postwar %>%
  mutate(bg = topic) %>%
  select(-topic)

topic.labs <- epistemology_subjects_fcap$subject
names(topic.labs) <- 1:40
ggplot(epistemology_yeargamma_postwar, aes(x = year, y = gamsum, color=topic, group=topic)) +
  geom_text(data = epistemology_facet_labels,
            mapping = aes(label = subject),
            vjust = "inward", 
            hjust = "inward",
            fontface = "bold", 
            size = 2) +
  geom_smooth(data = epistemology_facet_bg,
             aes(group = bg),
             size = 0.1,
             color = "grey85",
             alpha = 0.3,
             se = F,
             method = "loess", 
             formula = "y ~ x") + 
  geom_point(size = 0.15, alpha = 1) + 
  geom_smooth(size = 0.1, 
              alpha = 0.5, 
              method = "loess", 
              formula = "y ~ x",
              se = F) +
  epistemologystyle +
  facet_wrap(~topic, ncol = 4, labeller = labeller(topic = topic.labs)) +  
  theme(legend.position="none",
        panel.grid.major = element_line(color = "grey88", size = 0.05),
        panel.grid.minor = element_line(color = "grey88", size = 0.05)) +
  coord_cartesian(ylim=c(0,7.5)) +
  labs(x = element_blank(), y = "Number of articles")

epistemology_yeargamma_postwar$topic = as.numeric(epistemology_yeargamma_postwar$topic)

alt_text <- "The same graph as before, with each topic on its own facet. The trends are described in the text below, and the full data is in Table D.3."
```

The model makes these three weird divisions: splitting experts from testimony, having two assertion categories, and dividing skepticism and scepticism. Let's put those together, alongside the two big topics from theory of knowledge: contextualism and Gettier.

```{r epistemology-category-graph, fig.height=6, fig.cap = "Five epistemology categories.", fig.alt = alt_text}
epistemology_wide <- pivot_wider(epistemology_relabeled_gamma, id_col = document, names_from = topic, values_from = gamma)
epistemology_categories <- epistemology_wide %>%
  mutate(v.Gettier = `6`, 
         v.Testimony = `16` + `32`, 
         v.Assertion = `14` + `38`, 
         v.Scepticism = `8` + `18`, 
         v.Contextualism = `36`) %>%
  pivot_longer(cols = starts_with("v."),
               names_to = "category", 
               names_prefix = "v.") %>%
  select(document, category, gamma = value) %>%
  inner_join(articles, by = "document") %>%
  select(document, category, gamma, year) %>%
  filter(year > 1945) %>%
  group_by(category, year) %>%
  dplyr::summarise(y = sum(gamma))

ggplot(epistemology_categories, aes(x = year, y = y, color = category)) + 
  geom_smooth(se = F, size = 0.2, method = "loess", formula = 'y ~ x') +  
  epistemologystyle +
  labs(x = element_blank(), y = "Number of articles") +
  geom_point(size = 0.5, alpha = 0.5) + 
  theme(legend.title = element_blank(), legend.position = "right")

alt_text <- "A scatterplot showing the trends in five categories as described in the text. Gettier is by far the largest through the early 1980s. But then Scepticism passes it, and Testimony, Contextualism and Assertion pass it in the 1990s. Gettier ends the period as by far the smallest of the five categories. The full data is in Table D.4."
```

I think that gives a pretty good sense of what the central parts of epistemology have looked like over the last fifty or so years. The Gettier problem was the central question, for a while by far the central question. (Note that the loess curve here is well under some of the dots, so it understates the trend.) But scepticism keeps being taken more and more seriously, even if still as something haunting the land. But issues about language, and about social epistemology, are now as important as the Gettier problem ever was.

So why was the Gettier problem so widely thought to be dominating epistemology? The following four graphs might help explain this perception. I'll eventually do these for each of the 40 topics, though I won't include any commentary on any of them other than these. First, here's the graph (with trendline) of the raw number of articles about the Gettier problem each year.

```{r epistemology_article_list}
epistemology_the_articles <- epistemology_relabeled_articles %>%
  group_by(topic) %>%
  top_n(10, gamma) %>%
  ungroup() %>%
  select(gamma, topic, mcitation) %>%
  arrange(topic, gamma)
```


```{r gettier-first, fig.height=5, fig.cap = "Number of articles in topic 6, Gettier.", fig.alt = alt_text}
# First Gettier Graph for real
jjj <- 6

cate_nam <- epistemology_subjects$subject[jjj]

bg <- epistemology_yeartopics_postwar %>%
  mutate(topic = as.numeric(topic)) %>%
  inner_join(the_categories, by = "topic") %>%
  select(-topic)

print(
  ggplot(data = filter(epistemology_yeartopics_postwar, topic == jjj), 
         aes(x = year, y = tn)) +
    geom_smooth(data = epistemology_yeartopics_postwar, 
                aes(group=topic), 
                size = 0.1, 
                color = "grey85", 
                method = "loess", 
                alpha = 0.3,
                se = F,
                formula = "y ~ x") +
    scale_x_continuous(minor_breaks = 5 * 1:402,
                       expand = expansion(mult = c(0.01, 0.01))) +
    scale_y_continuous(expand = expansion(mult = c(0.01, .03))) +
    epistemologystyle +
    geom_point(size = 1.5, colour = hcl(h = (jjj-1)*(9)+15, l = 65, c = 100)) +
    geom_smooth(se = F, 
                formula = "y ~ x", 
                size = 0.5, 
                method = "loess",
                colour = hcl(h = (jjj-1)*(9)+15, l = 65, c = 100)) +
    theme(legend.position="none") +
    labs(x = element_blank(), 
         y = "Raw number of articles")
)

temp <- filter(epistemology_yeartopics_postwar, topic == jjj)

alt_text <- paste0(
  "A scatterplot showing the raw number of articles that are in the epistemology subtopic ",
  cate_nam,
  " each year from 1945-2013. The average value is ",
    round(
      mean(
      temp$tn
      ),
      2
    ),
  ", and the median value is ",
    median(
      temp$tn
    ),
  ". It reaches a peak value of ",
    slice_max(
      temp, tn, n = 1
    )$tn[1],
  " in ",
  slice_max(
    temp, tn, n = 1
  )$year[1],
  ", and has a minimum value of ",
    slice_min(
      temp, tn, n = 1
    )$tn[1],
  " in ",
  slice_min(
    temp, tn, n = 1
  )$year[1],
  "."
)
```

There were a few articles, especially in the late 1970s and early 1980s, but it doesn't look so huge. It's even less dramatic if using weighted counts.

```{r gettier-second, fig.height=5, fig.cap = "Weighted number of articles in topic 6, Gettier."}
source('epist-loop-b.R') 
```

We can also look at that as a percentage of all philosophy articles published in that year.

```{r gettier-third, fig.height=5, fig.cap = "Percentage of philosophy articles that are in topic 6, Gettier."}
source('epist-loop-c.R') 
```

At its height, it's about 1.3 percent of all the philosophy being done in a year. That's not a small number, but there are only four years where it is above 1 percent, and only four more between 0.75 percent and 1 percent. So why is it remembered as taking over everything? This graph I think is part of the explanation. Now we'll express these articles as a percentage of all epistemology being published.

```{r gettier-fourth, fig.height=5, fig.cap = "Percentage of epistemology articles in topic 6, Gettier."}
source('epist-loop-d.R')
```

For several years it was 15 to 20 percent of all epistemology that was being published. And remember that we have a very inclusive conception of epistemology, so there's a decent case that these numbers are on the low side, and it's really more like 20 to 25 percent. And that does seem excessive. So there's a reasonable case that for a while the Gettier problem literature was a rather excessive proportion of the epistemology literature. And maybe that's why it looms so large in a lot of people's impressions of what happens in epistemology.

The rest of this chapter is automatically generated. Every section covers one of these forty topics. It displays:

- The keywords for each topic.
- The raw and weighted counts of articles in that topic.
- The four graphs I just showed.
- The ten articles that have the highest probability of being in that category. (These aren't weighted by length, because so many of the significant articles are short.)
- If there are any of the six hundred highly cited articles, it includes those as well.

```{r e1a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 1 
source('epist-loop-a.R') 
```

```{r e1b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e1c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e1d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
# Burp
source('epist-loop-d.R') 
```

```{r e2a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 2 
source('epist-loop-a.R') 
```

```{r e2b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e2c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e2d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e3a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 3 
source('epist-loop-a.R') 
```

```{r e3b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e3c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e3d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e4a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 4 
source('epist-loop-a.R') 
```

```{r e4b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e4c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e4d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e5a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 5 
source('epist-loop-a.R') 
```

```{r e5b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e5c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e5d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e6a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 6 
source('epist-loop-a.R') 
```

```{r e6b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e6c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e6d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e7a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 7 
source('epist-loop-a.R') 
```

```{r e7b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e7c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e7d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e8a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 8 
source('epist-loop-a.R') 
```

```{r e8b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e8c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e8d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e9a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 9 
source('epist-loop-a.R') 
```

```{r e9b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e9c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e9d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e10a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 10 
source('epist-loop-a.R') 
```

```{r e10b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e10c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e10d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e11a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 11 
source('epist-loop-a.R') 
```

```{r e11b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e11c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e11d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e12a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 12 
source('epist-loop-a.R') 
```

```{r e12b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e12c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e12d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e13a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 13 
source('epist-loop-a.R') 
```

```{r e13b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e13c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e13d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e14a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 14 
source('epist-loop-a.R') 
```

```{r e14b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e14c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e14d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e15a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 15 
source('epist-loop-a.R') 
```

```{r e15b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e15c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e15d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e16a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 16 
source('epist-loop-a.R') 
```

```{r e16b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e16c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e16d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e17a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 17 
source('epist-loop-a.R') 
```

```{r e17b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e17c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e17d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e18a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 18 
source('epist-loop-a.R') 
```

```{r e18b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e18c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e18d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e19a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 19 
source('epist-loop-a.R') 
```

```{r e19b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e19c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e19d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e20a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 20 
source('epist-loop-a.R') 
```

```{r e20b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e20c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e20d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e21a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 21 
source('epist-loop-a.R') 
```

```{r e21b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e21c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e21d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e22a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 22 
source('epist-loop-a.R') 
```

```{r e22b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e22c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e22d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e23a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 23 
source('epist-loop-a.R') 
```

```{r e23b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e23c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e23d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e24a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 24 
source('epist-loop-a.R') 
```

```{r e24b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e24c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e24d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e25a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 25 
source('epist-loop-a.R') 
```

```{r e25b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e25c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e25d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e26a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 26 
source('epist-loop-a.R') 
```

```{r e26b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e26c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e26d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e27a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 27 
source('epist-loop-a.R') 
```

```{r e27b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e27c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e27d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e28a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 28 
source('epist-loop-a.R') 
```

```{r e28b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e28c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e28d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e29a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 29 
source('epist-loop-a.R') 
```

```{r e29b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e29c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e29d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e30a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 30 
source('epist-loop-a.R') 
```

```{r e30b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e30c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e30d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e31a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 31 
source('epist-loop-a.R') 
```

```{r e31b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e31c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e31d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e32a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 32 
source('epist-loop-a.R') 
```

```{r e32b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e32c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e32d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e33a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 33 
source('epist-loop-a.R') 
```

```{r e33b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e33c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e33d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e34a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 34 
source('epist-loop-a.R') 
```

```{r e34b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e34c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e34d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e35a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 35 
source('epist-loop-a.R') 
```

```{r e35b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e35c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e35d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e36a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 36 
source('epist-loop-a.R') 
```

```{r e36b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e36c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e36d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e37a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 37 
source('epist-loop-a.R') 
```

```{r e37b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e37c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e37d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e38a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 38 
source('epist-loop-a.R') 
```

```{r e38b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e38c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e38d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e39a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 39 
source('epist-loop-a.R') 
```

```{r e39b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e39c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e39d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

```{r e40a, fig.height = 5, fig.cap = paste0("Raw number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
jjj <- 40 
source('epist-loop-a.R') 
```

```{r e40b, fig.height = 5, fig.cap = paste0("Weighted number of articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-b.R') 
```

```{r e40c, fig.height = 5, fig.cap = paste0("Percentage of philosophy articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-c.R') 
```

```{r e40d, fig.height = 5, fig.cap = paste0("Percentage of epistemology articles in topic ",jjj, ", ", cate_nam,"."), fig.alt = alt_text}
source('epist-loop-d.R') 
```

<!--chapter:end:06-epistemology.Rmd-->

# Eras and Decades {#eraschapter}

All of the analysis so far has been done by year. But sometimes it is helpful to look at a number of years at once. In this chapter I do this twice over. 

First, I'm separating the times in the data set into five eras. My idea was to have eras that correspond to natural time periods in philosophy, but which are also roughly equal in terms of the number of articles in each era. This suggested the following five-way division.

```{r erasummary}
year_articles <- articles %>%
  group_by(year) %>%
  dplyr::summarise(n = n_distinct(document)) %>%
  mutate(cs = cumsum(n))

era <- function(x)
  case_when(
    x < 1946 ~ 1,
    x < 1966 ~ 2,
    x < 1982 ~ 3,
    x < 1999 ~ 4,
    TRUE ~ 5
  )

era_articles <- articles %>%
  mutate(epoch = era(year)) %>%
  group_by(epoch) %>%
  dplyr::summarise(n = n_distinct(document)) 

era_table <- era_articles %>%
  bind_cols(start = c(1876, 1946, 1966, 1982, 1999), end = c(1945, 1965, 1981, 1998, 2013)) %>%
  select(era = epoch, start, end, n)

era_headers <- c("1876–1945", "1946–1965", "1966–1981", "1982–1998", "1999–2013")

kable(era_table, 
      col.names = c("Era", "First Year", "Last Year", "Number of Articles"), 
      align=rep('c', 4), caption = "The five eras.")
```

Intuitively, I'm thinking of the eras the following way.

- Era 1 covers everything from idealism though positivism and World War 2.
- Era 2 is the postwar period, i.e., ordinary language philosophy in Britain and Quinean postpositivism in America.
- Era 3 is where the classic works of contemporary analytic philosophy were written by writers such as Kripke, Lewis, Putnam, Rawls, Thomson, Singer and Frankfurt.
- Era 4 is the one I have the hardest time conceptualizing, as it seems to me something of a period of consolidation between the revolutionary developments of the 1970s, and the series of new debates that open up in the 2000s.
- Era 5 is the first part of the current century, and is dominatedby a number of distinctive topics, such as reasons, vagueness, contextualism and Williamsonian epistemology and metaphysics.

In the second half of the chapter, I look at what happens when we break the data down by decades. This has the advantage of corresponding to time periods that people naturally understand, but two disadvantages. One is that data gets lost from either end of the data set. Before 1890 there are too few articles to do a useful analysis, and 2010–2013 isn't a complete decade. The other is that the number of articles is very uneven across the decades. But despite these disadvantages, I think it's helpful to get a decade-by-decade view at what was happening in the philosophy journals.

Within each of these parts, I'm going to run three studies. First, I'll look at the distribution of the ninety topics across the eras and decades. Second, I'll look at the distribution of the twelve categories across the eras and decades. And third, I'll look at the distribution of individual words across the eras and decades. The latter is useful because it doesn't involve processing everything that happened through a black box of an LDA implementation; we can just see what words were and weren't being used. As well as being interesting in its own right, this helps us check the plausibility of what the LDA comes up with.

```{r era_and_decade_setup}
# General setup tables that I'll use a lot

article_year <- articles %>%
  select(document, year)

cat_list <- the_categories %>%
  select(topic, subject)

# How common each subject is for normalisation

subject_gamma <- relabeled_gamma %>%
  group_by(topic) %>%
  dplyr::summarise(w = sum(gamma)) %>%
  inner_join(cat_list, by = "topic") %>%
  select(subject, w)

# Frequency of topic by era

era_gamma <- relabeled_gamma %>%
  select(-journal, -length) %>%
  inner_join(cat_list, by = "topic") %>%
  mutate(epoch = era(year)) %>%
  group_by(epoch, subject) %>%
  dplyr::summarise(g = sum(gamma)) %>%
  arrange(epoch,-g)

era_wide_top <- era_gamma %>%
  arrange(epoch,-g) %>%
  select(-g) %>%
  group_by(epoch) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = epoch, names_prefix = "era_", values_from = subject, id_cols = id) %>%
  slice(1:10) %>%
  select(-id)

era_wide_bottom <- era_gamma %>%
  arrange(epoch, g) %>%
  select(-g) %>%
  group_by(epoch) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = epoch, names_prefix = "era_", values_from = subject, id_cols = id) %>%
  slice(1:10) %>%
  select(-id)

era_gamma <- era_gamma %>%
  inner_join(subject_gamma, by = "subject") %>%
  mutate(f = g / w)

# Normalising to frequency of topic

era_distinctive_wide_top <- era_gamma %>%
  arrange(epoch,-f) %>%
  select(-g, -f, -w) %>%
  group_by(epoch) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = epoch, names_prefix = "era_", values_from = subject, id_cols = id) %>%
  slice(1:10) %>%
  select(-id)

era_distinctive_wide_bottom <- era_gamma %>%
  arrange(epoch,f) %>%
  select(-g, -f, -w) %>%
  group_by(epoch) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = epoch, names_prefix = "era_", values_from = subject, id_cols = id) %>%
  slice(1:10) %>%
  select(-id)

# Doing the same for categories

category_summary_gamma <- category_gamma %>%
  group_by(topic) %>%
  dplyr::summarise(w = sum(gamma)) %>%
  inner_join(the_categories, by = "topic") %>%
  group_by(cat_name) %>%
  dplyr::summarise(w= sum(w))

category_era_gamma <- category_gamma %>%
  inner_join(article_year, by = "document") %>%
  mutate(epoch = era(year)) %>%
  group_by(epoch, cat_name) %>%
  dplyr::summarise(g = sum(gamma)) %>%
  arrange(epoch,-g)

category_era_wide_top <- category_era_gamma %>%
  arrange(epoch,-g) %>%
  select(-g) %>%
  group_by(epoch) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = epoch, names_prefix = "era_", values_from = cat_name, id_cols = id) %>%
  slice(1:12) %>%
  select(-id)

category_era_gamma <- category_era_gamma %>%
  inner_join(category_summary_gamma, by = "cat_name") %>%
  mutate(f = g / w)

category_era_distinctive_wide_top <- category_era_gamma %>%
  arrange(epoch,-f) %>%
  select(-g, -f, -w) %>%
  group_by(epoch) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = epoch, names_prefix = "era_", values_from = cat_name, id_cols = id) %>%
  slice(1:12) %>%
  select(-id)

decade <- function(x)
  (x - x%%10) / 10 - 188
  

decade_gamma <- relabeled_gamma %>%
  filter(year > 1889, year < 2010) %>%
  select(-journal, -length) %>%
  inner_join(cat_list, by = "topic") %>%
  mutate(the_dec = decade(year)) %>%
  group_by(the_dec, subject) %>%
  dplyr::summarise(g = sum(gamma)) %>%
  arrange(the_dec,-g)

decade_wide_top <- decade_gamma %>%
  arrange(the_dec,-g) %>%
  select(-g) %>%
  group_by(the_dec) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = the_dec, names_prefix = "decade_", values_from = subject, id_cols = id) %>%
  slice(1:5) %>%
  select(-id)

decade_wide_bottom <- decade_gamma %>%
  arrange(the_dec, g) %>%
  select(-g) %>%
  group_by(the_dec) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = the_dec, names_prefix = "decade_", values_from = subject, id_cols = id) %>%
  slice(1:5) %>%
  select(-id)

decade_gamma <- decade_gamma %>%
  inner_join(subject_gamma, by = "subject") %>%
  mutate(f = g / w)

decade_distinctive_wide_top <- decade_gamma %>%
  arrange(the_dec,-f) %>%
  select(-g, -f, -w) %>%
  group_by(the_dec) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = the_dec, names_prefix = "decade_", values_from = subject, id_cols = id) %>%
  slice(1:5) %>%
  select(-id)

decade_distinctive_wide_bottom <- decade_gamma %>%
  arrange(the_dec,f) %>%
  select(-g, -f, -w) %>%
  group_by(the_dec) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = the_dec, names_prefix = "decade_", values_from = subject, id_cols = id) %>%
  slice(1:5) %>%
  select(-id)

# Category by Decade

category_decade_gamma <- category_gamma %>%
  inner_join(article_year, by = "document") %>%
  filter(year > 1889, year < 2010) %>%
  mutate(the_dec = decade(year)) %>%
  group_by(the_dec, cat_name) %>%
  dplyr::summarise(g = sum(gamma)) %>%
  arrange(the_dec,-g)

category_decade_wide_top <- category_decade_gamma %>%
  arrange(the_dec,-g) %>%
  select(-g) %>%
  group_by(the_dec) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = the_dec, names_prefix = "dec_", values_from = cat_name, id_cols = id) %>%
  slice(1:12) %>%
  select(-id)

category_decade_gamma <- category_decade_gamma %>%
  inner_join(category_summary_gamma, by = "cat_name") %>%
  mutate(f = g / w)

category_decade_distinctive_wide_top <- category_decade_gamma %>%
  arrange(the_dec,-f) %>%
  select(-g, -f, -w) %>%
  group_by(the_dec) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = the_dec, names_prefix = "dec_", values_from = cat_name, id_cols = id) %>%
  slice(1:12) %>%
  select(-id)

# Titles for the Decade Tables

dec_title <- 189:200 * 10
dec_title <- paste0(dec_title, "s")

# Onto the Big Words

big_word_list <- all_journals_tibble %>%
  filter(!word %in% short_words)

common_words <- big_word_list %>%
  group_by(word) %>%
  dplyr::summarise(c = sum(wordcount)) %>%
  arrange(-c) %>%
  slice(1:5000)

common_word_list <- big_word_list %>%
  filter(word %in% common_words$word)

common_word_list <- common_word_list %>%
  inner_join(article_year, by = "document")

common_word_year <- common_word_list %>%
  group_by(word, year) %>%
  dplyr::summarise(c = sum(wordcount))

common_word_total <- common_words %>%
  select(word, c)

era_words <- common_word_year %>%
  mutate(epoch = era(year)) %>%
  group_by(word, epoch) %>%
  dplyr::summarise(g = sum(c))

era_words_wide_top <- era_words %>%
  arrange(epoch,-g) %>%
  select(-g) %>%
  group_by(epoch) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = epoch, names_prefix = "era_", values_from = word, id_cols = id) %>%
  slice(1:10) %>%
  select(-id)

era_words <- era_words %>%
  inner_join(common_word_total, by = "word") %>%
  mutate(f = g / c)

era_distinctive_words_wide <- era_words %>%
  arrange(epoch,-f) %>%
  select(-g, -f, -c) %>%
  group_by(epoch) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = epoch, names_prefix = "era_", values_from = word, id_cols = id) %>%
  slice(1:10) %>%
  select(-id)

# words_by_limit(5000)

decade_words <- common_word_year %>%
  filter(year > 1889, year < 2010) %>%
  mutate(the_dec = decade(year)) %>%
  group_by(word, the_dec) %>%
  dplyr::summarise(g = sum(c))

decade_common_word_total <- common_word_year %>%
  filter(year > 1889, year < 2010) %>%
  group_by(word) %>%
  dplyr::summarise(d = sum(c))

decade_words <- decade_words %>%
  inner_join(decade_common_word_total, by = "word") %>%
  mutate(f = g / d) %>%
  mutate(dec_names = paste0((the_dec+188)*10,"s"))
# decade_words_by_limit(2000, 1)
```

```{r common_word_functions}
words_by_limit <- function(x)
  kable(
    (era_words %>%
      filter(word %in% slice(common_words, 1:x)$word) %>%
      arrange(epoch,-f) %>%
      select(-g, -f, -c) %>%
      group_by(epoch) %>%
      mutate(id = row_number()) %>%
      pivot_wider(names_from = epoch, names_prefix = "era_", values_from = word, id_cols = id) %>%
      slice(1:10) %>%
      select(-id)), 
    col.names = era_headers,
    caption = paste0("Most distinctive words in each era (among ",x," most common words)")
  )

early_decade_words_by_limit <- function(x)
  kable(
  decade_words %>%
      filter(the_dec < 7) %>%
      filter(word %in% slice(common_words, 1:x)$word) %>%
      arrange(the_dec,-f) %>%
      select(-g, -f, -d) %>%
      group_by(the_dec) %>%
      mutate(id = row_number()) %>%
      mutate(dec_names = paste0((the_dec+188)*10,"s")) %>%
      pivot_wider(names_from = dec_names, names_prefix = "", values_from = word, id_cols = id) %>%
      slice(1:10) %>%
      select(-id), 
    caption = paste0("Most distinctive words in each decade (among ",x," most common words)")
  )

late_decade_words_by_limit <- function(x)
  kable(
  decade_words %>%
      filter(the_dec >6) %>%
      filter(word %in% slice(common_words, 1:x)$word) %>%
      arrange(the_dec,-f) %>%
      select(-g, -f, -d) %>%
      group_by(the_dec) %>%
      mutate(id = row_number()) %>%
      mutate(dec_names = paste0((the_dec+188)*10,"s")) %>%
      pivot_wider(names_from = dec_names, names_prefix = "", values_from = word, id_cols = id) %>%
      slice(1:10) %>%
      select(-id)
  )

```

## Topics and Eras

I'll start by simply listing the ten most popular topics (out of the ninety) in each of the five eras. The popularity metric I'm using is the expected number of articles in that topic from that era. For each era-topic combination, I look at all articles in that era, and sum the probabilities that the article is in that topic. Then I rank the topics by this probability sum to get the following table:

```{r eratopicsa}
kable(era_wide_top, col.names = era_headers, caption = "Most popular topics in each era.")
```

The striking thing is that ordinary language philosophy comes out top in each era after World War II. This is related to the fact that it is as much a style as a topic, and the style never really went away.

We can also look at the bottom ten topics in each era, by the same metric. (The lowest ranked topics are at the top of this list. Any time I'm listing the lowest ranked anything, I'll put the lowest at the top.)

```{r eratopicsb}
kable(era_wide_bottom, col.names = era_headers, caption = "Least popular topics in each era")
```

That's not too surprising. Formal epistemology was not a big deal in philosophy pre-1945; Heidegger and Husserl haven't had much presence in these journals in this century.

But both of these tables, while somewhat useful, have a flaw. The ninety topics aren't all the same size. So this table is telling us as much about how the model carved up different parts of philosophy, as it is telling us about trends in the field. Here's a better approach. Take the probabilistic measure I just used, and divide it by the size of the topic as a whole. (That is, the expected number of articles in the topic across the whole data set from 1876-2013.) And use that to rank the topics. In effect, within each era we're ranking the topics by what propoportion of the work on that topic was in that era. So it's a measure of what were the distinctive topics within each era. Here is what we get.

```{r eratopicsc}
kable(era_distinctive_wide_top, col.names = era_headers, caption = "Most distinctive topics in each era.")
```

And that was a little surprising to me. I had not realised how much the work on crime and punishment (in these twelve journals) was concentrated in 1966–1981. If you'd given me twenty guesses for what would be the distinctive topic of this era, probably the era I care most about in all of philosophy, I wouldn't have guessed this. And I would not have guessed that promises and imperatives (which remember includes a lot of deontic logic), and sets and grue, would have been second and third.

In general, most of the topics turn up one to two eras later than the most famous works in those eras. This makes some sense; the secondary literature has to come after the primary literature. But it means that a story of the history of philosophy that concentrates on the great works will leave a misleading impression of when topics were being most discussed.

Let's do the same thing but for the topics that have the lowest share of their articles turning up in a given era.

```{r eratopicsd}
kable(era_distinctive_wide_bottom, col.names = era_headers, caption = "Least distinctive topics in each era.")
```

For me, the middle column is the most interesting one. The last two columns are pretty much what we'd expect. And the first two columns are as much noise as signal. In the first column the absolute values are all low, and one or two articles that by coincidence shared some keywords with topics from a century later could move something up or down by several spots. In the second column, the fact that the model doesn't see consequentialism and utilitarianism as having much in common drives the "top" result.

But in the middle we get an interesting mix. Idealism is over, so all the idealism-related topics are relegated. And the journals that are still publishing history are settling on just publishing work on the same few big names, so some history topics are relegated. But other topics have yet to start. It isn't a surprise that before 1982 there wasn't much work on vagueness or on evolutionary biology in these journals. (There are important articles in those fields from before 1982, but they don't tend to be in these twelve journals.) And norms is as much a style as a topic. But I'm a bit surprised cognitive science hadn't started getting more attention yet. And I'm very surprised that wide content is showing up here.

Remember that Putnam's "[Meaning and Reference](https://philpapers.org/rec/PUTMAR-2)" [@Putnam1973] is from the very middle of this era. And given the importance of twin Earth debates to philosophy of the last forty years, I would have thought it would have kicked off a huge debate. But it just didn't. I've already discussed this when talking about the [wide content topic](#topic85), but it is just shocking to me how long it takes for the literature on Wide Content to really get going.

## Categories and Eras

Let's turn from the ninety topics to the twelve more familiar categories. I'll start by ranking the categories by how many articles are in each category.

```{r cattopicsa}
kable(category_era_wide_top, col.names = era_headers, caption = "Most popular categories in each era.")
```

The first thing that jumps out is how prominent Idealism is in pre-1945 philosophy. Even though it is only one topic, and I could easily have included a couple of other prominent early topics in the category, it is pretty much above all the topics studied in contemporary philosophy. Philosophy of mind is first, to be sure, but only because of all the psychology articles that _Mind_ and then _Philosophical Review_ published in their early years. It just can't be stressed enough how big Idealism was in Anglophone philosophy for so long.

It's easy from a current perspective to think of Ethics as being central to philosophy, but before 1945 it just wasn't. Now this is somewhat an issue of classification. There is some Hegelian work that the model counts as idealism but which could be called ethics. It wouldn't be implausible to classify a lot of the papers that the model classified as life and value as in ethics rather than social and political. But still, ethics in anything like the sense we understand it in contemporary philosophy was something of a niche topic before 1945.

But none of that compares to how little attention there was to epistemology. Now again, there is a sense in which some of the idealism papers had to do with epistemology. Certainly a lot of it had epistemological motivations. But work that looks at all like contemporary epistemology virtually doesn't exist before 1945. (As I noted in the discussion of the [knowledge](#topic74) topic, the one exception is a paper from Calcutta.)

At the other end of the timeline, I was surprised that epistemology wasn't higher in 1999–2013. I certainly didn't remember that time period as being one in which logic and mathematics was a bigger presence in the journal than epistemology. So I went back and looked to see if the model was making a mistake here, or I had misremembered. And the mistake was mostly on my part.

The logic and mathematics category, at least in 1999–2013, is largely driven by two debates.

One is vagueness, and I've talked a bit back in chapter \@ref(sortingchapter) about why that is included in logic and mathematics. I think I was thinking of that as part of philosophy of language back when I was part of that debate, but the case for classifying it as Logic is reasonably strong. 

But the other debate, and numerically the larger one, is about truth. And a lot of that debate takes place in _Analysis_. If we look into why the model puts the category logic and mathematics so high up the ranks in 1999–2013, it's driven in very large part by there being so many articles about the theory of truth in _Analysis_ in those years. And if we look back at those years, the model seems pretty plausible. Indeed, the model sometimes understates things, only assigning 40 to 50 percent probability to an article being about the theory of truth when it quite clearly is about the theory of truth.

And I'd just misremembered how big that debate was. Now partially that's because I wasn't involved with it, and we have better memories for debates that we're involved with. But in part it's because while there were a lot of articles, they weren't very long. If we sorted the categories by pages, rather than by articles, the order here may have been different. And there is a case for doing that too. But there's also a case for doing things this way. If any quarterly journal publishes ten articles in a year on a particular topic, that tells you something about the importance of that topic to philosophical debates at the time. And that's true even if the articles are six to eight page _Analysis_ notes.

So while this ranking was a surprise to me, after looking at the underlying data I don't think it was a mistake. It turned out to be one of the advantages of having an analysis of the trends in philosophy that starts by looking at all the data.

The categories are different sizes, and the middle of the previous list doesn't tell us a lot that we couldn't see just by looking at the category graphs back in chapter \@ref(categorychapter). It's perhaps more interesting to rank the categories by which percentage of their works are in a given era.

```{r cattopicsb}
kable(category_era_distinctive_wide_top, col.names = era_headers, caption = "Most distinctive categories in each era.")
```

The journals have never paid a ton of attention to aesthetics or philosophy of religion. But in the postwar years, especially in America, they paid more attention to them than they otherwise did. 

History of philosophy looks quite strong during the second era, then falls away in the third era. This might make one sceptical of my story that it is [to do with Ryle's ascension to the editorship of _Mind_](#history-category-section). But note that a huge proportion of the history papers in this era are in _Philosophy and Phenomenological Research_. At that time _PPR_ was taking the third word in its name really seriously, and I'm classifying [phenomenology papers](#topic28) as part of history. This table shouldn't change your view on the impact Ryle becoming editor of _Mind_ had on history in the philosophy journals.

I've been [suspicious of the idea that there was a positivism-driven drop in metaphysics](#metaphysics-category-section). So it is only fair to note that by this measure (though seemingly no other) there really is a drop in metaphysics in the postwar era.

On the other hand, the prominence of ethics during the critical period from 1966–1981 is consistent with something I've been stressing a lot. Part of what made that era so distinctive is that so many topics in ethics were either created or renewed.

## Words and Eras {#words-eras}

For most of the book, I have worked from a dataset that started with the JSTOR word list, then made two major edits. First, I cut out words that don't look like they are part of the content of the papers I'm focussing on. Second, I cut out words that only appear one to three times in a paper. For this section, mostly still excluding the noncontentful words—though I'll note one point below when I consider them—but I'm restoring the words that appear one to three times. 

On the other hand, I'm restricting attention to the five thousand most common words in the data set. In practice, that means restricting attention to words that appear about 2,200 times or more. That is, the word has to appear about once ever fifteen articles. That doesn't mean it has to appear in one-fifteenth of the articles; it could appear very often in a few articles. But it does have to get those 2,200 appearances somewhere. The reason for this restriction will soon becme clear.

To start, here are the most common words from each era—remembering that we've filtered out a lot of "stop words".

```{r erawords1}
kable(era_words_wide_top, col.names = era_headers, caption = "Most common words in each era.")
```

That doesn't tell us a lot. It is a bit interesting that _theory_ and especially _case_ are much more prevalent after World War II than before it, but that's about it.

We learn more by comparing the frequency of each word in an era to that word's frequency across the whole data set. So rather than ask how many times a word appears in an era, we might ask what percentage of the word's appearances are in that era. That gives us a sense of the characteristic words of a given era.

But as it stands, that doesn't work either. A lot of words have all of their appearances in one or other era. For instance, we don't see any occurrences of _elga_, _kolodny_, _knobe_, _weatherson_, _rayo_, _obama_ or _greaves_ until era 5. So they'd all be tied for being the most characteristic words of the 1999–2013 era, since all of their appearances in this era. And while some of those words are somewhat important to the era, I don't think that's quite what we're looking for.

What I decided to do, following common practice, is to restrict the study to the five thousand most common words. This excludes all the words I just listed. (I'd have to extend it a fair bit further to get them; _elga_ is at position 10468, _greaves_ at 16583, and the others in between.) And then we can look at which words from that five thousand have the highest percentage of their occurrences in a given era.

But it turns out even that isn't quite what we want, though it's not clearly not what we want. If you do what I just said, most of the words that show up are between the four thousandth and five thousandth most common words. It's just much easier for rarer words, especially names, to appear at one particular time. Therefore, I decided to show you a whole bunch of tables.

For every one of the following tables, I restricted attention to the top _n_ thousand words in the data set, and then asked of those words, which have the highest percentage of their occurrences in different eras. I think looking at the tables for each value of _n_ from 1 to 5 is useful. There will be some repetition; sometimes a common word has a distinctive distribution. But there is some new information each time _n_ is increased. I cut it off at _n_ = 5, but you could keep going beyond that. (Though if we did, we'd find a few more latex words, and journal names, and OCR errors, that, in retrospect, I might have wanted to delete from the dataset.) So here's what we get when restricting attention to the one thousand most common words.

```{r erawords1000}
words_by_limit(1000)
```

Is _jones_ at the top of 1966–1981 because of Sellars, or Gettier, or Frankfurt? I think the answer is, all of the above! I think _frege_ appears prominently in 1982–1998 because of "Frege cases", not because of a particular upsurge in attention to Frege's own writings. Both _quine_ and _rawls_ are a bit later than their most famous writings, which makes sense. And note _intuitions_ turning up as a distinctive word in the 21st century literature. It's interesting, I think, that it isn't used as much in the era intuitions were allegedly dominating philosophy as in the era when metaphilosophy became such a big deal.

We can look at the graphs of how frequently these words appeared over time to get a sense of what it means for them to be the distinctive words of an era. I'll just graph the top five for each era, because otherwise the graphs get too cluttered.

```{r eragraphs1000a, fig.height=4, fig.cap = "Most distinctive words in era 1 (among one thousand most common words).", fig.alt = alt_text}
word_era_graphs(1000, 1)
alt_text <- word_era_graphs_alt_text(1000, 1)
```

```{r eragraphs1000b, fig.height=4, fig.cap = "Most distinctive words in era 2 (among one thousand most common words).", fig.alt = alt_text}
word_era_graphs(1000, 2)
alt_text <- word_era_graphs_alt_text(1000, 2)
```

```{r eragraphs1000c, fig.height=4, fig.cap = "Most distinctive words in era 3 (among one thousand most common words).", fig.alt = alt_text}
word_era_graphs(1000, 3)
alt_text <- word_era_graphs_alt_text(1000, 3)
```

```{r eragraphs1000d, fig.height=4, fig.cap = "Most distinctive words in era 4 (among one thousand most common words).", fig.alt = alt_text}
word_era_graphs(1000, 4)
alt_text <- word_era_graphs_alt_text(1000, 4)
```

```{r eragraphs1000e, fig.height=4, fig.cap = "Most distinctive words in era 5 (among one thousand most common words).", fig.alt = alt_text}
word_era_graphs(1000, 5)
alt_text <- word_era_graphs_alt_text(1000, 5)
```

We can confirm that the words in question really do peak in the era in question.

The Y-axis measures the frequency of the words among the words in the JSTOR data. That excludes 1 and 2 letter words, and whatever stop words JSTOR has excluded (like _the_, _and_, and the like), but includes things like bibliographic information and latex code. It probably overstates the actual frequency of the words by something like 25 to 50 percent. So if it says that a word appears one time in four hundred, its real frequency is, as far as I can tell, more like one time in five to six hundred.

Note that the distinctive words of the middle eras are much less frequent than the distinctive words of the early eras or even (to a lesser extent) the later eras. The word _consciousness_ seems to have appeared, on average, about once a page in the early years! No word is this prevalent in the later years.

Let's expand the data set and look at the two thousand most common words.

```{r erawords2000}
words_by_limit(2000)
```

Both _marx_ and _davidson_ turn up one era later than I would have guessed. And I would have thought _strawson_ was either an era earlier or an era (or two) later; earlier for the work on descriptions, later for the work on responsibility. So those are a bit interesting. Testimony really was a big topic in the early twenty-first century. And note _worry_ turning up. Recent philosophy has a very distinctive lexicon, which we'll see more and more of.

Here are the graphs for the first five words in each column.

```{r eragraphs2000a, fig.height=4, fig.cap = "Most distinctive words in era 1 (among two thousand most common words).", fig.alt = alt_text}
word_era_graphs(2000, 1)
alt_text <- word_era_graphs_alt_text(2000, 1)
```

```{r eragraphs2000b, fig.height=4, fig.cap = "Most distinctive words in era 2 (among two thousand most common words).", fig.alt = alt_text}
word_era_graphs(2000, 2)
alt_text <- word_era_graphs_alt_text(2000, 2)
```

```{r eragraphs2000c, fig.height=4, fig.cap = "Most distinctive words in era 3 (among two thousand most common words).", fig.alt = alt_text}
word_era_graphs(2000, 3)
alt_text <- word_era_graphs_alt_text(2000, 3)
```

```{r eragraphs2000d, fig.height=4, fig.cap = "Most distinctive words in era 4 (among two thousand most common words).", fig.alt = alt_text}
word_era_graphs(2000, 4)
alt_text <- word_era_graphs_alt_text(2000, 4)
```

```{r eragraphs2000e, fig.height=4, fig.cap = "Most distinctive words in era 5 (among two thousand most common words).", fig.alt = alt_text}
word_era_graphs(2000, 5)
alt_text <- word_era_graphs_alt_text(2000, 5)
```

```{r erawords3000}
words_by_limit(3000)
```

Apart from in the earliest era, we're starting to see the majority of the list here be names of famous (male) philosophers. And we get a pretty good sense of when they were being most commonly discussed. The graphs show this in slightly more detail. (The fourth graph is a little busted because of one year when _nuclear_ went nuclear.)

```{r eragraphs3000a, fig.height=4, fig.cap = "Most distinctive words in era 1 (among three thousand most common words).", fig.alt = alt_text}
word_era_graphs(3000, 1)
alt_text <- word_era_graphs_alt_text(3000, 1)
```

```{r eragraphs3000b, fig.height=4, fig.cap = "Most distinctive words in era 2 (among three thousand most common words).", fig.alt = alt_text}
word_era_graphs(3000, 2)
alt_text <- word_era_graphs_alt_text(3000, 2)
```

```{r eragraphs3000c, fig.height=4, fig.cap = "Most distinctive words in era 3 (among three thousand most common words).", fig.alt = alt_text}
word_era_graphs(3000, 3)
alt_text <- word_era_graphs_alt_text(3000, 3)
```

```{r eragraphs3000d, fig.height=4, fig.cap = "Most distinctive words in era 4 (among three thousand most common words).", fig.alt = alt_text}
word_era_graphs(3000, 4)
alt_text <- word_era_graphs_alt_text(3000, 4)
```

```{r eragraphs3000e, fig.height=4, fig.cap = "Most distinctive words in era 5 (among three thousand most common words).", fig.alt = alt_text}
word_era_graphs(3000, 5)
alt_text <- word_era_graphs_alt_text(3000, 5)
```

The pattern stays the same as we expand to four thousand words.

```{r erawords4000}
words_by_limit(4000)
```

I'm a bit surprised to see _bob_ here; I'm not sure if this is Stalnaker, or Brandom, or who is being referred to so informally. The graphs don't show a great deal that isn't visible on the previous set, so let's skip over them and move the limit up to the five thousand most common words.

```{r erawords5000}
words_by_limit(5000)
```

And finally we get that not all the names are of men. Korsgaard is here, and the model doesn't discriminate between the Churchlands, so at least part of the reason for _churchland_ in 1982–1998 is Patricia Churchland.

Apart from the names, the words in the final era are all fairly much as expected. I'll come back in [the last chapter]({#buzzwords-section) to _credence_, because I don't think everyone realizes how new a term it is. And _egalitarianism_ was used so much in 1982–1998 that I'm very surprised it can turn up here.

I have no idea why _muscular_ is such a common term in the first era. I suspect I wouldn't be happy to find out. 

Here are the graphs for the five most distinctive words in each of the eras.

```{r eragraphs5000a, fig.height=4, fig.cap = "Most distinctive words in era 1 (among five thousand most common words).", fig.alt = alt_text}
word_era_graphs(5000, 1)
alt_text <- word_era_graphs_alt_text(5000, 1)
```

```{r eragraphs5000b, fig.height=4, fig.cap = "Most distinctive words in era 2 (among five thousand most common words).", fig.alt = alt_text}
word_era_graphs(5000, 2)
alt_text <- word_era_graphs_alt_text(5000, 2)
```

```{r eragraphs5000c, fig.height=4, fig.cap = "Most distinctive words in era 3 (among five thousand most common words).", fig.alt = alt_text}
word_era_graphs(5000, 3)
alt_text <- word_era_graphs_alt_text(5000, 3)
```

```{r eragraphs5000d, fig.height=4, fig.cap = "Most distinctive words in era 4 (among five thousand most common words).", fig.alt = alt_text}
word_era_graphs(5000, 4)
alt_text <- word_era_graphs_alt_text(5000, 4)
```

```{r eragraphs5000e, fig.height=4, fig.cap = "Most distinctive words in era 5 (among five thousand most common words).", fig.alt = alt_text}
word_era_graphs(5000, 5)
alt_text <- word_era_graphs_alt_text(5000, 5)
```

Here part of the story, as is shown in the changing scale of the Y-axes, is ever-increasing diversity. Figures who feel like they dominate the current age, like Williamson, Hawthorne and Chalmers, are discussed much less than figures like Ayer or Ryle were a couple of generations ago.

## Topics and Decades

So far I've broken down the years by what I've called _eras_. These are helpful because they feature equal numbers of articles, and they just about correspond to philosophically significant periods. But most people are more familiar with decades than with any kinds of eras like this. (Having the first era end in 1945 was fortuitous, but otherwise they are a bit arbitrary.) I thought I'd run all the tests from the previous three sections with decades rather than eras as the underlying unit. Start with the ninety topics broken down by which are the most popular and the least popular topics in each decade. (As above, I'm using weighted sums of articles as the main measure here.)

```{r decadetopicsa}
kable(decade_wide_top[1:6], col.names =  dec_title[1:6], caption = "Most popular topics in each decade.")
kable(decade_wide_top[7:12], col.names =  dec_title[7:12])
```

That's mostly not that interesting. Idealism is the big story early, then the two "topics" that are really more styles or techniques than topics—ordinary language and arguments—are the big stories later. It is a bit interesting that some things come up a decade or two after the time I associate them with. I did not expect sets and grue, or verification, to be the big stories of the 1970s. But this is just a story that we've seen a few times now; philosophical debates don't burn out, they just fade away.

Let's quickly look at the bottom of these lists, remembering that these are very noisy measures especially in the early decades.

```{r decadetopicsb}
kable(decade_wide_bottom[1:6], col.names = dec_title[1:6], caption = "Least popular topics in each decade.")
kable(decade_wide_bottom[7:12], col.names =  dec_title[7:12])
```

The first table is almost all noise. In the first five columns, all of the topics listed have fewer than one article in each of the decades. The model getting confused by the words in one short article matching keywords of a later topic is enough to move off this list.

The second table is a bit more interesting. The model really doesn't think we started talking about semantic externalism until the 1980s. I've mentioned this a couple of times now, and it really surprises me. And I thought at least some cognitive science work was getting going by the 1970s, though perhaps it just wasn't turning up in these journals. In the last two decades, the relative erasure of traditional aesthetics and philosophy of religion from the twelve journals is quite striking. I thought in the 2000s the model would have been confused enough by Sleeping Beauty papers to have seen more aesthetics articles. It's perhaps possible things went the other way, and actual aesthetics articles were mixed up with formal epistemology.

It's a bit more interesting, I think, to look at the top of these tables after normalization. So these tables rank the topics by what percentage of the works in that topic are in each decade. (Again, it's weighted sum of articles that I'm using as the main underlying measure. And I'm normalising by the weighted sum of articles in each topic over the decades covered (i.e., 1890–2010, not over the whole study from 1876–2013).

```{r decadetopicsc}
kable(decade_distinctive_wide_top[1:6], col.names = dec_title[1:6], caption = "Most distinctive topics in each decade.")
kable(decade_distinctive_wide_top[7:12], col.names = dec_title[7:12])
```

The early decades are not much of a surprise. The big topics from those years are big in every decade, with slightly more attention paid to history as we get into the twentieth Century.

[Early modern](#topic21) hits a high point in the 1930s before it stops at the same time Ryle becomes editor of _Mind_. 

The founding of _Philosophy and Phenomenological Research_ makes [Heidegger and Husserl](#topic28) particularly prominent in the 1940s. 

And there is a sudden burst, especially in _Journal of Philosophy_ of attention both to philosophy of [history and cCulture](#topic10), and to [aesthetics](#topic08). Unlike the attention to pragmatism and other distinctively American fields, these two topics don't disappear in the earyl 1950s, but they do slow down rapidly.

Wittgensteinian philosophy turns up in the middle of the century, which is not a surprise.

And then one that is a surprise to me: [feminism](#topic69) having its distinctive decade be the 1970s. Looking back, especially at _Ethics_, there was a bit more feminist philosophy published then than in later decades. Before doing this project, I had a mental picture of Anglophone analytic philosophy being very hostile to feminist work, and this very very slowly breaking down over the last few decades. This looks a bit like evidence against that view, so let's dig a little deeper into where these articles are coming from. Here is a list of the articles from the 1970s that the model gives a probability of at least 0.4 of being in feminism:

```{r feminism-seventies}
feminism_seventies <- relabeled_gamma %>%
  filter(year > 1969, year < 1980, topic == 69, gamma > 0.4) %>%
  select(document) %>%
  inner_join(articles, by = "document") %>%
  arrange(year, length) %>%
  select(citation)

for (i in 1:nrow(feminism_seventies)){
  cat(i, ". ", feminism_seventies$citation[i],"\n", sep = "")
}
```

Four things about this list jump out. One is that a lot of these articles are very short. If I'd weighted by pages not by articles, the topic probably wouldn't have turned up on this list. A second is that these articles are as much about race as about gender. That's not too surprising; a lot of philosophers still talk as if "race-and-gender" form a single word. And the model reflects that. A third is that there are a lot of men on this list. And a fourth is that the median topic here is how awfully bad affirmative action is, and how it's really just as bad as the discrimination it is supposed to remedy. To be fair, a number of philosophers (including some of the men) are replying to this argument. But it's a bit special how much more interested philosophers got in discrimination once affirmative action became a live possibility.

I'll end this section with a quick look at the topics that have the lowest percentages of their works in a particular decade. Since there are many topics in any decade with practically zero works, this is a very noisy measure.

```{r decadetopicsd}
kable(decade_distinctive_wide_bottom[1:6], col.names = dec_title[1:6], caption = "Least distinctive topics in each decade.")
kable(decade_distinctive_wide_bottom[7:12], col.names = dec_title[7:12])
```

Most of that is noise, but the one thing that jumps out is the low position of other history in the 1970s and 1980s. There was less and less history in these journals over time. But even within the space allocated to history, the focus got really narrow for a while  I think things have gotten a little better in recent years, and especially after the end of this study, but for a while it was a striking gap in the literature.

## Categories and Decades {#categories-decades}

Looking at the categories by decades rather than eras doesn't tell us a whole lot more, but it tells us a few things. Here is the popularity of each category from the 1890s to 1940s.

```{r categorydecadeearly}
kable(category_decade_wide_top[1:6], col.names = dec_title[1:6], caption = "Popularity of categories in first six decades.")
```

The big story there, to my mind, is the continuing domination of idealism through the 1920s. Before I did this study, I had no idea that idealism was so important to the journals for so long. If not for the work that we'd now call psychology as much as philosophy, it would be the dominant philosophical topic for the first fifty years of the sample.

I've been downplaying the significance of positivism to the journals, so it's worth noting that here is a place that it seems to show up. When idealism is displaced, and the psychology articles are excised, it is categories associated with positivism that take over. When we looked at this at a topic-by-topic level it wasn't clear that it was the positivism in those topics that was the driving force. But it is clear that there is a hard turn here towards more technical work, and positivism has to be partially responsible for that.

Let's move on to the later six decades.

```{r categorydecadelate}
kable(category_decade_wide_top[7:12], col.names = dec_title[7:12], caption = "Popularity of categories in last six decades.")
```

The eras correspond more closely to decades after 1950, so this doesn't tell us much that we didn't already see. But it tells us one important thing: ethics is almost at the top in the 1960s. When looking at the earlier graphs and tables, I was wondering how much the rise of ethics as a category had to do simply with _Philosophy and Public Affairs_ opening in 1970. And the answer is that the rise is happening even before _Philosophy and Public Affairs_ comes online. Now of course when it does come online it makes a difference. But it's a less dramatic difference than I expected. (This is in part because _Philosophy and Public Affairs_ publishes long articles, and I'm primarily measuring by article count.) It might be more plausible to say that the rise in importance of ethics inside philosophy makes it possible for _Philosophy and Public Affairs_ to launch as a prestigious journal, rather than the launch of _Philosophy and Public Affairs_ as a prestigious journal making ethics more important in the discipline.

Let's also take a look at the distinctiveness measures. Recall that these tables rank each category by what proportion of the articles in that category appear in a particular decade. This is a particularly informative measure for the smaller categories such as aesthetics and philosophy of religion. 

```{r categorydecadeearlydist}
kable(category_decade_distinctive_wide_top[1:6], col.names = dec_title[1:6], caption = "Distinctiveness of categories in first six decades.")
```

That's mostly not too surprising. Though note how much idealism is hanging around into the 1940s. Moore's refutation really took a long time to bite. Philosophy of religion is, not surprisingly, a proportionally much bigger deal in the early twentieth century than it is later. And we're back to measures that really don't make positivism leap from the page. Let's end with a look at the later six decades.

```{r categorydecadelatedist}
kable(category_decade_distinctive_wide_top[7:12], col.names = dec_title[7:12], caption = "Distinctveness of categories in last six decades.")
```

This way of looking at things makes the fall of idealism look much later than I expected, but fairly dramatic when it happens.

The low rank for social and political in the 2000s really surprises me. I do not expect that would persist if we continued the study through the 2010s.

And I'm also surprised at the low rank in the 2000s of aesthetics, philosophy of religion and history of philosophy. It's not just that these topics aren't discussed much in the twelve journals. (Casual observation would tell you that.) It's that they are not discussed much relative to their historical norms. That's surprising and not an altogether positive development.

## Words and Decades {#words-decades}

I'm not going to start with the list of most common words each decade, because it just looks like what we found back in section \@ref(words-eras). Some words are really common and they stay that way through the data set. What's more interesting is to look at words that are more common in a particular decade than they are in general. And this in turn requires picking a domain of words. 

I'll start with the one thousand most common words (stop words excluded) from the data set. (I'm not going to do the graphs that I did in section \@ref(words-eras), because they would just be repetitive; it's very often just the same words.)

```{r decadewords1000}
early_decade_words_by_limit(1000)
late_decade_words_by_limit(1000)
```

I guess nothing there is too surprising. The keywords in the first few decades are basically what we'd expect if there was a lot of idealism going on. Then there is much more attention to aesthetics in the middle decades, as well as a huge focus on statements. (I think the term _statement_ is too ambiguous between sentence, utterance and proposition to be used nowadays, but that clearly wasn't the view then.) And then Rawls shows up, along with some people called _David_. _Probabilities_ shows up in the 1990s but not later because it is replaced, as we'll see, with _credences_. The boom in the topic norms looks a little less surprising when you see _normative_ is used so much, especially in the 2000s, that it ends up on the list of one thousand most common words.

Let's increase the domain to the three thousand most common words in the data set.

```{r decadewords3000}
early_decade_words_by_limit(3000)
late_decade_words_by_limit(3000)
```

The main auditorium at Monash University is the Alexander Hall. Graduations are there, but it isn't otherwise a major part of student life. I think I went there more often as a high school student than as a university student. But still, it was nice to know that a big part of the university was named after a philosopher, Samuel Alexander. There was  also a nice sketch of him on the wall of one of the seminar rooms in the philosophy department while I was there. That didn't mean that we actually talked about Alexander's work; I don't remember ever being assigned one of his papers in a class or seminar. That was perhaps a bit too bad, since he's arguably the second most important Australian philosopher for the first half of the twentieth century.

Let's expand this list to the five thousand most common words.

```{r decadewords5000}
early_decade_words_by_limit(5000)
late_decade_words_by_limit(5000)
```

When I was cleaning out the data I thought that _sensa_ was some kind of OCR error. But it's a really common word in 1920s and 1930s discussions of sense data. In later years, the distinctive words tend to be more and more to do with names.

That said, we also see here a word that I should have filtered out: _basil_. Some uses of it are legitimate, but most are because Basil Blackwell turns up in bibliographies. I wish I'd caught that one.

<!--chapter:end:07-eras.Rmd-->

# Outliers {#outliers}

This chapter investigates various outliers in the model. The aim of the chapter is twofold. One is to look for weird things that we can discover about the history of the journals by looking at extreme values in the model. The other is to test the model by looking at the extreme things it says. Usually the most extreme things a model says are the least plausible; they are artifacts of the model not facts about the underlying phenomena. So if the model is not too plausible at the extremes, that should give us some confidence in the other things it says.

## High-Confidence Articles {#high-confidence-articles}

The model is fairly confident about where some of the articles belong. Other articles it is more confused by. Let's start by looking at those extremes. First, here are the articles where the model is most sure of a classification. That is, they are the articles where the probability of being in one particular category is highest.

```{r highconf}
certainty_check <- relabeled_gamma %>%
  select(document, topic, gamma) %>% 
  mutate(gammasq = gamma ^ 2) %>%
  group_by(document) %>%
  dplyr::summarise(m = max(gamma), c = sum(gammasq)) %>%
  inner_join(relabeled_articles, by = "document") %>%
  select(document, topic, m, c)

certainty_check <- inner_join(certainty_check, articles, by = "document") %>%
  select(document, topic, m, c, citation, length) %>%
  arrange(-m) %>%
  inner_join(the_categories, by = "topic") %>%
  select(document, subject = sub_lower, m, c, citation, length) %>%
  mutate(subject = fcap(subject))
  
short_certainty <- certainty_check %>%
  top_n(10, m) %>%
  select(citation, subject)  

kable(short_certainty, 
      col.names = c("Article", "Subject"), 
      caption = "Articles the model is most certain about.")

# See 'most_and_least_certain.R' for more
```

There is a reasonable spread of topics here; nine of the ninety are represented in just these ten articles. But note that these tend to be very short articles. This might be why the model is so confident in them. What if we filter out all articles ten pages or shorter?

```{r highconf-p2}
medium_certainty <- certainty_check %>%
  filter(length > 9) %>%
  top_n(10, m) %>%
  select(citation, subject)

kable(medium_certainty, 
      col.names = c("Article", "Subject"),
      caption = "Articles the model is most certain about (minimum length ten pages).")
```

It isn't surprising that evolutionary biology starts to turn up a little here. It's a very specialized topic. (I'll come back to this question of specialization, and what it means to be a specialised topic, in section \@ref(raw-weight-count).) 

We mostly get new articles if we extend the minimum length to twenty pages.

```{r highconf-p3}
huge_certainty <- certainty_check %>%
  filter(length > 19) %>%
  top_n(10, m) %>%
  select(citation, subject)

kable(huge_certainty, 
      col.names = c("Article", "Subject"),
      caption = "Articles the model is most certain about (minimum length twenty pages).")
```

And note that all of these are just over twenty pages, or exactly twenty in a few cases. The model really loses confidence the longer a piece gets.

```{r highconf-p4}
long_certainty <- certainty_check %>%
  filter(length > 29) %>%
  top_n(10, m) %>%
  select(citation, subject)

kable(long_certainty, 
      col.names = c("Article", "Subject"),
      caption = "Articles the model is most certain about (minimum length thirty pages).")
```

Now we're mostly looking at papers in political philosophy. But still it is mostly papers that just fall over the thirty-page limit. So for the last one, let's look at the articles it is most confident about that are forty pages or longer.

```{r highconf-p5}
absurd_certainty <- certainty_check %>%
  filter(length > 39) %>%
  top_n(10, m) %>%
  select(citation, subject)

kable(absurd_certainty, 
      col.names = c("Article", "Subject"),
      caption = "Articles the model is most certain about (minimum length forty pages).")
```

Again, normative an political philosophy cover the very top.

The articles here are also mostly fairly recent, but that doesn't tell us much about the model. Rather, it is a sign that long articles are a relatively recent phenomenon. 

```{r long-art-histogram, fig.height= 4, fig.cap = "Number of articles each decade that are at least forty pages long.", fig.alt = alt_text}
long_articles <- articles %>%
  filter(length > 39)

ggplot(long_articles) +
  geom_bar(aes(year), color = "grey40", fill = "grey40", width = 0.5) + 
  scale_x_binned() +
  labs(x = "Decade", y = "Number of articles", title = "Articles are getting longer") +
  freqstyle +
    scale_y_continuous(expand = expansion(mult = c(0.01, .03)),
                     minor_breaks = scales::breaks_pretty(n = 15),
                     breaks = scales::breaks_pretty(n = 3))

alt_text <- "A histogram showing the number of articles in each decade from 1900 to 2010 that are at least forty pages long. The number begins to rise after 1970."
```

I'll come back to this point in section \@ref(article-length-section).

## Low-Confidence Articles {#low-confidence-articles}

What about the other direction? Which articles is the model most uncertain about. This is a bit more of a stress test of the model. The high-confidence articles all look pretty much right for the topics they are in. (Not least because I named the topics after the high-confidence articles.) But if the model throws up its hands at articles that are easy to place, that's relatively bad. So let's look. 

There are more or less sophisticated ways to measure how unsure the model is about an article. I'm going to go with one of the less sophisticated ways, because it is easy to understand and provides clear enough guidance. Implicitly in the previous section, I measured the model's certainty about an article by the maximal probability it gives to the article being in any one topic. I'll say it is most uncertain about an article if that maximal probability (for that article) is lowest.

By that measure, here are the ten articles the model is most unsure about.

```{r uncertainty-table}
all_uncertainty <- certainty_check %>%
  top_n(10, -m) %>%
  arrange(m) %>%
  select(citation, document)

kable(select(all_uncertainty, citation), 
      col.names = c("Article"), 
      caption = "Articles the model is most uncertain about.")
```

Did the model get it right? Should it be uncertain about these articles? Let's look at some cases, starting with the one it is most uncertain about.

```{r uncertainty-article-1}
individual_article(all_uncertainty$document[1])
```

For reasons best known to them, the editors of the _Philosophical Review_ commissioned a [critical notice](https://philpapers.org/rec/DONTEO) [@Donagan1970] of the eight-volume Encyclopedia of Philosophy. It could be called a book review I guess, but it's fifty-six pages long, so it feels like it should be in our study.

And I'm fairly happy that this was the article the model had the greatest trouble with. How could it classify a critical notice of an encyclopedia. What topic could it not be in? The answer seems to be the very late topics—there is no [wide content](#topic82) or [quantum physics](#topic66) there—and the very early topics. I'm actually a little surprised that [idealism](#topic02) doesn't turn up.

So far so good—the model threw up its hands exactly when it should have done so. It would have been wrong to confidently place Donagan's article. What about the second one?

```{r uncertainty-article-2}
individual_article(all_uncertainty$document[2])
```

And this is a bit more depressing. [Garfield's article](https://philpapers.org/rec/GARTMO-2) [-@Garfield1989] is wide-ranging, covering a number of big questions at the heart of philosophy of mind and epistemology. But still, it isn't that hard to say what it's about broadly. And to be sure, the model does recognize that this isn't a philosophy of physics article, or a political philosophy article, or an ancient philosophy article. But still, it should do better than this.

What's happened, in a picturesque sense, is this: the articles are arranged in a feature space. The feature space has many, many dimensions, and the articles do form clusters within it. What the model does is pick ninety points in that space such that as many articles as possible are reasonably close to one of the points. Now some articles, like the Donagan, are going to be so idiosyncratic that they aren't near a point. But the Garfield isn't like that. The model could have decided that Sellarsian theories of mind/epistemology are a focal point. It just didn't do that. Instead, articles like these ended up falling between many many different points.

I think it isn't a coincidence that there is another article by Wilfred Sellars on the list. I think it is a coincidence that there is an article by his father though. That one is weird. Here is its table.

```{r uncertainty-article-10}
individual_article(all_uncertainty$document[10])
```

In [this paper](https://philpapers.org/rec/SELACT) [@Sellars1941], Sellars is operating at a fairly high degree of abstraction, and considering the ways in which the big philosophical views (idealism, pragmatism, realism, etc) have characteristic theories of truth. The theory of truth he wants to offer is a correspondence theory that isn't so closely tied to general forms of realism. And we can see why the paper looks, to the model, like it could be about all sorts of things. I'm still a bit surprised that the model didn't just lump it in with other works on truth. It doesn't mention the paradoxes, and has no formalism, and maybe that was enough. But it's surprising.

Anyway, I'm pleased that the article it was most uncertain about was a really impossible-to-place article, disappointed that it couldn't do a better job with Sellarsian philosophy, and not surprised that it also threw up its hands at various methodology articles. It's not a perfect model, but it did fairly well.

## High-Confidence Topics {#high-confidence-topics}

I mentioned in section \@ref(high-confidence-articles) that there were several topics that were appearing frequently among the articles the model was very confident about. Let's look at those topics on a graph.

This graph looks at the fifty articles the model is most confident about, and asks how many of them are in the various different topics.

```{r high-confidence-function, fig.cap = "Topic distribution for the fifty articles the model is most certain about.", fig.alt = alt_text}
cols <- tibble(x = 1:90) %>%
  mutate(col = hcl(h = (x-1)*(360/90)+15, l = 65, c = 100)) %>%
  deframe()

graph_high_prob_page_limit <- function(x){
  c <- relabeled_articles %>%
  arrange(-gamma) %>%
  filter(length > x-1) %>%
  slice(1:50) %>%
  mutate(topicfactor = as.factor(topic)) %>%
  inner_join(the_categories, by = "topic")
  
  ylab <- paste0("Number of articles with topic probability at least ", round(min(c$gamma), 3), ", length at least ",x," pages")

  ggplot(c, aes(reorder(fcap(sub_lower), -topic), fill = topicfactor, drop = TRUE)) + 
    freqstyle +
  geom_bar(width = 0.8) +
  coord_flip() +
  theme(legend.position = "none") +
  labs(y = ylab, x = NULL) +
  scale_y_continuous(expand = expansion(mult = c(0, .03))) +
  scale_fill_manual(values = cols)
}

confident_articles_no_page_limit <- relabeled_articles %>%
  arrange(-gamma) %>%
  slice(1:50) %>%
  mutate(topicfactor = as.factor(topic)) %>%
  inner_join(the_categories, by = "topic")

ylab = paste0("Number of articles with topic probability at least ", round(min(confident_articles_no_page_limit$gamma), 3))

ggplot(confident_articles_no_page_limit, 
       aes(reorder(fcap(sub_lower), -topic), 
           fill = topicfactor)) + 
  freqstyle +
  geom_bar(width = 0.8) +
  coord_flip() +
  theme(legend.position = "none") +
  labs(y = ylab, x = NULL) +
  scale_y_continuous(expand = expansion(mult = c(0, .03))) +
  scale_fill_manual(values = cols)

alt_text <- "A hisogram showing the topic distribution for the fifty articles the model is the most certain about, shown as the number of articles with topic probability at least 0.905. Space and time has by far the most articles in this category."
```

As I noted back when talking about [space and time](#topic50), it has a surprising large number of articles the model is very confident about. But as we saw above, a lot of the articles the model is confident about are very short. Let's focus instead on the articles that are at least ten pages long, and again look at the distribution of the fifty articles the model is most confident about.

```{r high-conf-10-pages, fig.cap = "Topic distribution for the fifty articles the model is most certain about (min ten pages).", fig.alt = alt_text}
graph_high_prob_page_limit(10)

alt_text <- "A histogram showing the topic distribution for the fifty articles the model is the most certain about that are at least ten pages long. This is shown as the number of articles with topic probability at least 0.847. Evolutionary Biology is the most frequent in this distribution, followed by War. "
```

And this isn't surprising; the model gets really confident that [evolutionary biology](#topic82) articles are properly placed. The same thing happens when we increase the length to twenty pages.

```{r high-conf-20-pages, fig.cap = "Topic distribution for the fifty articles the model is most certain about (min twenty pages).", fig.alt = alt_text }
graph_high_prob_page_limit(20)

alt_text <- " A histogram showing the topic distribution for the fifty articles the model is the most certain about that are at least twenty pages long. This is shown as the number of articles with topic probability at least 0.785. Evolutionary biology is the most frequent in this distribution, followed by quantum physics. "
```

There are still ten evolutionary biology articles, though mostly not the same ten. And there are fewer categories here. Just eighteen categories are represented in these fifty articles. And the purples and reds indicate that the articles are getting much later. These trends extend when we raise the floor to thirty pages, though now the topics start to shift.

```{r high-conf-30-pages, fig.cap = "Topic distribution for the fifty articles the model is most certain about (min thirty pages" , fig.alt = alt_text}
graph_high_prob_page_limit(30)

alt_text <- "A histogram showing the topic distribution for the fifty articles the model is the most certain about that are at least thirty pages long. This is shown as the number of articles with topic probability at least 0.695. Liberal Democracy is the most frequent in this distribution, followed by Quantum Physics and Egalitarianism."
```

There is more quantum physics, and more political philosophy. And when we move to forty pages, which means we're just looking at the longest two percent of articles, these trends really accelerate.

```{r high-conf-40-pages, fig.cap = "Topic distribution for the fifty articles the model is most certain about (min forty pages).", fig.alt = alt_text }
graph_high_prob_page_limit(40)

alt_text <- "A histogram showing the topic distribution for the fifty articles the model is the most certain about that are at least forty pages long. This is shown as the number of articles with topic probability at least 0.559. Quantum Physics is the most frequent in this distribution, followed by Liberal Democracy and Early Modern."
```

By this stage the graph is measuring less which articles the model is really confident in, and more which kinds of philosophers write articles that long. The answer is, apparently, philosophers of (quantum) physics, political philosophers, and early modern historians.

## Correlations {#correlation-section}

The model assigns a probability to each topic-article pair. So across the articles, we can ask how tightly correlated those probabilities are. Which of them tend to go up when the other goes up? There are 8010 pairs of distinct topics, so there is too much data here to usefully examine, or even visualise. But I wanted to go over the extremes. First, here are the thirty-two strongest correlations. (Why thirty-two? Because these seemed particularly interesting.)

```{r correlation-setup}
art_corr <- relabeled_gamma %>%
  arrange(topic) %>%
  select(-year, -journal, -length) %>%
  pivot_wider(names_from = topic, values_from = gamma) %>%
  select(-document) %>%
  correlate(quiet = TRUE, diagonal = NA) %>%
  corrr::shave() %>%
  corrr::stretch(na.rm = FALSE) %>%
  filter(!x == y) %>%
  arrange(-r) %>%
  mutate(x = as.numeric(x), y = as.numeric(y)) %>%
  inner_join(the_categories, by = c("x" = "topic")) %>%
  select(subj_one = sub_lower, y, r, x) %>%
  mutate(subj_one = fcap(subj_one)) %>% 
  inner_join(the_categories, by = c("y" = "topic")) %>%
  select(subj_one, subj_two = sub_lower,  r, x, y) %>%
  mutate(subj_two = fcap(subj_two)) %>% 
  mutate(r = round(r, 4))
```

```{r correlation-kable}
kable(art_corr %>%
        slice(1:32) %>%
        select(1:3), 
      col.names = c("Subject One", "Subject Two", "Correlation"), 
      caption = "Highest topic correlations.") %>%
kable_styling(full_width = F)
```

I think these mostly make sense. The two epistemology topics are very tightly connected. The two topics that are about formal methods in scientific reasoning are correlated. (Remember that [chance](#topic44) included a lot of work on formal models of inference.) The philosophy of religion articles are correlated. Idealism is correlated with the other early topics. Topics about time are correlated. [denoting](#topic43) and [sense and Reference](#topic64) are correlated; Frege and Russell aren't that far apart.

The bottom few here are particularly interesting. [Moral Conscience](#topic25) and [value](#topic16) include some very analytic ethics; it's interesting that it they are so close to [Dewey and pragmatism](#topic05). [Marx](#topic23) the topic plays well with [life and value](#topic03), i.e., idealist ethics, with [liberal democracy](#topic52), and with [history and culture](#topic10). This is a bit surprising since Marx himself didn't play well with any of them. But life and value also plays well with [faith and theism](#topic08), the core philosophy of religion topic. That mildly surprised me, but perhaps it should not have given how important the Absolute is to idealists.

Let's turn to the strongest negative correlations. These are a little less interesting.

```{r correlation-kable-low}
kable(art_corr %>%
        arrange(r) %>%
        slice(1:25) %>%
        select(1:3), 
      col.names = c("Subject One", "Subject Two", "Correlation"), 
      caption = "Lowest topic correlations.") %>%
kable_styling(full_width = F)
```

The early topics and the late topics aren't correlated. The Idealists aren't correlated with anyone who isn't sympathetic to idealism. No one was offering arguments, at least not as such, in the early going. Let's come back to this table and see what we can find that's more interesting.

What about the topics that are perfectly independent? These topics are not correlated with each other at all.

```{r correlation-kable-middle}
kable(art_corr %>%
        arrange(abs(r)) %>%
        slice(1:25) %>%
        select(1:3), 
      col.names = c("Subject One", "Subject Two", "Correlation"), 
      caption = "Most independent topics.") %>%
kable_styling(full_width = F)
```

I don't know what I expected here, but I don't think it was this. Some of these felt like they should be positively correlated. I guess just on timing grounds I expected [personal Identity](#topic62) to correlate with [wide content](#topic85). But I would have guessed [beauty](#topic08) to be negatively correlated with [meaning and use](#topic22). Maybe there isn't anything to be found here; this mostly looks like noise to me.

The low correlation table featured mostly topics from the first half of the topics. (Indeed, every pair featured at least one such topic.) So let's do the high and low correlation tables again but restricted to topics 46–90.

```{r correlation-kable-late-high}
kable(art_corr %>%
        filter(x > 45, y > 45) %>%
        slice(1:25) %>%
        select(1:3), 
        col.names = c("Subject One", "Subject Two", "Correlation"), 
        caption = "Highest topic correlations (topics 46–90).") %>%
kable_styling(full_width = F)
```

Those all seem to make sense. That isn't totally surprising, but it's reassuring to see that the model seems to have not messed up here. Let's look at the other end of the table.

```{r correlation-kable-late-low}
kable(art_corr %>%
        filter(x > 45, y > 45) %>%
        arrange(r) %>%
        slice(1:25) %>%
        select(1:3), 
        col.names = c("Subject One", "Subject Two", "Correlation"), 
        caption = "Lowest topic correlations (topics 46–90).") %>%
kable_styling(full_width = F)
```

This is a bit surprising. I thought I'd see pairs like Liberal Democracy and [composition and Constitution](#Topic89) turning up a lot here. That is, I thought what we'd find would recreate the famiilar ethics versus M&E divide. But pairs like that are not the bulk of the table. Instead, we get a lot of negatively correlated pairs that are on the same side of this (alleged) divide.

Some such pairs are not surprising. [concepts](#topic78) and [formal epistemology](#topic84) are negatively correlated, but this makes perfect sense because virtually all the work in formal epistemology uses unstructured contents.

But one might worry that the lack of an Ethics versus M&E divide here shows that the model has missed something important. I think a better conclusion is that the model is correctly detecting that _M&E_ isn't a useful kind of classification in contemporary philosophy. This feels like something that could do with further study, but I doubt text mining will be the way forward here. It would be interesting, for example, to see whether citation studies show that there is (or is not) a big ethics versus M&E divide.

## Neighbors {#neighbours-section}

There is another way that we can measure distance between articles. This is the way that I was measuring distance in the topic summaries back in chapter \@ref(all-90-topics). For a pair of topics $\langle x, y\rangle$, look at the articles that are more likely in topic $x$ than any other topic and find the average probability that these articles are in $y$. Unlike correlations, this is an asymmetric measure. But it tells us something useful about the connections between the topics. I'll start by looking at the top of this table.

```{r neighbours-tibble}
n <- cross_topic_tibble %>%
  ungroup() %>%
  inner_join(the_categories, by = "topic") %>%
  select(subject_one = sub_lower, topic = othertopic, g) %>%
  mutate(subject_one = fcap(subject_one)) %>% 
  inner_join(the_categories, by = "topic") %>%
  select(subject_one, subject_two = sub_lower, g) %>%
  mutate(subject_two = fcap(subject_two)) %>% 
  arrange(-g)%>%
  mutate(g = round(g, 4))

```

```{r neighbours-high}
kable(n %>%
        slice(1:25) %>%
        select(1:3), 
      col.names = c("Subject One", "Subject Two", "Average Probability"), 
      caption = "Highest cross-topic probability.") %>%
kable_styling(full_width = F)
```

That's not as helpful as I'd hoped. Lots of topics are such that articles in them look a lot like [ordinary language philosophy](#topic24). I'll deal with this by simple brute force; I'll filter out the ordinary language philosophy topic, and rerun the table.

```{r neighbours-high-no-olp}
kable(n %>%
        filter(!subject_two == "Ordinary language") %>%
        slice(1:25) %>%
        select(1:3),
      col.names = c("Subject One", "Subject Two", "Average Probability"), 
      caption = "Highest Cross-Topic Probability (excluding ordinary language).") %>%
kable_styling(full_width = F)
```

That is a little more interesting, and a little more sensible, but there are a couple of things that jumped out.

One is that there are a bunch of things here that don't appear on the correlations table. It makes sense that [Kant](#topic32) and [idealism](#topic02) go together, but the correlation table didn't show that up. So maybe this is a better measure of proximity. It's at least an interestingly different measure.

But the other surprise is that there are so few pairs that are on this list in both directions. Possibly these two surprises are related. [Knowledge](#topic74) and [justification](#topic76) are there in both directions, and I think that's it. In some cases I think it's easy to see why. The modeling articles are often about causal modeling, so they feel like causation articles. But lots of causation articles, especially pre-Lewis, don't feel like causal modeling articles, and hence don't feel like modeling. But I would have guessed pairs like that woud be the outlier; they seem to be the usual case.

Next let's look at the lower end of this table.

```{r neighbours-low}
kable(n %>%
        arrange(g) %>%
        slice(1:25) %>%
        select(1:3),
      col.names = c("Subject One", "Subject Two", "Average Probability"), 
      caption = "Lowest cross-topic probability.") %>%
 kable_styling(full_width = F)
```

And this is why I've used this measure as my preferred distance measure. Those all look like topics that have nothing to do with each other. And they don't!

There is a relatively technical point that's worth emphasizing here. The model gives a nonzero probability to each article being in each topic. But it pretty clearly doesn't calculate each of those probabilities particularly carefully. If you look at the probability distribution for any article, there are some carefully calculated probabilities for anywhere from one to twenty topics. (Usually five to eight, at least by my impression.) Then all the other topics get the very same probability. What that same probability is seems, as far as I can tell, to be a factor of how confident the model is in its assignment. But it's just some very very low number.

What we're seeing here is that for a bunch of pairs of topics, every one of the articles that is naturally in the first topic gets one of these residual probabilities for the second topic. For example, for every article in [psychology](#topic01), the probability that it is in [formal epistemology](#topic84) is minimal.

That means we really shouldn't care about the order of this table. This is a list of topics that the model thinks have basically nothing in common. And apart from being a little surprised about [Kant](#topic32) being paired up that way with [medical ethics and Freud](#topic70), I can't see much to complain about here. And note that even in that case, there are some medical ethics articles that the model thinks are a bit about Kant; it just thinks that no Kant articles are maybe about medical ethics. And that seems perfectly sensible.

## Articles and Pages {#topic-length-section}

I've mostly been analyzing the data here by looking at how many articles (or how many expected articles) are in a topic. But there's a case to be made for using pages rather than articles as the basic measure. To give  a sense of how much this matters, here are the topics with the longest and shortest average lengths.^[Note I'm using weighted, or expected, articles here, so for each topic I'm summing over all articles the probability of the article being in that topic times the length of the article and dividing by the expected number of articles in the topic.]

```{r topic-length-table-long}
page_ratio <- inner_join(weight_numerator, page_weight_numerator, by = c("year", "topic")) %>%
  filter(year > 1799, year < 2110) %>%
  mutate(pages = y.y/y.x) %>%
  arrange(-pages)

page_ratio_summary <- page_ratio %>%
  group_by(topic) %>%
  dplyr::summarise(a = sum(y.x), p = sum(y.y)) %>%
  mutate(r = p/a) %>%
  arrange(topic) %>%
  mutate(topic = as.numeric(topic)) %>%
  inner_join(the_categories, by = "topic") %>%
  mutate(a = round(a, 2), p = round(p, 2)) %>%
  select(Subject = sub_lower, Articles = a, Pages = p) %>%
  mutate(Subject = fcap(Subject)) %>% 
  mutate(avg = Pages/Articles) %>%
  arrange(-avg) %>%
  mutate(avg = round(avg, 1))

kable(page_ratio_summary %>% select(Subject, avg) %>% slice(1:5), col.names = c("Topic", "Average Length"), caption = "Topics with longest average page length.") %>% 
 kable_styling(full_width = F)
```

```{r topic-length-table-short}
kable(page_ratio_summary %>% arrange(avg) %>% select(Subject, avg) %>% slice(1:5), col.names = c("Topic", "Average Length"), caption = "Topics with shortest average page length.") %>% 
 kable_styling(full_width = F)
```

The difference between the longest and the shortest is almost a 2:1 ratio. So if we measured things by pages, it would make some changes. But in most cases, this difference won't show up on the graphs I've displayed so far. That's because most of the differences are screened by the changes in average article lengths over time. That is, most of the changes are due to the fact that the topics appear at different times in the data, and that the norm for article lengths change over time.

There are some exceptions to this. The truth topic has a lot of shortish articles appearing in recent times, when the article lengths have been getting longer. Though even there something weird happens. Many of these articles are in _Analysis_, and they are very long by _Analysis_ standards, but short overall. Still, it's worth looking at just how much article lengths have changed to provide a sense of how much these changes in length trends are driving the tables in this section.

## Article Lengths {#article-length-section}

This isn't specifically to do with the model I built, but it's an interesting finding. Articles have been getting longer, much longer, over time. Some of this is to do with the journals getting rid of things like abstracts of APA or PSA papers. But it's also just a fact that articles have been getting longer. Here is a helpful graph that shows the patterns.

```{r article-length-graph, fig.height = 5, fig.cap = "Measures of article length in each year.", fig.alt = alt_text}
p <- ((1:5)/5) - 0.1
p_names <- map_chr(p, ~paste0("d.", .x*100, "%"))
p_funs <- map(p, ~partial(quantile, probs = .x, na.rm = TRUE)) %>% 
  set_names(nm = p_names)
length_deciles <- articles %>% 
  group_by(year) %>% 
  summarize_at(vars(length), p_funs) %>%
  pivot_longer(cols = starts_with("d."), names_to = "decile", names_prefix = "d.", values_to = "length")
ggplot(length_deciles, aes(x = year, y = length, color = decile)) + 
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(se = F, method = "loess", formula = 'y ~ x', size = 0.1) +
  freqstyle +
  theme(legend.title = element_blank()) +
  labs(x = element_blank(), y = "Number of Pages", title = "The Articles are Getting Longer (Again)")

alt_text <- "A scatterplot (with trend lines) showing the number of pages of articles from roughly 1880 to 2000, sorted into five deciles (10 percent, 30 percent, 50 percent, 70 percent, and 90 percent). Across all categories, the median number of pages has a peak in the early 1900s, drops until the middle of the centrury, and begins to rise sharply around 1980."
```

For each year, I've sorted the articles by length, then plotted the lengths of the articles at five decile markers. The red curve is the length of the article that is 10 percent of the way up the length table, the olive line is the article that is 30 percent of the way up the length chart, the green line is the length of the median article (by length), and so on. 

I'm using medians rather than means because the outliers here are really significant. I don't want the numbers to be thrown off by the fact that a journal publishes a single ninety-page article. But I also want to be able to see on the graph how much impact the one-page articles are having.

The latter turns out not to be too significant. Even when the articles are at their shortest in the early 1960s, the olive line only gets down to five pages. So even then, 70 percent or so of the articles are five or more pages. There are more abstracts and discussion notes being posted then than there are now, but not enough to explain all of what's happening. The red line creeps up very slowly as first the regular journals start abolishing short articles and then _Analysis_ starts increasing its average page length as well.

But here's the really striking feature of the graph: the median article in the 2010s is as long as the ninetieth percentile article from the 1950s and 1960s. For a while there, articles over twenty pages were real outliers. Now they are the norm. The outliers are now over thirty-five pages. This feels like a bad thing; articles are getting bloated, and we need to find a way to get them back to a reasonable length.

## The Bump {#the-bump}

A strange thing about these models is that there is a "bump" around the early 1980s. I'll explain what I mean by a bump in a minute, but I want to stress that this is not an artifact of the particular LDA model I'm using. It might be an artifact of the LDA process, but it turns up in practically every model I built, no matter how the different parameters get set. Just about the only constants across all model runs were that there was a huge topic on idealism, and there was a bump.

To get to the bump, start with a slightly different graph. One useful measure of the informativeness of a probability distribution is how different it is from the flat distribution. So if you've got a probability distribution over N atoms, the more distribution is more informative the further away it is from the distribution that assigns probability 1/N to each of the N atoms. There are a bunch of ways to measure distance here, but for simplicity I'll use a simple Pythagorean measure. This takes the sum (over the N atoms) of the square of the difference between the probability assigned to that atom, and 1/N. (If I was being more careful I'd divide by N to get an average distance, but since we're not going to be varying the number of atoms, this won't be necessary.)

Now one thing we can do with the model I've built is for each year, work out for each topic the average probability that an article in that year will be in that topic. Since this is the average of a bunch of probability functions, it is a probability function. And we can then ask how informative this function is by measuring its distance from this flat probability distribution. Or, more or less equivalently, we can ask how "normal" it is by taking the inverse of this informativeness measure. And if we plot that over time, we get the following graph.

```{r normality-graph, fig.height = 4, fig.cap="How similar the average probability distribution is to the flat distribution.", fig.alt = alt_text}
year_normality <- weight_ratio %>%
  group_by(year) %>%
  summarise(inform = sum((y - 1/90)^2)) %>%
  mutate(normality = 1/inform)

ggplot(year_normality, aes(x = year, y = normality)) + 
  geom_point(size = 0.5) +
  freqstyle +
  labs(x = element_blank(), y = "Normality (i.e., inverse informativeness)", title = "Everything happened in the 1980s")

alt_text <- "A scatterplot measuring the flatness of the probability distribution over all 90 topics in each year. It roughly measures how dispersed the topics are in each year; it would be at 0 if every article was definitely in the same topic, and at infinity if the average probability for each topic in the year was 1 in 90. It has a very sharp peak around 1982, rising fairly continuously and rapidly before that, and falling rapidly, and still fairly continuously, after that."
```

I don't know quite what I expected this to look like, but it wasn't *that*. The distribution gets flatter and flatter, at an accelerating rate, until about 1982, when it turns around and gets more informative in a hurry.

There are other ways of looking at the model that tell us similar things. Rather than looking at each topic one at a time, we can ask the following question for each year: what's the lowest number of (expected) articles in any topic that year? That can be thrown off by random topics, so we'll also look at the third lowest and fifth lowest average topic probability in each year.

```{r bump-graph, fig.height = 6, fig.cap = "Lowest, third lowest, and fifth lowest average topic probability by year.", fig.alt = alt_text}
bump_tibble <- weight_ratio %>%
  group_by(year) %>%
  top_n(5, -y) %>%
  dplyr::summarise(z1 = min(y), z2 = median(y), z3 = max(y)) %>%
  pivot_longer(
    cols = starts_with("z"),
    values_to = "y"
    )

ggplot(bump_tibble, aes(x = year, y = y, color = name, group = name)) + 
  geom_point(size = 0.5, alpha = 0.2) +
  freqstyle +
  geom_smooth(se = F, method = "loess", size = 0.2, formula = 'y ~ x') +
  scale_color_discrete(labels = c("Lowest", "Third Lowest", "Fifth Lowest")) +
  theme(legend.title = element_blank()) +
  labs(x = element_blank(), y = "Average Topic Probability", title = "No one stayed home in the 1980s")

alt_text <- "A scatterplot (with trend lines) showing the lowest, third lowest, and fifth lowest average topic probability across time from 1880 to 2010. In all three categories, the probability begins to rise dramatically around 1940 and peaks around 1980."
```

We see something very similar to the previous graph. Around the early 1980s, every topic is getting at least some attention from the model. But that changes the further we get away from 1980.

The results around 1980 are really striking I think. For several years, no topic is below 0.2 percent, and only 4fourtopics are below 0.5 percent. Since by definition the average topic is at 1.1 percent, this means that there are very few topics that are very far from the mean.

I don't have a good theory as to why this should be true. And there are two quite different explanations that seem plausible to me.

1. The twelve journals in the early 1980s really were more pluralist than they have been before or since. There was, at last, space for philosophy of biology, and formal epistemology. But there was still (largely thanks to _Philosophy and Phenomenological Research_ and _Philosophical Quarterly_) space for articles continuous with idealism, pragmatism and phenomenology.
2. It's just an artifact of the model-building process. If there is ever any kind of drift in topics, and there is always going to be some kind of drift in topics, there will be a point near the middle of the data set where all the topics are represented.

I don't know how to tell between these topics without running a lot more studies. For example, I could redo everything I've done, but stop in 1985, and see what these graphs look like. It might be that they look the same (with the last twenty-eight years missing), or it might be that the peaks move back fifteen or twenty years. It would take a huge amount of processing time to tell these apart, and I don't think it's a particularly worthwhile exercise.

And one reason for that (one I'll return to in the next chapter when faced with [a similar puzzle](#buzzwords-section)) is that the best way to solve this involves doing something that would be good to do independently: extend the model forwards in time. Hopefully in the future we'll see models like this that don't stop at 2013. The "artifact" explanation predicts that in those models, the bump will drift forwards a bit. The explanation in terms of actual pluralism predicts that it won't. When those models are built, we'll know more about what's driving the phenomena I've just graphed.

## Trans-Atlantic Philosophy {#atlantic-section}

Especially in the middle of the twentieth century, there is a very different feel to the UK journals as compared to the US journals. Or at least that's how it feels to me when I read those journals. Let's see how much the model agrees with that impressionistic assessment.

I'll run this test in two parts. First, I'll focus on 1924–1973. This takes us from the _Journal of Philosophy_ getting going until Ryle leaves the editorship of _Mind_. (He left in 1971, but the backlog of papers he left behind meant the next two years were spent publishing articles he accepted.) And so we're on roughly even footing, I'll focus on just four journals:

- Two British journals: _Mind_ and _Proceedings of the Aristotelian Society_.
- Two American journals: _Philosophical Review_ and _Journal of Philosophy_.

For each topic, I'll look at the proportion of the (weighted) articles they had in those four journals (over those fifty years) that were in the British journals. This is a rough and ready way of teasing apart what UK and US philosophy looked like over that time. (Obviously neither pair of journals is fully representative of their country's philosophical scene, but they aren't the worst proxies either.)

I'm leaving off topics 81–90 because they are so little represented in the journals over this time period that the ratios being graphed are more noise than signal.

```{r atlantic-mid-century-setup}
uk_mid_century_articles <- articles %>%
  filter(journal == "Mind" | journal == "Proceedings of the Aristotelian Society") %>%
  filter(year > 1923) %>%
  filter(year < 1974)

uk_mid_century_gamma <- relabeled_gamma %>%
  filter(document %in% uk_mid_century_articles$document) %>%
  filter(topic < 81) %>%
  group_by(topic) %>%
  dplyr::summarise(uk = sum(gamma))

us_mid_century_articles <- articles %>%
  filter(journal == "Journal of Philosophy" | journal == "Philosophical Review") %>%
  filter(year > 1923) %>%
  filter(year < 1974)

us_mid_century_gamma <- relabeled_gamma %>%
  filter(document %in% us_mid_century_articles$document) %>%
  filter(topic < 81) %>%
  group_by(topic) %>%
  dplyr::summarise(us = sum(gamma))

atlantic_mid_century_gamma <- inner_join(uk_mid_century_gamma, us_mid_century_gamma, by = "topic") %>%
  inner_join(the_categories, by = "topic") %>%
  mutate(topicfactor = as.factor(topic)) %>%
  mutate(rat = uk/(uk + us))

cols <- tibble(x = 1:90) %>%
  mutate(col = hcl(h = (x-1)*(360/90)+15, l = 65, c = 100)) %>%
  deframe()
```

```{r mid-century-graph-by-topic, fig.height = 12.2, fig.cap = "Proportion of articles in the big four journals that are in UK journals, 1924–1973.", fig.alt = alt_text}
ggplot(atlantic_mid_century_gamma, aes(x = reorder(fcap(sub_lower), -topic), y = rat, fill = topicfactor)) +
  geom_col(width = 0.6) +
  freqstyle +
  coord_flip() +
  theme(legend.position = "none") +
  labs(y = "Proportion of Articles in UK journals", x = NULL) +
  scale_y_continuous(expand = expansion(mult = c(0, .03))) +
  scale_fill_manual(values = cols)

alt_text <- "A histogram showing the proportion of articles in the big four journals that are in UK journals from 1924 to 1973, across eighty topics."
```

As we can see, there's quite a bit of variation there. Let's reorder the bars so the extremes are more visible.

```{r mid-century-graph-by-ratio, fig.height = 12.2, fig.cap = "Proportion of articles in the big four journals that are in UK journals, 1924–1973.", fig.alt = alt_text}
ggplot(atlantic_mid_century_gamma, aes(x = reorder(fcap(sub_lower), -rat), y = rat, fill = topicfactor)) +
  geom_col(width = 0.6) +
  freqstyle +
  coord_flip() +
  theme(legend.position = "none") +
  labs(y = "Proportion of Articles in UK journals", x = NULL) +
  scale_y_continuous(expand = expansion(mult = c(0, .03))) +
  scale_fill_manual(values = cols)

alt_text <- "A histogram showing the proportion of articles in the big four journals that are in UK journals from 1924 to 1973, across eighty topics, sorted in order of lowest to highest proportion. Heidegger and Husserl is the lowest, and Propositions and Implications is the highest."
```

There is a lot more phenomenology in the US journals than the UK journals. This surprised me a little, since I didn't include _Philosophy and Phenomenological Research_ in the study. Less surprisingly, [Dewey and pragmatism](#topic05) is a more American than British topic. After that, there is much more attention to philosophy of science, and social and political philosophy.

On the UK side, there is [ordinary language philosophy](#topic24). That's not news, but it's nice to see the model found this pattern. There are a couple of topics that have tiny numbers on each side of the Atlantic in these journals: [game theory](#topic75) and [feminism](#topic69). But otherwise the focus in the United Kingdom is more heavily on language and on ethics. The latter is a bit of a theme across the journals. From day one there is attention being paid to questions of value. But for a long time there is very little work on ethics that treats it as an autonomous subject as opposed to deriving moral conclusions from, e.g., metaphysical premises.

There is also, as perhaps should be clear from the names of the journals, more philosophy of mind in the UK journals. Apart from idealist-tinged works on self-consciousness, there isn't much of this in the US journals. There were more psychology articles in the _Philosophical Review_ in its early years, but they had stopped by 1924.

Let's do the same study for 1974–2013. This time I'll leave off the first ten topics, because they are largely noise.

```{r late-century-atlantic-setup}
uk_late_century_articles <- articles %>%
  filter(journal == "Mind" | journal == "Proceedings of the Aristotelian Society") %>%
  filter(year > 1973)

uk_late_century_gamma <- relabeled_gamma %>%
  filter(document %in% uk_late_century_articles$document) %>%
  filter(topic > 10) %>%
  group_by(topic) %>%
  dplyr::summarise(uk = sum(gamma))

us_late_century_articles <- articles %>%
  filter(journal == "Journal of Philosophy" | journal == "Philosophical Review") %>%
  filter(year > 1973)

us_late_century_gamma <- relabeled_gamma %>%
  filter(document %in% us_late_century_articles$document) %>%
  filter(topic >10) %>%
  group_by(topic) %>%
  dplyr::summarise(us = sum(gamma))

atlantic_late_century_gamma <- inner_join(uk_late_century_gamma, us_late_century_gamma, by = "topic") %>%
  inner_join(the_categories, by = "topic") %>%
  mutate(topicfactor = as.factor(topic)) %>%
  mutate(rat = uk/(uk + us))

cols <- tibble(x = 1:90) %>%
  mutate(col = hcl(h = (x-1)*(360/90)+15, l = 65, c = 100)) %>%
  deframe()
```

```{r late-century-atlantic-graph-topic, fig.height = 12.2,  fig.cap = "Proportion of articles in the big four journals that are in UK journals, 1974–2013.", fig.alt = alt_text}
ggplot(atlantic_late_century_gamma, aes(x = reorder(fcap(sub_lower), -topic), y = rat, fill = topicfactor)) +
  geom_col(width = 0.6) +
  freqstyle +
  coord_flip() +
  theme(legend.position = "none") +
  labs(y = "Proportion of Articles in UK journals", x = NULL) +
  scale_y_continuous(expand = expansion(mult = c(0, .03))) +
  scale_fill_manual(values = cols)

alt_text <- "A histogram showing the proportion of articles in the big four journals that are in UK journals, from 1974 to 2013, across 70 topics."
```

Again, there is substantial variation. I think this is telling us more about the journals than the philosophical scenes in the different countries, but I think there are some signals here. Let's reorder that graph to make the outliers more explicit.

```{r late-century-atlantic-graph-ratio, fig.height = 12.2, fig.cap = "Proportion of articles in the big four journals that are in UK journals, 1974-2013", fig.alt = alt_text}
ggplot(atlantic_late_century_gamma, aes(x = reorder(fcap(sub_lower), -rat), y = rat, fill = topicfactor)) +
  geom_col(width = 0.6) +
  freqstyle +
  coord_flip() +
  theme(legend.position = "none") +
  labs(y = "Proportion of Articles in UK journals", x = NULL) +
  scale_y_continuous(expand = expansion(mult = c(0, .03))) +
  scale_fill_manual(values = cols)

alt_text <- "A histogram showing the proportion of articles in the big four journals that are in UK journals, from 1974 to 2013, sorted in order of lowest to highest proportion. Evolutionary Biology is the lowest, and Vagueness is the highest."
```

The US side is very heavily represented by philosophy of science topics. I don't think that's really a national difference; it mostly tells us that the _Journal of Philosophy_ was the generalist journal that was most friendly to philosophy of science.

The UK side is a bit more interesting. logic, philosophy of language, and philosophy of mind are very heavily represented at that end of the graph. It isn't surprising that [speech acts](#topic63), [perception](#topic47) and [concepts](#topic78) are near that end, but the magnitude was greater than I expected. And I had no idea that [vagueness](#topic86) was so English; though maybe if I'd included other US journals here (especially _Noûs_) this would have been different.

It would be useful to have more matching pairs of journals to confirm this, but I think this is some evidence of a fairly substantial split in interests between the two sides of the Atlantic.

## Raw Counts and Weighted Counts {#raw-weight-count}

```{r counting-tibble-creation}
counting_tibble <- tibble(
  topic = 1:90, r_c = 0, w_in_r = 0
)

weight_count <- relabeled_gamma %>%
  group_by(topic) %>%
  dplyr::summarise(w_c = sum(gamma))

for (i in 1:90){
counting_tibble$topic[i] <- i
articles_in_topic <- relabeled_articles %>% filter(topic == i)
counting_tibble$r_c[i] <- nrow(articles_in_topic)
counting_tibble$w_in_r[i] <- sum(filter(relabeled_gamma, topic == i, document %in% articles_in_topic$document)$gamma)
}

counting_tibble <- inner_join(counting_tibble, weight_count, by = "topic")

counting_tibble <- counting_tibble %>%
  mutate(x = w_in_r/r_c, y = w_c/r_c, z = w_in_r/w_c)

# ggplot(counting_tibble, aes(x = x, y = z)) + geom_point()

counting_tibble <- counting_tibble %>%
  inner_join(the_categories, by = "topic")

counting_tibble <- counting_tibble %>%
  mutate(topicfactor = as.factor(topic))

m <- counting_tibble$r_c[80]
mw <- counting_tibble$w_c[80]
mw <- round(mw, 2)
```

This section has two aims.

The first aim is to see if we can find a measure of how specialised a topic is. Intuitively, some topics are extremely specialized - only people working in quantum physics talk about [quantum physics](#topic66). But other topics cut across philosophy - everyone talks about [arguments](#topic55). Is there some way internal to the model to capture that notion?

The second is to cast some light on the relationship between the two ways of measuring topic size I've been using: raw count and weighted count. The raw count is the number of articles such that the probability of being in that topic is higher than the probability of being in any other topic. The weighted count is the sum, across all articles, of the probability of being in that topic. Mathematically, it is the expected number of articles in the topic.

I'll start by looking at the relationship between the raw count and the weighted count for a single topic: [modality](#topic80). And then I'll turn to see what happens when the focus expands to the other 89 topics. Start with two variables:

- $r$, the raw count of articles in the topic. For modality, this is `r m`. That is, there are `r m` articles that the model thinks are more probably in Modality than in any other topic.
- $w$, the weighted count of articles in the topic. For modality, this is `r mw`. That is, across all the articles, the sum of the probabilities that they are in modality is `r mw`.

So these are fairly close, though as we'll see, that's not typical. (Indeed, I picked Modality to focus on because they were close, and I wanted to see how typical that was.) There is a third variable I'm going to spend a bit of time on. Focus on those `r m` articles whose probablility of being in Modality is greatest. The model gives them very different probabilities of being in Modality. Here are two articles from the `r m`.

```{r modality-gamma-high-low}
modality_tibble <- relabeled_articles %>%
  filter(topic == 80) %>%
  arrange(-gamma)
```

```{r modality-high-article}
individual_article(modality_tibble$document[1])
```

```{r modality-low-article}
individual_article(modality_tibble$document[370])
```

These were not picked at random. The Bird article is the one the model is most confident is in modality, and the Keyt article is the one of the `r m` that it is least confident about. (Honestly I think the model got this one wrong and is confused by _Lewis_.) There is a range of probabilities between those though.

Among the `r m` articles that make up the raw count for modality, the average probability the model gives to them being in the topic is `r counting_tibble$x[80]`. (That's a little under the midpoint between the Bird and the Keyt articles, but not absurdly so.) This is the value for modality of the third variable I'm interested in:

- $p$, the average probability of being in the topic among articles that are more probably in that topic than any other.

Between $r$, $w$ and $p$, there are three things that look like plausible specialization measures.

Measure One
:    Ratio of raw count to weighted count, $\frac{r}{w}$.

Measure Two
:    Average probability of being in the topic among articles that have maximal probability of being in the topic, i.e.,$p$.

Measure Three
:    What proportion of the weighted count comes from articles that are "in" the topic, i.e., $\frac{rp}{w}$.

The first measure is intuitive because (as we'll see) it places arguments right at the bottom of the scale. And that's intuitively our least specialized topic.

The second measure is intuitive because it measures how confident the model is in its placement for articles that get maximal probability of being in a topic. And specialized topics should, in general, do well on this. We won't find the model thinking that this article is most probably a quantum Physics article, but maybe jusy maybe it's a [social contract theory](#topic31) article, and maybe it's a [depiction](#topic42) article, so we should spread the probability around between those.

The third measure is intuitive for a similar reason. If a topic is specialized, it won't pick up an extra 3 percent there or 5 percent there to its weighted count from articles in other topics. Most of $w$ will come from articles "in" the topic, i.e., from the part of $w$ that $rp$ measures.

The second and third are obviously related, since $p = \frac{rp}{r}$. In some sense, they are just different ways of normalizing $rp$ to the size of the topic.

But intuitively all three should be related, since they all feel like measures of specialization. It turns out this isn't quite right.

Let's start with measure one, the ratio of raw to weighted count. This is something I talked a bit about back in chapter \@ref(all-90-topics) when discussing two ways of measuring topic size. 

```{r counting-tibble-y-topic, fig.height=12.2, fig.cap = "Ratio of raw count to weighted count by topic.", fig.alt = alt_text}
# Graph for Measuring Ratio of Weighted Count to Raw Count

ggplot(counting_tibble, aes(x = reorder(fcap(sub_lower), -topic), y = 1/y, fill = topicfactor)) +
  geom_col(width = 0.6) +
  freqstyle +
  coord_flip() +
  theme(legend.position = "none") +
  labs(y = "Ratio of Raw Count to Weighted Count", x = NULL) +
  scale_y_continuous(expand = expansion(mult = c(0, .03)))

alt_text <- "A histogram showing the ratio of raw to weighted count across article topics."
```

There is an enormous outlier here: [arguments](#topic55). (Though [self-consciousness](#topic12) and [concepts](#topic78) are also fairly low.) This makes sense—at least once the model decides that this will be a topic. There aren't that many articles that are about arguments as such, but there are plenty of discussions of arguments in papers, so there are lots of ways to talk the model into thinking there's a 2 or 3 percent chance that that's the right topic for a particular paper.

To get a sense of how big the outliers are, it is useful to sort this graph by the ratio it represents.

```{r counting-tibble-y-ratio, fig.height=12.2, fig.cap = "Ratio of raw count to weighted count by topic (sorted).", fig.alt = alt_text}
# Graph for Measuring Ratio of Weighted Count to Raw Count

ggplot(counting_tibble, aes(x = reorder(fcap(sub_lower), -y), y = 1/y, fill = topicfactor)) +
  geom_col(width = 0.6) +
  freqstyle +
  coord_flip() +
  theme(legend.position = "none") +
  labs(y = "Ratio of Raw Count to Weighted Count", x = NULL) +
  scale_y_continuous(expand = expansion(mult = c(0, .03)))

alt_text <- "A histogram showing the ratio of raw to weighted count across article topics, sorted from highest to lowest ratio. Beauty has the highest ratio, and Arguments has the lowest."
```

I don't quite understand why [beauty](#topic08) and [crime and punishment](#topic36) are at the top of this graph. But I'll keep a note of where beauty appears in subsequent graphs, because it is an interesting case.

Now we'll look at the second measure, i.e., $p$. For modality, the value of $p$ is `r counting_tibble$x[80]`. How typical is that? Is the average maximal topic probability always around 0.4, or does it vary by topic? Let's look at the data,, and note that the scale does not start at zero.

```{r counting-tibble-x-topic, fig.height=12.2, fig.cap = "Average maximal probability by topic.", fig.alt = alt_text}
# Graph for Measuring average gamma of articles in a topic

ggplot(counting_tibble, aes(x = reorder(fcap(sub_lower), -topic), y = x, fill = topicfactor)) +
  geom_col(width = 0.6) +
  freqstyle +
  coord_flip(ylim = c(0.2, 0.6)) +
  theme(legend.position = "none") +
  labs(y = "Average Topic Probability of Articles in Topic", x = NULL)

alt_text <- "A histogram showing the average maximal probability of articles across topics."

```

It turns out modality is reasonably typical, though there is a lot of variation here. Let's look at the same graph but sorted by probability rather than topic number.

```{r counting-tibble-x-prob, fig.height=12.2, fig.cap = "Average maximal probability by topic (sorted)", fig.alt = alt_text}
ggplot(counting_tibble, aes(x = reorder(fcap(sub_lower), x), y = x, fill = topicfactor)) +
  geom_col(width = 0.6) +
  freqstyle +
  coord_flip(ylim = c(0.2, 0.6)) +
  theme(legend.position = "none") +
  labs(y = "Average topic probability of articles in topic", x = NULL)

alt_text <- "A histogram showing the average maximal probability of articles across topics, sorted from highest to lowest. Evolutionary biology has the highest average maximal probability, and ordinary language has the lowest."

```

This does look like a measure of specialization. If a paper is talking about [quantum physics](#topic66) or [evolutionary biology](#topic82), then it's probably really talking about quantum physics or evolutionary biology. But lots of people use [ordinary language](#topic24) and discuss [concepts](#topic78). So lots of articles that are about lots of different topics can end up looking like ordinary language or concepts articles.

Let's turn to our last measure of specialization, $\frac{rp}{w}$. One way to think about how big that is, is to think about the average probability of being in modality for the articles whose maximal probability is in one of the other eighty-nine topics. Some of these probabilities are quite large, such as for this article.

```{r modal-find-highest-second-best}
mm <- relabeled_gamma %>%
  filter(topic == 80) %>%
  filter(!document %in% filter(relabeled_articles, topic == 80)$document) %>%
  arrange(-gamma)

individual_article(mm$document[1])
```

But most contributions aren't that big. In fact, the average is under 1 percent. But there are nearly 32,000 of them, so they add up. In fact, they add up to about 61 percent of the weighted count for ,odality. So the value of $\frac{rp}{w}$ for Modality is about 39 percent, since that's what is left once you take out the 61 percent that comes from the rest of the articles. Is that 39 percent typical? Well, we can again look at the graph for all ninety topics.

```{r counting-tibble-z-topic, fig.height=12.2, fig.cap = "Proportion of weighted count that's from non-topic articles.", fig.alt = alt_text}
ggplot(counting_tibble, aes(x = reorder(fcap(sub_lower), -topic), y = z, fill = topicfactor)) +
  geom_col(width = 0.6) +
  freqstyle +
  coord_flip(ylim = c(0, 0.8)) +
  theme(legend.position = "none") +
  labs(y = "Proportion of Weighted Count that's From Outside Topic", x = NULL)

alt_text <- "A histogram showing the proportion of weighted count that's from non-topic articles, across topics."
```

And 39 percent is somewhat middling, though there's so much variation that I'm not sure what I'd say is typical. Let's look at that one arranged by order.

```{r counting-tibble-z-ratio, fig.height=12.2, fig.cap = "Proportion of weighted count that's from non-topic articles (sorted).", fig.alt = alt_text}
ggplot(counting_tibble, aes(x = reorder(fcap(sub_lower), z), y = z, fill = topicfactor)) +
  geom_col(width = 0.6) +
  freqstyle +
  coord_flip(ylim = c(0, 0.8)) +
  theme(legend.position = "none") +
  labs(y = "Proportion of Weighted Count that's From Outside Topic", x = NULL)

alt_text <- "A histogram showing the proportion of weighted count that's from non-topic articles, across topics, sorted from highest to lowest. Quantum Phsyics has the highest proportion, and Arguments has the lowest."

```

And again, it looks like a measure of specialization. The model doesn't give quantum physics much credit for articles that aren't squarely about quantum physics. But it gives arguments lots of credit for articles that are not primarily about arguments.

If these three things are all sort of measures of specialization, they should correlate reasonably well. But this turns out not quite to be right. The first and second, for example, are not particularly well correlated. Here is the graph of the two of them, i.e., $\frac{r}{w}$ and $p$, against each other.

```{r compare-specialization-measures-b, fig.height = 6, fig.cap = "Correlation between the first and second specialization measures.", fig.alt = alt_text}
# Comparing Second and Third
# First specialization measure - Raw count/Weighted Count
# Second specialization measure - Average in-topic topic probability
# Third specialization measure - Weighted count that's outside topic

ggplot(counting_tibble, aes(x = 1/y, y = x, color = topicfactor)) + 
  geom_point() +
  freqstyle +
  labs(x = "Ratio of raw count to weighted count", y = "Average topic probability of articles in topic") +
  ggrepel::geom_text_repel(aes(label=ifelse(topic == 8 | topic == 68 | topic == 55 | topic == 82 | topic == 78,as.character(sub_lower),''))) +
  theme(legend.position = "none") +
  scale_x_continuous(expand = expansion(mult = c(0.1, .2))) +
  scale_y_continuous(expand = expansion(mult = c(0.1, .1)))

alt_text <- "A scatterplot comparing the ratio of raw count to weighted count (r/w) to the average topic probability of articles (p), across topics; i.e., comparing the first and second specialization measures. There is a weak trend."

```

There is a trend there, but it's not a strong one. But the other two pairs are much more closely correlated. (We have here a real-life example of how _being closely correlated_ is not always transitive.) Here is the graph of the first against the third (i.e., $\frac{r}{w}$ on one axis, and $\frac{rp}{w}$ on the other).

```{r compare-specialization-measures-c, fig.height = 6, fig.cap = "Correlation between the first and third specialization measures.", fig.alt = alt_text}
# Comparing First and Third
# First specialization measure - Raw count/Weighted Count
# Second specialization measure - Average in-topic topic probability
# Third specialization measure - Weighted count that's outside topic

ggplot(counting_tibble, aes(x = 1/y, y = z, color = topicfactor)) + 
  geom_point() +
  freqstyle +
  labs(x = "Ratio of raw count to weighted count", y = "Proportion of weighted count that's from outside topic") +
#  geom_text(aes(label=ifelse(topic == 8 | topic == 68 | topic == 55 | topic == 82 | topic == 78,as.character(subject),'')),hjust=0.2,vjust=1.7) +
  ggrepel::geom_text_repel(aes(label=ifelse(topic == 8 | topic == 68 | topic == 55 | topic == 82 | topic == 78,as.character(sub_lower),''))) +
  theme(legend.position = "none") +
  scale_x_continuous(expand = expansion(mult = c(0.05, .1))) +
  scale_y_continuous(expand = expansion(mult = c(0.12, .05)))

alt_text <- "A scatterplot comparing the ratio of raw count to weighted count (r/w) to the proportion of weighted count that's from outside the topic (rp/w); i.e., comparing the first and third specializaiton measures. The two measures are well-correlated."

```

Interestingly, although those two are correlated, part of what that means is that there are some cases that they both misclassify if construed as measures of specialization. If we're looking for a measure of specialization, we don't want [functions](#topic68) and [evolutionary biology](#topic82) at opposite ends. 

The last two measures, $p$ and $\frac{rp}{w}$ are also well correlated, though with some outliers.

```{r compare-specialization-measures-a, fig.height = 6, fig.cap = "Correlation between the second and third specialization measures.", fig.alt = alt_text}
# Comparing Second and Third
# First specialization measure - Raw count/Weighted Count
# Second specialization measure - Average in-topic topic probability
# Third specialization measure - Weighted count that's outside topic

ggplot(counting_tibble, aes(x = x, y = z, color = topicfactor)) + 
  geom_point() +
  freqstyle +
  labs(x = "Average topic probability of articles in topic", y = "Proportion of weighted count that's from outside topic.") +
  ggrepel::geom_text_repel(aes(label=ifelse(topic == 8 | topic == 68 | topic == 55 | topic == 82 | topic == 78,as.character(sub_lower),''))) +
  theme(legend.position = "none") +
  scale_x_continuous(expand = expansion(mult = c(0.1, .2))) +
  scale_y_continuous(expand = expansion(mult = c(0.1, .1)))

alt_text <- "A scatterplot comparing the averate topic probability of articles in topic (p) to the proportion of weighted count that's from outside the topic (rp/w); i.e. comparing the second and third specialization measures. The two measures are well-correlated, with some outliers."
```

And our two measures of specialization do sort of line up. There are some outliers—[functions](#topic68) is above the line and Beauty is below it.

I think that what we learn from that is that the best measure of specialization is $p$, the average probability of being in the topic among articles whose maximal probability is being in just that topic. The other measures are roughly correlated with $p$, but where they differ, $p$ seems to do a better job of measuring specialization. And they are (somewhat surprisingly) very well correlated with each other.

And we also learned something about the relationship between $r$ and $w$. It's really well correlated with $\frac{rp}{w}$. And the best way to understand $\frac{rp}{w}$ is to think about the average probability of being in a topic when that topic isn't maximal. So $r$ is low relative to $w$ when a topic is often the second, third or fourth highest topic, and high when it is not. This isn't surprising, but I had thought that this would in turn line up with how specialized a topic is. And that didn't really do that. What did line up with intuitive specialization is the average probability of being in a topic among those papers where that topic probability is maximal.

<!--chapter:end:08-outliers.Rmd-->

# Looking Outwards {#lookingoutward}

To conclude the book, I end by looking at three ways in which the model can tell us about things beyond the journals in the years 1876–2013. The first two use a technique I haven't used yet: applying the LDA model out of sample to texts that the model wasn't built around. The third (and last) looks at word trends within the the journals in recent years, and makes some speculations about how those trends might continue.

## Classic Books {#classic-book-section}

```{r get-gutenberg-books}
# # This is commented out because it isn't actually run in building the book
# # In the middle of the next chunk we just load the results
# # This is to save having to put calls on the Gutenberg website over and over again
# # Get Books with Chapter Headings
# 
# # 5827 is Problems of Philosophy
# # 46743 is Methods of Ethics
# # 15776 is Economic Consequences of the Peace
# # 852 is Democracy and Education
# 
# require(gutenbergr)
# 
# titles <- c(5827, 46743, 15776)
# 
# books <- gutenberg_download(titles, meta_fields = "title")
# 
# # Have to Get Moore separately because it doesn't have metadata in gutenberg package
# # Not sure why this throws up so many errors
# # Note we'll have to have all this happening offline - can't download the books every time we compile
# 
# moore <- gutenberg_download(53430) %>%
#   mutate(title = "Principia Ethica")
# 
# books <- bind_rows(books, moore)
# 
# # On Liberty
# # Have to Get Rid of opening or it thinks the table of contents is five chapters
# 
# mill <- gutenberg_download(34901, meta_fields = "title") %>%
#   slice(-1:-440)
# 
# books <- bind_rows(books, mill)
# 
# require(stringr)
# 
# # divide into documents, each representing one chapter
# by_chapter <- books %>%
#   group_by(title) %>%
#   mutate(chapter = cumsum(str_detect(text, regex("^CHAPTER ", ignore_case = FALSE)))) %>%
#   ungroup() %>%
#   filter(chapter > 0) %>%
#   unite(document, title, chapter)
# 
# # Get Books with lower case chapter headings
# # In this case just Democracy and Education
# # And need to slice it as well because of the annoying table of contents
# 
# dewey <- gutenberg_download(852) %>%
#   add_column(title = "Democracy and Education")
# 
# dewey_chapters <- dewey %>% 
#   slice(-1:-52) %>%
#   group_by(title) %>%
#   mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
#   ungroup() %>%
#   filter(chapter > 0) %>%
#   unite(document, title, chapter)
# 
# # Get Books with Lecture Headings
# 
# lecture_titles <- c("Our Knowledge of the External World as a Field for Scientific Method in Philosophy", "The Analysis of Mind")
# 
# lecture_books <- gutenberg_works(title %in% lecture_titles) %>%
#   gutenberg_download(meta_fields = "title")
```

```{r gutenberg-analyse}
# # divide into documents, each representing one chapter
# lecture_by_chapter <- lecture_books %>%
#   group_by(title) %>%
#   mutate(chapter = cumsum(str_detect(text, regex("^LECTURE ", ignore_case = FALSE)))) %>%
#   ungroup() %>%
#   filter(chapter > 0) %>%
#   unite(document, title, chapter)
# 
# # Put the lectures and the chapters back together
# 
# by_chapter <- bind_rows(by_chapter, lecture_by_chapter, dewey_chapters)

load("by_chapter.RData")

# split into words
by_chapter_word <- by_chapter %>%
  unnest_tokens(word, text)

# find document-word counts
word_counts <- by_chapter_word %>%
  filter(!word %in% short_words) %>%
  filter(nchar(word) > 2) %>%
  count(document, word, sort = TRUE) %>%
  ungroup()

# Create DTM

chapters_dtm <- word_counts %>%
  cast_dtm(document, word, n)

# Apply the old LDA to them
gutenberg_lda <- posterior(thelda, chapters_dtm)

# Get the topic probabilities
# The mess here is to move the topic numbers that it uses internally onto my topic numbers (which are chronological)
gutenberg_gamma<- as_tibble(gutenberg_lda$topics, rownames = NA) %>%
  rownames_to_column(var = "document") %>%
  pivot_longer(-document) %>%
  select(document, topic = name, gamma = value) %>%
  mutate(topic = as.integer(topic)) %>%
  inner_join(year_topic_mean, by = "topic") %>%
  select(document, topic = rank, gamma) %>%
  arrange(document, -gamma)

gg_revised <- gutenberg_gamma %>% 
  mutate(chap_num = str_extract(document, "[^_]+$")) %>%
  mutate(book_name = str_extract(document, "^.*(?=_)"))
```

In this section I look at 8 philosophy books that are available on [gutenberg.org](http://gutenberg.org). The books are:

- [_Democracy and Education_](https://www.gutenberg.org/ebooks/852), by John Dewey
- [_The Economic Consequences of the Peace_](https://www.gutenberg.org/ebooks/15776), by J. M. Keynes^[Is this a philosophy book? Well, I think it's an important work of applied political philosophy.]
- [_On Liberty_](https://www.gutenberg.org/ebooks/34901), by John Stuart Mill
- [_Principia Ethica_](https://www.gutenberg.org/ebooks/53430), by G. E. Moore
- [_Our Knowledge of the External World_](https://www.gutenberg.org/ebooks/37090), by Bertrand Russell
- [_The Analysis of Mind_](https://www.gutenberg.org/ebooks/2529), by Bertrand Russell
- [_The Problems of Philosophy_](The Problems of Philosophy), by Bertrand Russell
- [_The Methods of Ethics_](https://www.gutenberg.org/ebooks/46743), by Henry Sidgwick

I then applied the model to these eight books, one chapter at a time. That is, I asked the model what probability it gave to each chapter from each of these books to being in each of the ninety topics. The outputs looked like this. (I'm just including the ten topics with the highest probability.)

```{r gutenberg-top-ten}
gtt <- function(x, y){
kable(gg_revised %>%
    filter(book_name == x, chap_num == y) %>%
    select(topic, gamma) %>%
    arrange(-gamma) %>%
    slice(1:10) %>%
    inner_join(the_categories, by = "topic") %>%
    mutate(gamma = round(gamma, 4)) %>%  
    select(sub_lower, gamma) %>%
    mutate(sub_lower = fcap(sub_lower)),
    col.names = c("Subject", "Probability"),
    caption = paste0("_", x, "_, chapter ", y,".")) %>%
    kable_styling(full_width = F)
}
```

```{r russell-chap-five}
gtt("The Problems of Philosophy", 5)
```

There are three things to note about this table.

One is that the probabilities are very widely spread around. This is what normally happens when doing these out-of-sample applications. The model is much more confident about the data it was trained on than it is about other data. And even in the training data, the average maximal probability was around 0.4. Here it is more usually 0.2 or lower.

The second is that [ordinary language](#topic24) plays an outside role in these models. The fact that it really isn't like the other topics, that it is a style as much as a subject matter, keeps complicating the analysis.

And the third is that the topics here are old. Chapter 5 of _The Problems of Philosophy_ reminds the model a little of "On Denoting", which makes sense, and a little of Frege, which also makes sense, but the other subjects are very old. In fact, what's surprising about this chapter is that it reminds the model of two relatively modern topics, not that it has eight or more old topics mixed in.

Let's look at the top topic across each of the books. Since I am averaging the chapter probabilities, these numbers will be even lower than for individual chapters.

```{r gutenberg-book-function}
gbb <- function(x){
  kable(filter(gg_revised, book_name == x) %>%
          group_by(topic) %>%
          summarise(g = mean(gamma)) %>%
          arrange(-g) %>%
          slice(1:10) %>%
          inner_join(the_categories, by = "topic") %>%
          select(subject, g),
    col.names = c("Subject", "Probability"),
    caption = x) %>%
    kable_styling(full_width = F)      
}
```

```{r democracy-and-education}
gbb("Democracy and Education")
```

It's a bit surprising that the model doesn't identify this with [pragmatism](#topic05), and even more surprising that [feminism](#topic70) turns up here. But otherwise this broadly makes sense.

```{r keynes-book}
gbb("The Economic Consequences of the Peace")
```

This, on the other hand, doesn't look quite right. It's about World War I, so I guess it looks like war. But it isn't really a history book. And it's certainly not a Marx book. This does look like it pushed the model way past its comfort level.

```{r on-liberty}
gbb("On Liberty")
```

Putting this in with [other history](#topic04) does make some sense because a few of the papers in that topic are about Mill. But it's striking to me that the model thinks of this book as going with social work of its time, even more than it sees it as going with topics on liberalism, or freedom. This is bringing up a limit of this approach that we've seen a few times before. Literary styles change over time, and the model doesn't do well with that kind of change.

```{r principia}
gbb("Principia Ethica")
```

One way to look at this data is that it's bringing out the extent to which the ordinary language movement wasn't a repudiation of the philosophy that had gone before them, but a return to the way of doing philosophy exemplified by Moore and Russell. The model does not typically think papers from 1903 are ordinary language papers, but it does think that _Principia Ethica_ is ordinary language.

```{r external-world}
gbb("Our Knowledge of the External World as a Field for Scientific Method in Philosophy")
```

The model really doesn't identify _Our Knowledge of the External World_ with any of the contemporary topics. The closest is [mathematics](#topic51), but this book seems surprisingly dated.

```{r analysis-of-mind}
gbb("The Analysis of Mind")
```

This is part of why I was happy to include [psychology](#topic01) as a philosophy of mind topic. It is a bit different to how we now do philosophy of mind. But it includes a lot of what Russell does in _The Analysis of Mind_. And that's a paradigm of a philosophy of mind book.

```{r problems-philosophy}
gbb("The Problems of Philosophy")
```

Again, we see that Moore and Russell were precursors as much as opponents of ordinary language philosophy. And between [perception](#topic48), [denoting](#topic63), [knowledge](#topic74) and [justification](#topic76), we see flickers of contemporary philosophy entering into the picture.

```{r methods-ethics}
gbb("The Methods of Ethics")
```

This, on the other hand, is a bit disappointing. I would have thought it would have done a better job of identifying _The Methods of Ethics_ as, well, a work of ethics. And we do see a few topics from Ethics here, but also a lot of others.

Let's turn to chapters. I'm not going to go through every chapter and display the topics for it. But it is interesting to look at the chapters the model is most confident about.

```{r gutenberg-high-confidence}
gg_high_gamma <- gg_revised %>%
  arrange(-gamma) %>%
  inner_join(the_categories, by = "topic") %>%
  select(book_name, chap_num, sub_lower, gamma, topic) %>%
  mutate(book_name = replace(book_name, book_name == "Our Knowledge of the External World as a Field for Scientific Method in Philosophy", "Our Knowledge of the External World"))
```

```{r gutenberg-highest-a}
kable(gg_high_gamma %>%
        select(book_name, chap_num, sub_lower, gamma) %>%
        mutate(sub_lower = fcap(sub_lower)) %>%
        slice(1:10),
      col.names = c("Book", "Chapter", "Subject", "Probability"),
      digits = c(0, 0, 0, 4),
      caption = "Ten chapters with the highest topic probabilities.")
```

It really is confident about Russellian works, relatively speaking. Let's see what happens if we leave off psychology.

```{r gutenberg-highest-b}
kable(gg_high_gamma %>%
        filter(topic > 1) %>%
        select(book_name, chap_num, sub_lower, gamma) %>%
        mutate(sub_lower = fcap(sub_lower)) %>%
        slice(1:10),
      col.names = c("Book", "Chapter", "Subject", "Probability"),
      digits = c(0, 0, 0, 4),
      caption = "Ten chapters with the highest topic probabilities (excluding psychology).")
```

And pushing further forward, let's see what happens if we leave off all of topics 1–30.

```{r gutenberg-highest-c}
kable(gg_high_gamma %>%
        filter(topic > 30) %>%
        select(book_name, chap_num, sub_lower, gamma) %>%
        mutate(sub_lower = fcap(sub_lower)) %>%
        slice(1:10),
      col.names = c("Book", "Chapter", "Subject", "Probability"),
      digits = c(0, 0, 0, 4),
      caption = "Ten chapters with the highest topic probabilities (excluding first thirty topics).")
```

This doesn't look like the model is doing too bad a job. Russell does talk about philosophy of mathematics, Keynes about war, and Mill about liberal democracy. And remember that egalitarianism is largely about Parfit, and hence about consequentialism, so it isn't surprising Sidgwick ends up there. What if we restrict things to topics from 61–90?

```{r gutenberg-highest-d}
kable(gg_high_gamma %>%
        filter(topic > 60) %>%
        select(book_name, chap_num, sub_lower, gamma) %>%
        mutate(sub_lower = fcap(sub_lower)) %>%
        slice(1:10),
      col.names = c("Book", "Chapter", "Subject", "Probability"),
      digits = c(0, 0, 0, 4),
      caption = "Ten chapters with the highest topic probabilities (excluding the first sixty topics)")
```

And while the numbers are low, Russell on epistemology reminds the model much more of contemporary philosophy than most of the other books. This makes sense, I think.

To end this little inquiry, I want to look a bit at how much these books resemble the articles of their time. The next six graphs compare the eight books (collectively) to the journal articles published between 1876–1925. I'll compare the average topic probability, and the maximum topic probability, for each of the ninety topics in the journals and in the books.^[Small note on methodology. When I talk about the average topic probability for the books, this is something that gets calculated in two steps. First, I calculate the average for each book, across its chapters. Then I average the books. I'm doing this rather than averaging the chapters because that approach would mean that the books with more chapters would swamp the books with fewer.] I'll do this thirty topics at a time, because otherwise we get a bunch of dots clustered together in the bottom left corner of the graph. So on this graph the x axis measures the average probability of a topic in journal articles up to 1925, and the y axis measures the average probability of a topic in the eight books.

```{r compare-journals-and-books}
# Get articles from before 1926
early_articles <- articles %>%
  filter(year < 1926)

# Restrict gamma to those
early_gamma <- relabeled_gamma %>%
  filter(document %in% early_articles$document)

# Mean and max
early_journal_summary <- early_gamma %>%
  group_by(topic) %>%
  summarise(jh = max(gamma), jm = mean(gamma))

# Now work out book averages and maxes in two steps
gg_summary <- gg_revised %>%
  group_by(book_name, topic) %>%
  summarise(bh = max(gamma), bm = mean(gamma)) %>%
  group_by(topic) %>%
  summarise(bh = max(bh), bm = mean(bm))

book_journal_graph <- inner_join(early_journal_summary, gg_summary, by = "topic") %>%
  mutate(topicfactor = as.factor(topic)) %>%
  inner_join(the_categories, by = "topic")
```

```{r first-thirty-book-journal-compare-mean, fig.height = 5, fig.cap = "Average probability for the first thirty topics in journals and books.", fig.alt = alt_text}
ggplot(filter(book_journal_graph, topic < 31), aes(x = jm, y  = bm, color = topicfactor)) + 
  geom_point() +
  scale_color_manual(values = cols) +
  freqstyle +
  theme(legend.position = "none") +
  labs(x = "Average probabilty of topic in journals", y = "Average probability of topic in books") +
  ggrepel::geom_text_repel(aes(label=ifelse(topic == 1 | topic == 2 | topic == 3 | topic == 16 | topic == 24,as.character(sub_lower),''))) +
  theme(legend.position = "none") +
  scale_x_continuous(expand = expansion(mult = c(0.03, 0.01))) +
  scale_y_continuous(expand = expansion(mult = c(0.03, 0.03)))

alt_text <- "A scatterplot comparing the distribution of topics 1–30 in journal articles up to 1925 and in the books being discussed. Most topics are present to roughly the same amount, but idealism is much less prevalent in the books, and ordinary language philosophy is more prevalent."
```

The books I've chosen are more like ordinary language, and less like idealism, than the journals. And they have a little more ethics in them. We get a similar story if we look at the maximum values instead of the average values.

```{r first-thirty-book-journal-compare-max, fig.height = 5, fig.cap = "Maximum probability for the first thirty topics in journals and books.", fig.alt = alt_text}
ggplot(filter(book_journal_graph, topic < 31), aes(x = jh, y  = bh, color = topicfactor)) + 
  geom_point() +
  scale_color_manual(values = cols) +
  freqstyle +
  theme(legend.position = "none") +
  labs(x = "Maximum probabilty of topic in journals", y = "Maximum probability of topic in books") +
  ggrepel::geom_text_repel(aes(label=ifelse(topic == 1 | topic == 2 | topic == 3 | topic == 16 | topic == 24,as.character(sub_lower),''))) +
  theme(legend.position = "none") +
  scale_x_continuous(expand = expansion(mult = c(0.03, 0.01))) +
  scale_y_continuous(expand = expansion(mult = c(0.03, 0.03)))

alt_text <- "A scatterplot comparing the maximum probability distribution of topics 1–30 in journal articles up to 1925 and in the books being discussed. Maximum probabilities are generally higher for journal articles than for books. Ordinary language and Value are much more present in books. "
```

There are a lot more journal articles, and some of them are very short, so the maximum probabilities go much higher for the journals than the chapters. But otherwise there isn't much of a pattern here. Let's move on to the middle thirty topics.

```{r second-thirty-book-journal-compare-mean, fig.height = 5, fig.cap = "Average probability for the second thirty topics in journals and books.", fig.alt = alt_text}
ggplot(filter(book_journal_graph, topic < 61, topic > 30), aes(x = jm, y  = bm, color = topicfactor)) + 
  geom_point() +
  scale_color_manual(values = cols) +
  freqstyle +
  theme(legend.position = "none") +
  labs(x = "Average probabilty of topic in journals", y = "Average probability of topic in books") +
  ggrepel::geom_text_repel(aes(label=ifelse(topic == 32 | topic == 41 | topic == 47 | topic == 52 | topic == 60,as.character(sub_lower),''))) +
  theme(legend.position = "none") +
  scale_x_continuous(expand = expansion(mult = c(0.03, 0.7))) +
  scale_y_continuous(expand = expansion(mult = c(0.03, 0.01)))

alt_text <- "A scatterplot comparing the average probability distribution of topics 31-60 in journal articles up to 1925 and in the books being discussed. War and liberal democracy are much more prevalent in books, and perception and Kant are more prevalent in journals. Otherwise, most topics are present roughly the same."
```

This perhaps tells us more about the books I chose than the difference between philosophy in books and philosophy in journals. I'm sure there was discussion of Kant and perception in books at the time; just not so much in these eight books. Maybe war and liberal democracy are under-represented in the journals relative to their importance to philosophy at the time; I would need more information. No one is talking about radical translation before 1925. The same patterns hold, more or less, if we look at maximum values.

```{r second-thirty-book-journal-compare-max, fig.height = 5, fig.cap = "Maximum probability for the second thirty topics in journals and books.", fig.alt = alt_text}
ggplot(filter(book_journal_graph, topic < 61, topic > 30), aes(x = jh, y  = bh, color = topicfactor)) + 
  geom_point() +
  scale_color_manual(values = cols) +
  freqstyle +
  theme(legend.position = "none") +
  labs(x = "Maximum probabilty of topic in journals", y = "Maximum probability of topic in books") +
  ggrepel::geom_text_repel(aes(label=ifelse(topic == 32 | topic == 40 | topic == 41 | topic == 47 | topic == 51 | topic == 52 | topic == 60,as.character(sub_lower),''))) +
#  geom_text(aes(label=ifelse(topic == 32 | topic == 47 ,as.character(subject),'')),hjust=-.02,vjust=-0.4) +
#  geom_text(aes(label=ifelse(topic == 41 | topic == 52 ,as.character(subject),'')),hjust=-.01,vjust=1.4) +
#  geom_text(aes(label=ifelse(topic == 60 | topic == 41 | topic == 52 ,as.character(subject),'')),hjust=-0.03,vjust=-0.2) +
#  geom_text(aes(label=ifelse(topic == 40 | topic == 51 ,as.character(subject),'')),hjust=1.1,vjust=-0.1) +
  theme(legend.position = "none") +
  scale_x_continuous(expand = expansion(mult = c(0.25, 0.02))) +
  scale_y_continuous(expand = expansion(mult = c(0.06, 0.1)))

alt_text <- "A scatterplot comparing the maximum probability distribution of topics 31-60 in journal articles up to 1925 and in the books being discussed. Maximum probabilities are generally higher for journals than books. War is much more prevalent in books."
```

The maximum probabilities are, as always, higher for the journals than for the book chapters. And there are some articles that are really about color, or about philosophy of mathematics. There is, as I've already mentioned, one book chapter that's also about philosophy of mathematics. Onto the last thirty.

```{r third-thirty-book-journal-compare-mean, fig.height = 5, fig.cap = "Average probability for the last thirty topics in journals and books.", fig.alt = alt_text}
ggplot(filter(book_journal_graph, topic < 91, topic > 60), aes(x = jm, y  = bm, color = topicfactor)) + 
  geom_point() +
  scale_color_manual(values = cols) +
  freqstyle +
  theme(legend.position = "none") +
  labs(x = "Average probabilty of topic in journals", y = "Average probability of topic in books") +
  ggrepel::geom_text_repel(aes(label=ifelse(topic == 65 | topic == 83 | topic == 74,as.character(sub_lower),''))) +
#  geom_text(aes(label=ifelse(topic == 60 | topic == 60 ,as.character(subject),'')),hjust=-0.03,vjust=-0.2) +
#  geom_text(aes(label=ifelse(topic == 74 | topic == 51 ,as.character(subject),'')),hjust=1.1,vjust=-0.1) +
  theme(legend.position = "none") +
  scale_x_continuous(expand = expansion(mult = c(0.1, 0.02))) +
  scale_y_continuous(expand = expansion(mult = c(0.1, 0.02)))

alt_text <- "A scatterplot comparing the average probability distribution of topics 61-90 in journal articles up to 1925 and in the books being discussed. Average probabilities are generally lower for journals. Egalitarianism and population ethics are much higher for books; knowledge is also higher in books."
```

Note that the scales here are very different. The numbers are all low, but they are much lower for the journals than the books. So even though knowledge is way over to the right of the graph, the numbers for it are actually bigger in the books than the journals. And it's a little bigger in the journals than I had quite realised; there are no articles primarily in epistemology, but it isn't at zero like some other topics. Let's end this section with looking at the maximum probabilities in each topic.

```{r third-thirty-book-journal-compare-max, fig.height = 5, fig.cap = "Maximum probability for the last thirty topics in journals and books.", fig.alt = alt_text}
ggplot(filter(book_journal_graph, topic < 91, topic > 60), aes(x = jh, y  = bh, color = topicfactor)) + 
  geom_point() +
  scale_color_manual(values = cols) +
  freqstyle +
  theme(legend.position = "none") +
  labs(x = "Maximum probabilty of topic in journals", y = "Maximum probability of topic in books") +
  ggrepel::geom_text_repel(aes(label=ifelse(topic == 65 | topic == 71 |topic == 74 | topic == 76 | topic == 83 ,as.character(sub_lower),''))) +
#  geom_text(aes(label=ifelse(topic == 65 | topic == 52 ,as.character(subject),'')),hjust=-.01,vjust=1.5) +
#  geom_text(aes(label=ifelse(topic == 1 | topic == 41 | topic == 52 ,as.character(subject),'')),hjust=-0.03,vjust=-0.2) +
#  geom_text(aes(label=ifelse(topic == 71 | topic == 51 ,as.character(subject),'')),hjust=1.03,vjust=-0.01) +
  theme(legend.position = "none") +
  scale_x_continuous(expand = expansion(mult = c(0.16, 0.02))) +
  scale_y_continuous(expand = expansion(mult = c(0.06, 0.02)))

alt_text <- "A scatterplot comparing the maximum probability distribution of topics 61-90 in journal articles up to 1925 and in the books being discussed. The maximum probablity for journals is higher for all topics than for the maximum probability for books. Abortion and self-defense is the most extreme data-point, and is much higher in journal articles than in books."
```

For all thirty of these, the highest probability in the journals is higher than the highest probability in any book chapter. I was wondering whether the epistemology chapters would be the counterexamples to this claim, but they didn't come that close. Where we did get close to a counterexample was that Sidgwick _almost_ sounds more like a modern Parfitian than any journal author. But not quite—when there are three thousand journal articles, there will usually be one counterexample to any generalisation in there somewhere.

## Philosophers' Imprint {#imprint-section}

I've often speculated in the book about what would happen if we ran the model forward beyond the end of JSTOR's moving wall. The purpose of this section is to look at what happens if we run the model forward in one very particular way. I downloaded all the articles published in 2019 by the open-access journal [Philosophers' Imprint](https://www.philosophersimprint.org). I extracted the text from these articles using Jeroen Ooms's package [pdftools](https://docs.ropensci.org/pdftools/). And then I applied the model to these articles using the posterior function in [topicmodels](https://cran.r-project.org/web/packages/topicmodels/index.html). Here are the primary topics for each of the fifty-four articles.

```{r get-imprint-articles}
# Load the titles from the CSV
require(readr)
imprint_titles <- read_csv("imprint-titles.csv")

# Adjust the number to the code that was used in the LDA
imprint_titles <- imprint_titles %>%
  mutate(document = paste0("imprint-",number)) %>%
  mutate(citation = paste0(author, ", \"", title, "\""))

# Merge with the list of top gammas for each article
imprint_top_gamma_display <- inner_join(imprint_titles, imprint_top_gamma, by = "document") %>%
  inner_join(the_categories, by = "topic") %>%
  arrange(topic) %>%
  select(citation, subject, gamma, everything())
```

```{r display-article-summary}

require(DT)
cat("<table style=\'margin-bottom:0px\'>",
    paste0("<caption>",
    	   "(#tab:display-article-summary)",
          "Topic assignments for articles in <i>Philosophers' Imprint</i>, 2019.",
           "</caption>",
           "</table>", sep =" ")
)
datatable(imprint_top_gamma_display %>%
        select(citation, subject, gamma), 
          colnames = c("Article", "Topic", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:2)))
        )%>%
  formatSignif('gamma',3) %>%
  formatStyle(1:3,`text-align` = 'left')
```

For each article I've shown the topic to which the model gives the highest probability. And these are, I think, mostly sensible classifications. But I've also shown the probability the model gives to that article being in just that topic. And given that these are maximal, they might look surprisingly low.

As we saw in the [previous section](#classic-book-section), the model is extremely cautious when making out-of-sample classifications. So even for what looks to a human reader like a fairly clear case, like say Janum Sethi's "[Two Feelings in the Beautiful: Kant on the Structure of Judgments of Beauty](http://hdl.handle.net/2027/spo.3521354.0019.034)", the model is only 30 percent sure of its judgment. Now I guess it isn't surprising that the model is a little hesitant about whether to classify this with article on Kant or articles on beauty, but the uncertainty seen here in out-of-sample applications goes well beyond this. Here, for instance, is the list of all the topics it thinks Sethi's article might be in. (As has been the standard through this book, I've cut the table off at 2 percent.)

```{r individual-article-imprint-kable}
imprint_kable <- function(x){
temp_gamma <- imprint_gamma %>%
  filter(document == x, gamma > 0.02) %>%
  select(topic, gamma) %>%
  inner_join(the_categories, by = "topic") %>%
  select(sub_lower, gamma) %>%
  mutate(sub_lower = fcap(sub_lower)) %>%
  mutate(gamma = round(gamma, 4)) %>%
  arrange(-gamma)

#capt <- paste0(filter(imprint_top_gamma_display, document == x)$citation[1],
#               ", ",
#               "Philosophers' Imprint, 2019, vol. 19, number",
#               ", ",
#               filter(imprint_top_gamma_display, document == x)$number[1]-1900
capt <- filter(imprint_top_gamma_display, document == x)$citation[1]


kable(temp_gamma, 
      col.names = c("Subject", "Probability"), 
      caption = capt) %>% 
 kable_styling(full_width = F)
}
```

```{r sethi-kable}
imprint_kable("imprint-1934")
```

The top three look reasonable, though the numbers are too low, especially for Kant. This is just what happens in these out-of-sample applications. I have no idea why the model thought [dewey and pragmatism](#topic32) was there. But the story about [norms](#topic90) is a bit more interesting.

It turns out that the language of twenty first century philosophy is rather different from the language of twentieth-century philosophy. This is like the way in which the language of midcentury British philosophy was rather different to the language of other places and times. And the norms topic picks up some of these newly fashionable words. I'll come back to this at much greater length in section \@ref(buzzwords-section).

It isn't only Sethi's paper that is part of this linguistic change. There are many papers that are primarily in norms, even though they aren't really all about normative philosophy. Some of that is washed away when using the weighted counts, but far from all of it. Here is the table of the weighted sum of each of the topics over the 54 articles. (That is, it's the sum, over the 54 articles, of the article being in that topic.) The table is long, so it's split up, sortable, and searchable.

```{r imprint-topic-summary}
imprint_summary_DT <- inner_join(imprint_summary, the_categories, by = "topic") %>%
  arrange(-g) %>%
  select(topic, sub_lower, g) %>%
  mutate(sub_lower = fcap(sub_lower))

require(DT)
cat("<table style=\'margin-bottom:0px\'>",
    paste0("<caption>",
    	   "(#tab:imprint-topic-summary)",
          "Weighted sum of topic probabilities for articles in <i>Philosophers' Imprint</i>, 2019.",
           "</caption>",
           "</table>", sep =" ")
)
datatable(imprint_summary_DT, 
          colnames = c("Topic Number", "Topic", "Weighted Sum"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:2))),
          )%>%
  formatSignif('g',3) %>%
  formatStyle(1:3,`text-align` = 'left')
```

How does norms get to 4.8 like that? Lots of small pieces it turns out.

```{r imprint-norms-summary}
imprint_norms_DT <- inner_join(imprint_titles, imprint_gamma, by = "document") %>%
  filter(topic == 90) %>%
  inner_join(the_categories, by = "topic") %>%
  arrange(-gamma) %>%
  select(citation, gamma, everything())

require(DT)
cat("<table style=\'margin-bottom:0px\'>",
    paste0("<caption>",
    	   "(#tab:imprint-norms-summary)",
          "Probability of being in topic 90 for articles in <i>Philosophers' Imprint</i>, 2019.",
           "</caption>",
           "</table>", sep =" ")
)
datatable(imprint_norms_DT %>%
            select(citation, gamma), 
          colnames = c("Article", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:1))),
          )%>%
  formatSignif('gamma',3) %>%
  formatStyle(1:2,`text-align` = 'left')
```

This is really surprising, and a sign that something's gone wrong. It tells us something, I think, about the language of twenty-first-century philosophy. I'll return to this in the next section. What I'd rather look at, because I think it tells us something more about where philosophy is heading, is the topic that was second on the above list: [other history](#topic04).

This topic had looked rather dead in recent journals, but it turns up second overall here. And just eyeballing the article list that might not be too surprising. There are articles on Suárez (two of them!), William King, Amo, and Mary Shepherd. But this isn't entirely why other history pops up so high here.

```{r imprint-history-summary}
imprint_norms_DT <- inner_join(imprint_titles, imprint_gamma, by = "document") %>%
  filter(topic == 4) %>%
  inner_join(the_categories, by = "topic") %>%
  arrange(-gamma) %>%
  select(citation, gamma, everything())

require(DT)
cat("<table style=\'margin-bottom:0px\'>",
    paste0("<caption>",
    	   "(#tab:imprint-history-summary)",
          "Probability of being in topic 4 for articles in <i>Philosophers' Imprint</i>, 2019.",
           "</caption>",
           "</table>", sep =" ")
)
datatable(imprint_norms_DT %>%
            select(citation, gamma), 
          colnames = c("Article", "Probability"), 
          rownames = FALSE,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:1))),
          )%>%
  formatSignif('gamma',3) %>%
  formatStyle(1:2,`text-align` = 'left')
```

This makes the lack of attention to other history in the journals up to 2013 even more striking. James, Wittgenstein and Nietzche are hardly obscure figures. If articles on them are enough to keep the other history scoreboard ticking over, it's really telling that that scoreboard had ground to a halt.

Apart from this topic, the results are largely as I expected. The contemporary topics mostly keep growing. [Vagueness](#topic86), which peaked in the early 2000s, is an exception. And Imprint does less philosophy of science than some journals, so the recent philosophy of science topics aren't showing up much. But otherwise the new topics are still growing. And the old topics are still mostly shrinking, but with two big exceptions. One, the resurgence of interest in historical figures outside of the huge names such Plato, Descartes and Kant, I've already mentioned. The other is that there is a paper on [idealism](#topic02).

```{r idealism-imprint-paper-summary}
imprint_kable("imprint-1944")
```

Now on the one hand, that's only a 10.1 percent probability of being in idealism. On the other hand, it doesn't look like a mistake for the model to put this article in idealism. And to my eyes, in the twelve journals I'm focussing on, there are no articles from about 1995–2013 that were correctly classified as idealism articles.

I don't want to say that one article in _Imprint_ is a trend, even if it is something we hadn't seen in the last seven thousand or so articles in the original data set. But it is interesting, and something to watch for over upcoming years. It is so striking to me that idealism went from being so big to so small, and some regression to the mean seems like it wouldn't be a surprise.

## Buzzwords {#buzzwords-section}

One of the methods I used for building the model was to run repeated [refinements](#refinements-section) of the model, to try to make it better track actual philosophical topics. At the time I did this, I was worried that this would have bad consequences. It felt like tightening the strings. And while this is generally a good idea, if things are made too tight, they snap. After a few refinements, I started to think this was a silly bit of reasoning by analogy. These aren't actually strings, so they can't actually snap, right? Well, after one hundred iterations of the refinement script, I got a topic whose distribution looked like this.

```{r bad-gamma-absolute, fig.cap = "Number of articles in the norms topic in the bad LDA.", fig.alt = alt_text}
# Doing the facet graph with number of articles, not proportion
# It's a bit cheating to do this first, but it looks really striking
jjj <- 90

yupper <- max(bad_gamma_year_journal$g,na.rm=TRUE)

facet_labels <- chap_two_facet_labels %>%
  mutate(year = 1880, g = yupper)

facet_labels$journal <- factor(facet_labels$journal, levels = journal_order)

ggplot(data = bad_gamma_year_journal %>% drop_na(), aes(x = year, y = g))  +
  facetstyle +
  scale_y_continuous(breaks = c(1, 2, 3, 4)) +
  geom_point(size = 0.15, colour = hcl(h = (jjj-1)*(360/cats)+15, l = 65, c = 100)) +
  theme(legend.position="none") +
  labs(x = element_blank(), y = "Weighted Number of articles", title = "The Bad Topic") +
  facet_wrap(~journal, ncol = 3, labeller = as_labeller(journal_short_names)) +
    geom_text(data = facet_labels,
            mapping = aes(label = short_name),
            vjust = "inward", 
            hjust = "inward",
            fontface = "bold", 
            size = 3,
            colour = "grey40")

alt_text <- "A series of scatterplots showing the weighted number of articles about norms in twelve journals from 1900 to 2000. The number rises dramatically starting sometime between 1940 and 1980 across all journals."

```

Those are remarkable graphs—it seems that this topic is getting to be a bigger and bigger deal in all the journals. I had not seen anything like this; after 1970 it's almost impossible to get the "generalist" journals, the philosophy of science journals, and the ethics journals moving in the same direction. Maybe it's a function of the journals publishing more articles over time. We could check this by looking at what proportion of the journals are made up by this topic, not the absolute number of (expected) articles.^[Most of the graphs in chapter \@ref(all-90-topics) are proportional, not the absolute graph I just showed you.]

```{r bad-gamma-relative, fig.cap = "Proportion of articles in the norms topic in the bad LDA.", fig.alt = alt_text}
# A standard facet graph, but with the bad LDA instead of the good LDA
jjj <- 90

yupper <- max(bad_gamma_year_journal$y,na.rm=TRUE)

facet_labels <- chap_two_facet_labels %>%
  mutate(year = 1880, y = yupper)

facet_labels$journal <- factor(facet_labels$journal, levels = journal_order)

ggplot(data = bad_gamma_year_journal %>% drop_na(), aes(x = year, y = y))  +
  facetstyle +
  scale_y_continuous(breaks = 0.03 * 1:3) +
  geom_point(size = 0.15, colour = hcl(h = (jjj-1)*(360/cats)+15, l = 65, c = 100)) +
  theme(legend.position="none") +
  labs(x = element_blank(), y = "Average Probability", title = "The Bad Topic") +
  facet_wrap(~journal, ncol = 3, labeller = as_labeller(journal_short_names)) +
      geom_text(data = facet_labels,
            mapping = aes(label = short_name),
            vjust = "inward", 
            hjust = "inward",
            fontface = "bold", 
            size = 3,
            colour = "grey40")
alt_text <- "A series of scatterplots showing the proportion of articles about norms in twelve journals from 1900 to 2000. The proportion rises dramatically sometime between 1940 and 1980 for all journals, though the rise is less steep for Analysis, Philosophy of Science, and British Journal for the Philosophy of Science."
```

It's slightly less steep, especially in _Philosophy of Science_. But the generalist journals—except _Analysis_—and _British Journal for the Philosophy of Science_ are still rising rapidly. Let's look at what articles are primarily in this topic.

```{r bad-gamma-top-articles}
# A very simple kable of which articles have highest probability of being in this bad topic
kable(bad_gamma %>% 
        slice(1:10) %>% 
        select(citation, gamma) %>%
        mutate(gamma = round(gamma,4)), 
      col.names = c("Article", "Topic Probability"), 
      caption  = "Top articles in the norms topic in the bad LDA.")
```

This is very confusing in three different ways.

1. Although the topic seems concentrated in the twenty-first century, two of the top ten articles are from a fair way ago - including the top one.
2. If that top article is excluded, no article has a topic probability of over 0.4. This is true even though for some journal-year pairs, the average topic probability is over 0.1. It feels like every article must get a reasonable probability of being in this topic.
3. Relatedly, there doesn't seem to be any thematic unity to the articles here. What could we even call the "topic" which has these ten articles as paradigm cases? I'm calling it _norms_ because it looks from the graphs like the counterpart of our topic norms, but this is hardly a perfect name.

For comparison, the original [topic 90](#topic90) had a top ten list that looked a little more sensible.

```{r bad-gamma-compare-top-articles}

t <- relabeled_articles %>%
  filter(topic == 90) %>%
  arrange(-gamma) %>%
  slice(1:10) %>%
  select(citation, gamma) %>%
  mutate(gamma = round(gamma, 4))

kable(t, col.names = c("Article", "Topic Probability"), caption = "Top articles in the norms topic in the good LDA.")
```

The papers are more recent, the probabilities are higher, and there is some more unity to the topic. It's about normativity and objectivity, very broadly construed, with a bit of a focus on Brandom. And we can still see hints of that in the new top ten list, but it's gotten blurrier. The Rosen article that's at nearly 70 percent in the original topic has dropped to around 20 percent in the new topic, reflecting the lack of thematic unity.

Maybe we can get a bit of a better look at this new weird topic by looking at its keywords. Remember that an LDA model assigns each word a probability of being in a paradigm article in the topic. We can compare that to the frequency of that word in the whole data set to get a sense of what's characteristic of the topic. (Again, it's necessary to restrict attention to the five thousand most common words here to prevent too much focus on words that appear just a handful of times.) And here's what we get. The second column here is the ratio of the probability of the word being in a (paradigm) article in this topic to the word's overall frequency.

```{r bad-beta-top-ratio}
kable(bad_beta %>% select(word, y) %>% slice(1:20) %>% mutate(y = round(y, 1)), 
      col.names = c("Word", "Ratio"),
      caption = "Top words in the norms topic in the bad LDA.") %>% 
 kable_styling(full_width = F)
```

This I think is the clue to what's happened. The "topic" here is just the distinctive vocabulary of twenty-first-century philosophy. The topic appears in all journals because these words have become more and more prevalent in all journals.

I'll come back to direct evidence for this hypothesis in a minute, but first I wanted to show what a table like this looks like for normal topics. Here's the same table for topic 90 in the original model.

```{r good-beta-top-ratio}
kable(good_beta %>% select(word, y) %>% slice(1:20) %>% mutate(y = round(y, 1)), col.names = c("Word", "Ratio"),
       caption = "Top words in the norms topic in the good LDA.") %>% 
 kable_styling(full_width = F)
```

It's possible to see some of the twenty-first-century vocabulary, like _challenge_ and _accounts_ turning up at the bottom. But this is what things mostly should look like for a topic about normativity and objectivity. 

Just to get a sense of the appropriate scale for what's being measured here, here's what the top of the table for the [Kant topic](#topic32) looks like.

```{r kant-beta-top-ratio}
kable(kant_beta %>% select(word, y) %>% slice(1:10) %>% mutate(y = round(y, 1)), col.names = c("Word", "Ratio"),
       caption = "Top words in the Kant topic in the good LDA.") %>% 
  kable_styling(full_width = F)
```

That makes sense; the word _Kant_ is 163 times more likely to appear in a paradigm Kant article than in philosophy in general. A ratio like this of 163 is high, but having the highest ratio be fifteen is a sign something has gone wrong. The only topic that is really like this in the original model is [ordinary language philosophy](#topic 24).

```{r olp-beta-top-ratio}
kable(olp_beta %>% select(word, y) %>% slice(1:20) %>% mutate(y = round(y, 1)), col.names = c("Word", "Ratio"),
       caption = "Top words in the ordinary language philosophy topic in the good LDA.") %>% 
 kable_styling(full_width = F)
```

Like the new topic 90, this topic in the original model really tracked a style and not a content. It's really striking that the distinctive words in ordinary language philosophy are so much shorter than the distinctive words in contemporary philosophy.^[I wanted to include here some graphs about average word lengths over time, but they don't really show very much. There is a very gentle increase, focussed on the philosophy of science journals, but on the whole the distinctively short keywords don't really track anything about average word length. There was a notable drop in average word length in _Proceedings of the Aristotelian Society_ in the 1950s, but it didn't show up elsewhere. Most notably, there was no similar drop in average word length in _Mind_ or _Philosophical Quarterly_. That suggested it was something about the journal, perhaps connected to the fact that papers are read to the society, rather than about British philosophical culture more broadly.] But it's probably time to start actually proving that words like _commitment_, _challange_ and _approach_ are distinctively twenty-first-century words. So rather than just look at the outputs of complicated models, I'm going to end with some simple graphs of word frequency over time. 

The data set I'm using for the graphs to follow is the word lists as provided by JSTOR. That excludes some stop words, and all one and two letter words, but not all the other words that I filtered out before building the LDA. I'll start with some graphs of the keywords from this new topic 90.

```{r buzzwords-graphs-a, fig.height = 5, fig.cap = "Words about theories.", fig.alt = alt_text}
word_frequency_graphs(c("account", "accounts", "claim", "claims"))
alt_text <- "A scatterplot showing the frequency of words about theories (_account_, _accounts_, _claim_, and _claims_) in journal articles from 1880 to after 2000. The frequency of all four words begins to increase more rapidly beginning around 1960. "
```

```{r buzzwords-graphs-b, fig.height = 5, fig.cap = "Words about plans.", fig.alt = alt_text}
word_frequency_graphs(c("role", "appeal", "project", "focus"))
alt_text <- "A scatterplot showing the frequency of words about plans (_appeal_, _focus_, _project_, and _role_) in journal articles from 1880 to after 2000. The frequency of all four words begins to increase more rapidly after 1960."
```

```{r buzzwords-graphs-c, fig.height = 5, fig.cap = "Words about views.", fig.alt = alt_text}
word_frequency_graphs(c("commitment", "commitments", "proposal", "proposals"))
alt_text <- "A scatterplot showing the frequency of words about views (_commitment_, _commitments_, _proposal_, and _proposals_) in journal articles from 1880 to after 2000. The frequency for all four is quite low early on, and begins to increase significantly around 1930-1940."
```

```{r buzzwords-graphs-d, fig.height = 5, fig.cap = "Words about objections.", fig.alt = alt_text}
word_frequency_graphs(c("worry", "worries", "challenge", "challenges"))
alt_text <- "A scatterplot showing the frequency of words about objections (_challenge_, _challenges_, _worries_, and _worry_) in jounal articles from 1880 to after 2000. The frequency for all four begins to rise by around 1950, though _challenge_ starts to rise earlier, around 1920. "
```

```{r buzzwords-graphs-e, fig.height = 5, fig.cap = "Words about what's common.", fig.alt = alt_text}
word_frequency_graphs(c("typically", "relevant", "practices"))
alt_text <- "A scatterplot showing the frequency of words about what's common (_practices_, _relevant_, _typically_) in journal articles from 1880 to after 2000. The frequency of _relevant_ begins to rise dramatically around 1900; _practices_ and _typically_ begin to rise around 1960."
```

Not all of these words are shooting upwards, but many of them are. I had originally drawn these graphs with trend lines, but they aren't needed to see the pattern. At this rate we'll soon see articles made up of just the words _account_, _typically_, _relevant_ and _challenge_, plus perhaps their plurals.

So this is why I just used fifteen refinements of the model rather than one hundred. The language of early twenty-first-century philosophy is distinctive enough that if you push a text-based analysis too hard, it ends up just tracking form rather than content.

But didn't we have this already back in [ordinary language philosophy](#topic 24)? We did, though fortunately the binary sort helped find a couple of natural topics within it. Still, it would be nice to confirm that these words really were being used more frequently in midcentury. So let's look at the same graphs for the keywords from ordinary language philosophy.

```{r olpwords-graphs-a, fig.height = 5, fig.cap = "Words about speech acts.", fig.alt = alt_text}
word_frequency_graphs(c("ask", "answer", "question", "said"))
alt_text <- "A scatterplot showing the frequency of words about speech acts (_answer_, _ask_, _question_, and _said_) in journal articles from 1880 to after 2000. _Question_ is always the most frequent and _ask' is always the least frequent. All four words show a similar pattern with frequency peaks around the 1960s."
```

```{r olpwords-graphs-b, fig.height = 5, fig.cap = "Words about epistemic modality.", fig.alt = alt_text}
word_frequency_graphs(c("really", "perhaps", "course", "certainly"))
alt_text <- "A scatterplot showing the frequency of words about epistemic modality (_certainly_, _course_, _perhaps_, and _really_) in journal articles from 1880 to after 2000. _Course_ is generally the most frequent, and _certainly_ is the least frequent. All four words show a peak in frequency around 1960."
```

```{r olpwords-graphs-c, fig.height = 5, fig.cap = "Words about quantity.", fig.alt = alt_text}
word_frequency_graphs(c("quite", "sort", "seem", "much"))
alt_text <- "A scatterplot showing the frequency of words about quantity (_much_, _quite_, _seem_, and _sort_) in journal articles from 1880 to after 2000. _Much_ generally has the highest frequency, though its frequency declines steadily with time. _Sort_ has the lowest frequency in the 1880s through mid-1900s. The words somewhat converge in frequency around 1970. "
```

```{r olpwords-graphs-d, fig.height = 5, fig.cap = "Words about mental state attribution.", fig.alt = alt_text}
word_frequency_graphs(c("think", "want", "get"))
alt_text <- "A scatterplot showing the frequency of words about mental state attribution (_get_, _think_, and _want_) in journal articles from 1880 to after 2000. _Think_ appears much more frequently than the other words throughout the time span. All three words have moderate peaks in frequency around the 1970s. "
```

```{r olpwords-graphs-e, fig.height = 5, fig.cap = "Words about quantification.", fig.alt = alt_text}
word_frequency_graphs(c("anything", "something", "things"))
alt_text <- "A scatterplot showing the frequency of words about quantification (_anything_, _something_, and _things_) in journal articles from 1880 to after 2000. _Anything_ is the least frequent throughout most of the time span. All three words show a peak in frequency around 1960, though the peak is more pronounced for _something_ and _things.'"
```

The first three have roughly the pattern I was expecting, but the last two don't. I think there is a sense in which some of the stylistic changes that the ordinary language philosophers brought in persisted. And there is also a sense in which they were the last holdouts against the move to a more scientific philosophy. As is so often the case, it helps to look at a distinctive era as both the end of what came before it and the start of what came after it.

There is another puzzle that I left open above that I want to return to. How could we square the low ratio between the maximal and average topic probabilities for some journal-year pairs? The obvious answer is that every article is in the topic to some nontrivial degree. Let's see how true that is. So for a few journal-year pairs, I'm going to go through every article and list the probability that it is in this new topic. I'll start with _Philosophical Review_ in 2004.

```{r bad-gamma-pr-2004}
kable(bad_gamma %>% 
        filter(year == 2004, journal == "Philosophical Review") %>% 
        select(citation, gamma) %>%
        mutate(gamma = round(gamma, 4)),
      col.names = c("Article", "Topic Probability"),
      caption = "_Philosophical Review_, 2004, probability that each article is in the bad topic.")
```

The Roth paper really is about commitments, so it isn't surprising that it's a little higher than the others. But look how much this spreads around other articles. The model thinks there is something that all but one of these articles have seriously in common. And I think there isn't anything substantive (as opposed to stylistic) this could be. Let's move on to _Ethics_ in 2010.

```{r bad-gamma-ethics-2010}
kable(bad_gamma %>% 
        filter(year == 2010, journal == "Ethics") %>%
        select(citation, gamma) %>%
        mutate(gamma = round(gamma, 4)), 
      col.names = c("Article", "Topic Probability"),
      caption = "_Ethics_, 2010, probability that each article is in the bad topic.")
```

The same pattern shows up; almost all the articles are in the topic at a 5 percent probability or higher. To the extent that there was anything substantive in the topic, it was in normative ethics, so maybe the topic being so visible in _Ethics_ isn't too surprising. But let's see what happens when we do the same thing for _British Journal for the Philosophy of Science_ in 2011.

```{r bad-gamma-bjps-2011}
kable(bad_gamma %>% 
        filter(year == 2011, journal == "British Journal for the philosophy of Science") %>% 
        select(citation, gamma) %>%
        mutate(gamma = round(gamma, 4)), 
      col.names = c("Article", "Topic Probability"),
      caption = "_BJPS_, 2011, probability that each article is in the bad topic.")
```

Here we do get more articles that are clearly excluded. The difference between the last six articles is unimportant. Once you get below 0.1 percent, the probabilities are functions of how confident the model is in its central classifications. But it's still striking how many of these are above 1.1 percent. There are ninety topics, so if the model had no idea it would put each probability at 1.1 percent. The vast majority of the articles here are above that.

There is another study I could imagine running here, but it would take so long that I'm going to leave it to later work. Repeatedly refining the model broke because of the distinctive language of twenty-first-century philosophy. There are two possible explanations for that.

1. There has been a linguistic revolution over the last generation, and philosophers now write in a very different style to how they wrote a generation ago.
2. This is an artifact of model building, and if the model was stopped at any time, and ran the same study I did, there would be results like this. That is, doing what I did will get  weird results whenever there is linguistic drift, and there is always linguistic drift.

I actually could test these by running the study I did for this book but stopping in, say, 1993. But I don't think spending several hundred hours processing time on teasing apart these two explanations would be worthwhile.

That's in part because this question will resolve itself over time naturally. Hopefully more studies like mine (or preferably better designed studies than mine) will be run on data that goes through 2020 and beyond. Those will tell us even more about where philosophy is going, and answer several questions that I've left open as pleasant side effects.

<!--chapter:end:09-buzzwords.Rmd-->

# References {-}

Note that I'm not including here papers that are merely mentioned from the twelve journals, or from the other sources that were mined in chapter 9. But I am including papers where I drew on their content (beyond just their word distribution) in preparing this book.

<div id="refs"></div>

# (APPENDIX) Data Tables {-} 

# Subject Prominence by Subject

There are 90 tables below, one for each subject. In each table there are 138 rows, one for each year. In each year, there are 8 columns. This is what each of the columns represents.<br><br><br>

Weight
:    The weighted number of articles in that subject in that year. This is the main data point that's used throughout the book as the measure of the subject's prominence in the year.

Raw
:    The raw number of articles in that subject in that year.

W-Ratio
:    The weighted number of articles in that subject in that year per 1000 articles published that year.

R-Ratio
:    The raw number of articles in that subject in that year per 1000 articles published that year.

W-Pages
:    The weighted number of pages of articles on that subject in that year.

R-Pages
:    The raw number of pages of articles on that subject in that year.

WP-Ratio
:    The weighted number of pages of articles in that subject in that year per 1000 pages published that year.

RP-Ratio
:    The raw number of pages of articles in that subject in that year per 1000 pages published that year.


```{r append-two-loop}
app_a_table <- weight_numerator %>% 
  rename(Weight = y) %>% 
  mutate(Weight = round(Weight, 1)) %>% 
  full_join(count_numerator, by = c("year", "topic")) %>% 
  rename(Raw = y) %>% 
  full_join(weight_ratio, by = c("year", "topic")) %>%
  mutate(y = round(y * 1000)) %>% 
  rename(`W-Ratio` = y) %>% 
  select(-d) %>% 
  full_join(count_ratio, by = c("year", "topic")) %>%
  mutate(y = round(y * 1000)) %>% 
  rename(`R-Ratio` = y) %>% 
  select(-d) %>% 
  full_join(page_weight_numerator, by = c("year", "topic")) %>%
  mutate(y = round(y, 1)) %>% 
  rename(`W-Pages` = y) %>% 
  full_join(page_count_numerator, by = c("year", "topic")) %>%
  rename(`R-Pages` = y) %>% 
  full_join(page_weight_ratio, by = c("year", "topic")) %>%
  mutate(y = round(y * 1000)) %>% 
  rename(`WP-Ratio` = y) %>% 
  select(-d) %>% 
  full_join(page_count_ratio, by = c("year", "topic")) %>%
  mutate(y = round(y * 1000)) %>% 
  rename(`RP-Ratio` = y) %>% 
  select(-d) %>% 
  rename(Year = year)
  
  
for (jjj in 1:90){
  cat_subject <- fcap(the_categories$sub_lower[jjj])

  cat(" \n")
  cat("### ", cat_subject, " {-#a",jjj,"}\n\n", sep="")

  temp <- filter(app_a_table, topic == jjj) %>% 
    arrange(Year) 
  
  print(
    kable(
      temp %>% select(-topic), 
      label = paste0("appendyearfre",jjj),
      caption = paste("Measures of the prominence of", cat_subject, "in each year's journals.")) 
  )
}
```

# Subject Prominence by Year

There are 138 tables below, one for each year In each table there are 90 rows, one for each subject. In each row, there are 8 columns. This is what each of the columns represents.<br><br><br>

Weight
:    The weighted number of articles in that subject in that year. This is the main data point that's used throughout the book as the measure of the subject's prominence in the year.

Raw
:    The raw number of articles in that subject in that year.

W-Ratio
:    The weighted number of articles in that subject in that year per 1000 articles published that year.

R-Ratio
:    The raw number of articles in that subject in that year per 1000 articles published that year.

W-Pages
:    The weighted number of pages of articles on that subject in that year.

R-Pages
:    The raw number of pages of articles on that subject in that year.

WP-Ratio
:    The weighted number of pages of articles in that subject in that year per 1000 pages published that year.

RP-Ratio
:    The raw number of pages of articles in that subject in that year per 1000 pages published that year.


```{r append-three-loop}
app_b_table <- weight_numerator %>% 
  rename(Weight = y) %>% 
  mutate(Weight = round(Weight, 1)) %>% 
  full_join(count_numerator, by = c("year", "topic")) %>% 
  rename(Raw = y) %>% 
  full_join(weight_ratio, by = c("year", "topic")) %>%
  mutate(y = round(y * 1000)) %>% 
  rename(`W-Ratio` = y) %>% 
  select(-d) %>% 
  full_join(count_ratio, by = c("year", "topic")) %>%
  mutate(y = round(y * 1000)) %>% 
  rename(`R-Ratio` = y) %>% 
  select(-d) %>% 
  full_join(page_weight_numerator, by = c("year", "topic")) %>%
  mutate(y = round(y, 1)) %>% 
  rename(`W-Pages` = y) %>% 
  full_join(page_count_numerator, by = c("year", "topic")) %>%
  rename(`R-Pages` = y) %>% 
  full_join(page_weight_ratio, by = c("year", "topic")) %>%
  mutate(y = round(y * 1000)) %>% 
  rename(`WP-Ratio` = y) %>% 
  select(-d) %>% 
  full_join(page_count_ratio, by = c("year", "topic")) %>%
  mutate(y = round(y * 1000)) %>% 
  rename(`RP-Ratio` = y) %>% 
  select(-d) %>% 
  mutate(topic = as.numeric(topic)) %>% 
  left_join(the_categories, by = "topic") %>% 
  arrange(year, topic) %>% 
  select(-cat_num, -cat_name, -topic) %>% 
  select(Subject = sub_lower, everything()) %>% # What comes next is a hack to change the capitalisation
  select(-subject) %>% 
  mutate(Subject = fcap(Subject))
  
  
  
for (jjj in 1:138){
  cat_subject <- jjj + 1875

  cat(" \n")
  cat("### ", cat_subject, " {-#b",jjj,"}\n\n", sep="")

  temp <- filter(app_b_table, year == cat_subject)
  
  print(
    kable(
      temp %>% select(-year), 
      label = paste0("appendsubjectfre",cat_subject),
      caption = paste("Measures of the prominence of each subject in ", cat_subject, "'s journals.")) 
  )
}
```

# Data Tables for Chapter 4

### Category Prevalence for Each Topic {-}

```{r category-count-table}

temp <- category_count %>% 
  arrange(year, category) %>% 
  full_join(category_year, by  = c("year", "category")) %>% 
  mutate(y = round(y * 10)/10) %>% 
  full_join(category_frequency %>% 
              arrange(year, category) %>%
              select(year, category, f), by = c("year", "category")) %>% 
  mutate(f = scales::percent(f, accuracy = 0.1, suffix = "%")) %>% 
  select(category, Year = year, Count = n, `Weighted Count` = y, `Weighted Frequency` = f)

cat_list <- temp %>% 
  ungroup() %>% 
  group_by(category) %>% 
  tally()

for (jjj in cat_list$category){
  temp2 <- temp %>% 
    filter(category == jjj) %>% 
    select(-category)
  
cat("**",jjj,"**  \n  \n", sep="")

print(
  kable(
    temp2, align = 'lccc', caption = paste0("Three measures of the prevalence of ",str_to_lower(jjj)," in each year.")
  )
)
}
```

### Weighted Frequency of Categories in Each Year {-}

```{r category-by-year}
temp <- category_frequency %>% 
  select(year, category, f) %>% 
  mutate(f = scales::percent(f, accuracy = 0.1, suffix = "%")) %>% 
  pivot_wider(id_cols = year, names_from = category, values_from = f) %>% 
  rename(Year = year)

temp1 <- temp %>% 
  select(c(1, 4, 5, 7, 10, 12, 13))

temp2 <- temp %>% 
  select(c(1, 2, 3, 6, 8, 9, 11))
  
kable(temp1, align = c('r', 'c', 'c', 'c', 'c', 'c', 'c'), caption = "Weighted frequency by year for the six larger topics.")

kable(temp2, align = c('r', 'c', 'c', 'c', 'c', 'c', 'c'), caption = "Weighted frequency by year for the six smaller topics.")
```


<!-- ### Data Table for Figures 4.1 and 4.2 {-} -->

<!-- ```{r category-count-table} -->
<!-- temp <- category_count %>%  -->
<!--   arrange(year, category) %>%  -->
<!--   rename(Year = year, Category = category, `Number of Articles` = n) -->

<!-- kable(temp, caption = "Number of articles in each category in each year.") -->
<!-- ``` -->

<!-- ### Data Table for Figures 4.3 and 4.4 {-} -->

<!-- ```{r category-weight-table} -->
<!-- temp <- category_year %>%  -->
<!--   arrange(year, category) %>% -->
<!--   mutate(y = round(y * 10)/10) %>%  -->
<!--   rename(Year = year, Category = category, `Weighted Number of Articles` = y) -->

<!-- kable(temp, caption = "Weighted number of articles in each category in each year.") -->
<!-- ``` -->

<!-- ### Data Table for Figures 4.5 and 4.6 {-} -->

<!-- ```{r category-weighted-frequency-table} -->
<!-- temp <- category_frequency %>%  -->
<!--   arrange(year, category) %>% -->
<!--   select(year, category, f) %>%  -->
<!--   mutate(f = scales::percent(f, accuracy = 0.1, suffix = "%")) %>%  -->
<!--   rename(Year = year, Category = category, `Weighted Frequency of Articles` = f) -->

<!-- kable(temp, caption = "Weighted frequency of articles in each category in each year.") -->
<!-- ``` -->

<!-- ### Data Table for Figure 4.7 {-} -->

<!-- ```{r category-frequency-facet} -->
<!-- temp <- category_frequency %>%  -->
<!--   arrange(category, year) %>% -->
<!--   select(category, year, f) %>%  -->
<!--   mutate(f = scales::percent(f, accuracy = 0.1, suffix = "%")) %>%  -->
<!--   rename(Year = year, Category = category, `Weighted Frequency of Articles` = f) -->

<!-- kable(temp, caption = "Weighted frequency of articles in each year for each category.") -->
<!-- ``` -->

# Data Tables for Chapter 6

### Data Table for Figure 6.1 {-}

```{r epistemology-summary-table}
temp <- epistemology_yearcount %>% 
  arrange(year) %>% 
  select(year, n) %>% 
  rename(Year = year, `Number of Articles` = n)

kable(temp, align = 'cc', caption = "Number of epistemology articles in each year.")
```

### Data Table for Figure 6.2 {-}


```{r epistemology-topic-count}
temp <- epistemology_yeartopics_postwar %>% 
  drop_na() %>% 
  arrange(year, topic) %>% 
  select(year, topic, tn) %>% 
  filter(topic <= 10) %>% 
  pivot_wider(id_cols = year, names_from = topic, values_from = tn) %>% 
  rename(Year = year)

kable(temp, align = 'rcccccccccc', caption = "Raw number of epistemology articles in topics 1–10 in each year.")

temp <- epistemology_yeartopics_postwar %>% 
  drop_na() %>% 
  arrange(year, topic) %>% 
  select(year, topic, tn) %>% 
  filter(topic > 10, topic <= 20) %>% 
  pivot_wider(id_cols = year, names_from = topic, values_from = tn)  %>% 
  rename(Year = year)

kable(temp, align = 'rcccccccccc', caption = "Raw number of epistemology articles in topics 11–20 in each year.")

temp <- epistemology_yeartopics_postwar %>% 
  drop_na() %>% 
  arrange(year, topic) %>% 
  select(year, topic, tn) %>% 
  filter(topic > 20, topic <= 30) %>% 
  pivot_wider(id_cols = year, names_from = topic, values_from = tn)  %>% 
  rename(Year = year)

kable(temp, align = 'rcccccccccc', caption = "Raw number of epistemology articles in topics 21–30 in each year.")

temp <- epistemology_yeartopics_postwar %>% 
  drop_na() %>% 
  arrange(year, topic) %>% 
  select(year, topic, tn) %>% 
  filter(topic > 30, topic <= 40) %>% 
  pivot_wider(id_cols = year, names_from = topic, values_from = tn)  %>% 
  rename(Year = year)

kable(temp, align = 'rcccccccccc', caption = "Raw number of epistemology articles in topics 31–40 in each year.")



```

### Data Table for Figures 6.3 and 6.4 {-}


```{r epistemology-topics-weight}
temp <- epistemology_yeargamma_postwar %>% 
  drop_na() %>% 
  arrange(year, topic) %>% 
  select(year, topic, gamsum) %>% 
  filter(topic <= 10) %>% 
  mutate(gamsum = round(gamsum, 1)) %>% 
  pivot_wider(id_cols = year, names_from = topic, values_from = gamsum) %>% 
  rename(Year = year)

kable(temp, align = 'rcccccccccc', caption = "Weighted number of epistemology articles in topics 1–10 in each year.")

temp <- epistemology_yeargamma_postwar %>% 
  drop_na() %>% 
  arrange(year, topic) %>% 
  select(year, topic, gamsum) %>% 
  filter(topic > 10, topic <= 20) %>% 
  mutate(gamsum = round(gamsum, 1)) %>% 
  pivot_wider(id_cols = year, names_from = topic, values_from = gamsum)  %>% 
  rename(Year = year)

kable(temp, align = 'rcccccccccc', caption = "Weighted number of epistemology articles in topics 11–20 in each year.")

temp <- epistemology_yeargamma_postwar %>% 
  drop_na() %>% 
  arrange(year, topic) %>% 
  select(year, topic, gamsum) %>% 
  filter(topic > 20, topic <= 30) %>% 
  mutate(gamsum = round(gamsum, 1)) %>% 
  pivot_wider(id_cols = year, names_from = topic, values_from = gamsum)  %>% 
  rename(Year = year)

kable(temp, align = 'rcccccccccc', caption = "Weighted number of epistemology articles in topics 21–30 in each year.")

temp <- epistemology_yeargamma_postwar %>% 
  drop_na() %>% 
  arrange(year, topic) %>% 
  select(year, topic, gamsum) %>% 
  filter(topic > 30, topic <= 40) %>% 
  mutate(gamsum = round(gamsum, 1)) %>% 
  pivot_wider(id_cols = year, names_from = topic, values_from = gamsum)  %>% 
  rename(Year = year)

kable(temp, align = 'rcccccccccc', caption = "Weighted number of epistemology articles in topics 31–40 in each year.")


```

### Data Table for Figures 6.5 {-}


```{r epistemology-categories-count}
temp <- epistemology_categories %>% 
  arrange(year, category) %>% 
  select(year, category, y) %>% 
  mutate(y = round(y, 1)) %>% 
  pivot_wider(id_cols = year, names_from = category, values_from = y) %>% 
  rename(Year = year)

kable(temp, align = 'rccccc', caption = "Weighted number of epistemology articles in each category in each year.")
```

<!--chapter:end:references.Rmd-->

