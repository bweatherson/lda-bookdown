```{r t62a}
jjj <- 62
source('topic_comments/topic_summary_data.R') # Get data
opts_knit$set(eval.after = "fig.cap")
```

```{r t62b, fig.cap=the_categories$subject[jjj], fig.height= 5.2}
source('topic_comments/topic_summary_overall_graph.R')
```

```{r t62c, fig.cap=paste(the_categories$subject[jjj], "Articles in Each Journal")}
source('topic_comments/topic_summary_facet_graph.R')
```

```{r t62d}
source('topic_comments/topic_summary_char_art.R')
temp_dt
```

```{r t62e}
source('topic_comments/topic_summary_high_cites.R')
if(is_high_cites == 1){high_table} # Conditional because not all topics have high cites
cat("\n
	**Comments**
	\n")
```

Given how big a topic this was when I was a graduate student, I was very surprised that it wasn't bigger than it turned out to be. But a look at the overall graph explains why I got this wrong. The high water mark for this topic was from about 1985-1995. When I was in graduate school, it really was over all the journals that we were reading. We couldn't have predicted that it would fall from nearly 2% of the articles to under 1%.

One of the upsides of having a computer model analyse the data rather than going with one's own impressions is that it reduces the impact of having an idiosyncratic set of experiences in cases like this one.